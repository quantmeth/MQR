[{"path":"index.html","id":"bienvenue","chapter":"Bienvenue!","heading":"Bienvenue!","text":"Bienvenue dans la version préliminaire et en ligne du livre Méthodes Quantitatives avec R de P.-O. Caron. Le livre porte sur la programmation statistique en R et vise à démystifier les mécanismes ésotériques derrière les logiciels statistiques afin de les rendre plus accessibles. Son objectif est de faire le pont entre la statistique et la programmation afin que les lecteurs comprennent et autonomisent leur pratique. L’approche permet à la fois une meilleure compréhension des statistiques à l’aide du logiciel, mais constitue également un motif pour débuter la programmation.Il vise un public autant intéressé à s’initier à R qu’à en connaître davantage sur la statistique, mais surtout un lecteur qui souhaite maîtriser les deux à la fois.Si vous avez des commentaires ou des suggestions, n’hésitez pas à les partager. C’est ouvrage est en construction. Il est imparfait et contient malheureusement des erreurs. Vous pouvez m’écrire à l’adresse suivante : pier-olivier.caron[]teluq.ca pour toutes suggestions d’amélioration.","code":""},{"path":"index.html","id":"remerciements","chapter":"Bienvenue!","heading":"Remerciements","text":"Les étudiants et étudiantes de la classe PSY7105 d’automne 2021.","code":""},{"path":"préface.html","id":"préface","chapter":"Préface","heading":"Préface","text":"La méthode scientifique impose aux expérimentateurs de tirer des conclusions avec minutie sur leur objet d’étude. Toutefois, ils font confiance à d’autres pour des tâches qui dépassent leur propre champs de compétence, et ce, avec raisons, car celles-ci peuvent être fort compliquées et nécessitées des formations académiques complémentaires. Les statistiques font parties de ces expertises extérieures en sciences humaines et sociales. Pour ne pas faillir de son esprit critique, ne serait-ce qu’un instant, il est nécessaire de comprendre les fondements et l’implantation logiciel. Trop souvent, les expérimentateurs n’ont qu’une brève introduction aux statistiques et font confiance à des logiciels commerciaux pour les calculs sophistiqués. Le tout se déroule sans tracas apparent. Toutefois, une expérience plus soutenue avec ces logiciels révéle qu’ils ont eux-aussi leurs lots de défauts, de bogues et d’erreurs de calcul. Ces erreurs se produisent parfois à l’insu de l’utilisateur. Sans connaître le résultat attendu, comment distinguer le vrai du faux des sorties statistiques?Il faut être naïf pour croire que le code fonctionne tel que prévu.Par exemple, que vaut l’expression \\(-9^{(.5)}\\)? Reconnaître qu’il s’agit de \\(\\sqrt{-9}\\) indique immédiatement que le résultat n’est pas réel (c’est un nombre complexe). Si ce calcul est demandé à R,celui-ci retourne la réponse \\(-3\\) à cause de la priorité des opérations, \\(-(9)^{.5}\\). Était-ce la réponse désirée?Des idiosyncrasies computationnelles se retrouvent dans tous les logiciels. En pratique, le programmeur doit en avoir conscience et ce type de problème devrait rester trivial…Bennett et al. (2010) montrent que les saumons de l’Atlantique décédés ont des cognitions sociales. Ils placent sous imagerie par résonance magnétique fonctionnelle (IRMf) le saumon pendant une tâche de reconnaissance des émotions chez les humains. La Figure 0.1 montre le résultat de leur analyse. Soit ils sont tombés sur une découverte étonnante en termes de cognition ichtyologique post-mortem, soit quelque chose cloche en ce qui concerne l’approche statistique utilisée.\nFigure 0.1: Cognitions sociales d’un saumon atlantique décédés. Tirés de Bennett et al. 2010, p.4\nLes points rouges de la Figure 0.1 indiquent les cognitions sociales du saumon. Plus sérieusement, ces chercheurs critiquent l’absence de contrôle de l’erreur de type (faux positif) lors de comparaisons multiples. Ce problème est d’autant plus important avec des humains sous IRMf, car ces situations comportent beaucoup de bruits statistiques, contrairement au saumon décédé qui n’en produit pas du tout. Les auteurs montrent bien qu’en absence de ce contrôle, l’analyse peut fournir des résultats farfelus. Il revient à l’expérimentateur de bien utiliser les analyses statistiques et de savoir ce que le logiciel fait et ne fait pas.L’étude de Bennett et al. (2010) n’est qu’un canular. Les auteurs ont remporté le prix Ig Nobel en 2012, un prix scientifique pour de vrais chercheurs ayant réalisé une vraie étude faisant d’abord rire, puis réfléchir. En pratique, ce type de problème devrait rester trivial…Eklund et al. (2016) tentent de valider les méthodes statistiques derrière l’IRMf à l’aide de données réelles. Ils utilisent des données d’IRMf provenant de 499 personnes saines et en état de repos pour effectuer 3 millions d’analyses de groupes de tâches. En utilisant ces données nulles avec différents plans expérimentaux, ils estiment l’incidence des résultats significatifs (en théorie fixé à 5% de faux positifs pour un seuil de signification de 5%). Toutefois, ils constatent que les progiciels les plus courants pour l’analyse de l’IRMf entraînent des taux de faux positifs allant jusqu’à 70%. Ces résultats remettent en question la validité d’un certain nombre d’études utilisant l’IRMf et ont des conséquences importantes sur l’interprétation des résultats de neuro-imagerie.Cette étude fait un tollé dans la littérature scientifique. Bien que certains aspects de l’étude soient attaquables, il n’en demeure pas moins qu’elle met en lumière le danger de faire confiance aveuglément aux analyses statistiques.L’objectif de ce court texte n’est pas de discrédité les analyses statistiques ou les chercheurs qui les emploient. Ce n’est pas non plus de concrétiser l’adage : faire dire n’importe quoi à une statistique (c’est plus facile de faire dire n’importe quoi à des mots, par ailleurs). Les statistiques sont très utiles et fournissent de l’information qu’il est impossible d’obtenir autrement. Il s’agit d’une mise en garde justifiant en partie l’approche pédagogique de ce livre.Il est dit en partie, car le seconde, en filigrane de ce livre, repose sur la double approche d’apprentissage statistique et programmation. Il s’agit de la symbiose entre les deux. D’une part, rendre accessible et efficace des calculs parfois laborieux et ennuyants et, d’autre part, de fournir un objectif de programmation afin de susciter l’intérêt et l’engagement. En programmant des modèles statistiques, le lecteur une plus grande emprise sur ceux-ci. Il peut mieux les utiliser, les étudier et les modifier. Il peut s’en servir pour connaître et apprendre davantage. Certaines boîtes noires resteront irrémédiablement des boîtes noires pour le lecteur, mais plus elles seront démustifiées, mieux il comprendra ce qui se cache derrière les algorithmes statistiques.La première section de ce livre justifie et explique le logiciel R et montre comment l’installer ainsi que quelques rudiments statistiques qui seront fort utiles pour le reste de la lecteur. La seconde section couvre ce qui sera probablement le plus important pour l’utilisateur : l’importation, la gestion et la visualisation des données. La troisième section explique quelques rudiments fondamentaux des statistiques ainsi que des analyses de bases.TODO","code":"-9^(.5)\n> [1] -3"},{"path":"commencer.html","id":"commencer","chapter":" 1 Commencer","heading":" 1 Commencer","text":"Dans cette section, le logiciel R est brièvement décrit ainsi qu’une justification, plus spécifiquement ces avantages, en plus de pointer quelques options complémentaires (RStudio) pour son utilisation. Par la suite, les étapes pour installer et commencer avec les logiciels R et RStudio sont présentées.","code":""},{"path":"commencer.html","id":"r","chapter":" 1 Commencer","heading":"1.1 R?","text":"R (R Core Team, 2022) est un logiciel de programmation statistique libre-accès et un environnement pour la computation statistique et l’affichage graphique. Il s’agit d’un projet GNU qui est similaire au langage et à l’environnement S, développés aux Laboratoires Bell (anciennement &T, aujourd’hui Lucent Technologies) par John Chambers. Créé par Ross Ihaka et Robert Gentleman, R fournit une grande variété de techniques statistiques (modélisation linéaire et non linéaire, analyses statistiques classiques, analyse de séries chronologiques, classification) et graphiques, et est hautement extensible.R est un logiciel basé sur la syntaxe plutôt qu’une approche pointer-et-cliquer (point--click) comme les logiciels traditionnels. Il peut être plus effrayant ou apparaître trop complexe pour un nouvel utilisateur, mais une fois apprivoisée, cette bête démontre un bien meilleur potentiel que ce soit en automatisation, en personnalisation, en production de figure de haute qualité, etc. R l’avantage de mettre en plein contrôle ses utilisateurs. Bref, c’est une créature qu’il vaut la peine de maîtriser.","code":""},{"path":"commencer.html","id":"pourquoi-r","chapter":" 1 Commencer","heading":"1.2 Pourquoi R?","text":"Les logiciels traditionnelles suspendent trop souvent la réflexion critique. Ils sont dociles. L’usager clique sur les bonnes options et obtient les résultats désirés (espérons-le!). En échange d’une expérience “simple et intuitive”, ils compromettent l’épanouissement de l’utilisateur et cloîtrent l’analyse statistique dans une boîte noire, un programme dont le fonctionnement ne peut être connu. Sont évacuées toutes connaissances des analyses, seules les entrées et les sorties sont pertinentes.En se limitant à ces logiciels, les utilisateurs sont également à la merci des compagnies qui les distribuent. Celles-ci maintiennent des prix exorbitants pour des licences annuelles, malgré le faible soutien technique, la désuétude ou le manque de mises à jour, la présence de bogues informatiques. Ces problèmes sont monnaie courante bien que la licence ne soit pas de la petite monnaie.Contrairement aux logiciels traditionnels, R permet de réaliser les analyses, mais aussi de les programmer soi-même, de générer des données propres à un modèle, de rester à jour sur les nouvelles tendances et les découvertes en méthodologie de la recherche, de partager aisément les connaissances et la reproduction d’analyses sophistiquées, et tout cela, gratuitement. Évidemment, cela n’est possible que par l’immense communauté derrière le logiciel.R est complètement gratuit. Il est le logiciel le plus utilisé parmi les scientifiques de sciences de données, statisticiens, etc. Il est l’exemple ultime d’une plateforme communautaire qui fait mieux que les compétiteurs commerciaux. De plus en plus de personnes migrent vers R, mais peu de ses utilisateurs quittent le logiciel vers un autre. Et plus de personnes se joignent à la communauté, plus il y de documentation, d’aide, de soutien, que ce soit sous forme de livres, de vidéos, d’article, d’ateliers, de formations, de forum. L’expérience R devient de plus en plus accessible aux nouveaux immigrants. Comme le code source est ouvert, ses utilisateurs collaborent à la création de modules augmentant ses capacités qui permettent de résoudre des problèmes de plus en plus sophistiqués (et pas juste en statistiques!). Ces modules sont également gratuits et il y en littéralement des milliers.L’avantage de maîtriser R, plus spécifiquement de l’utilisation de syntaxe, pour l’expérimentateur est de rendre l’analyse statistique facilement transmissible entre expérimentateur (facilite grandement la collaboration), reproductible (vérification et collaboration), répétable (pour de nouvelles données ou expériences), et ajustable (pour de nouveaux scénarios). Quelqu’un d’autre peut jeter un coup d’oeil à l’analyse réalisée et voir exactement ce qui s’est produit, tant dans la gestion du jeu de données que l’analyse et la génération de graphiques. L’analyse peut être reproduite par les pairs, voire répéter si des données supplémentaires ont été recueillies ou si une nouvelle expérience été réalisée.Même si R est un langage de programmation et que cela peut en intimider plus d’un, il est relativement intuitif à apprendre et assez simple d’utilisation dans la mesure où les ressources appropriées pour apprendre sont accessibles à l’utilisateur. L’apprentissage de la programmation, même si ce n’est pas en R, permet de mieux comprendre le fonctionnement des ordinateurs en plus de reconsidérer le dogmatisme de tout-puissant “algorithmes”. Il permettra aussi à l’utilisateur lorsqu’il saura suffisamment maîtriser la bête à créer lui-même la syntaxe qu’il lui permettra de résoudre les problèmes sur lesquels il s’intéresse.Enfin, R possède un incroyable moteur pour la visualisation de données et de capacités graphiques. Aucun autre logiciel ne lui arrive à la cheville.","code":""},{"path":"commencer.html","id":"installer-r","chapter":" 1 Commencer","heading":"1.3 Installer R","text":"R est compatible pour Windows, Mac et Linux. Pour télécharger le logiciel, il faut se rendre sur le site http://www.r-project.org et sélectionner les hyperliens “download R” ou “CRAN mirror”. Il faut ensuite choisir un mirror de son pays d’origine. Par la suite, la page permet de choisir la version appropriée à son système d’exploitation. Il faut alors suivre les indications.Il est principalement utilisé en anglais bien qu’peut le définir en français. La plupart de l’aide est en anglais surtout celle pour les analyses spécialisées. Toutefois, il y beaucoup de ressources accessibles en ligne en français pour une introduction débutante et intermédiaire.","code":""},{"path":"commencer.html","id":"rstudio","chapter":" 1 Commencer","heading":"1.3.1 RStudio","text":"Bien que R puisse être utilisé seul, son aspect rudimentaire peut en inquiéter plus d’un. Le logiciel RStudio permet une utilisation plus fluide et intuitive pour les usagers ayant peu ou pas d’expériences en programmation. RStudio est un environnement de développement intégré (IDE pour integrated development environment) pour R. Il comprend une console, un éditeur de mise en évidence de la syntaxe qui prend en charge l’exécution directe du code, ainsi que des outils de traçage, d’historique, de débogage et de gestion de l’espace de travail. Le logiciel est gratuit et libre-accès pour une utilisation personnelle. Il comporte aussi une version commerciale qui nécessite un certain déboursement de fonds.Pour télécharger le logiciel, il faut se rendre sur le site http:rstudio.com et naviguer jusqu’au téléchargement du logiciel. La version gratuite de RStudio est téléchargeable spécifiquement sur cette page https://www.rstudio.com/products/rstudio/download/#download. RStudio est uniquement disponible en anglais.","code":""},{"path":"commencer.html","id":"les-avantages-de-rstudio","chapter":" 1 Commencer","heading":"1.3.1.1 Les avantages de RStudio","text":"Les avantages de RStudio, comparativement à l’utilisation de R, sont d’offrir une gestion de la console, du script, de l’environnement des variables et des documents externes en une seule interface (et bien d’autres!). En plus de la console, RStudio permet l’édition de syntaxe qui peut être commandée ligne par ligne en utilisant CTRL + Enter (Windows) ou CMD + Enter (MacOS), ce qui s’avére fort utile lors de la programmation de fonction ou d’analyses de données. RStudio affiche également les variables en mémoire dans le menu Global Environment, ce qui permet de suivre l’état de la programmation (quelle variable existe ou n’existe pas). Enfin, RStudio affiche dans un quatrième menu les fichiers dans le directoire des fichiers R ce qui permet de voir notamment les jeux de données, mais aussi d’autres fonctions ou scripts. C’est également à cet endroit où l’aide (help) est fournie et les figures (plot) affichées.Tout le contenu du présent ouvrage est réalisable avec R ou avec RStudio. L’usage de ce dernier sera toutefois plus agréable aux lecteurs.Oh! Un autre avantage de Rstudio est qu’il peut être utiliser comme un éditeur de texte. D’ailleurs, cet ouvrage est complètement rédigé avec RStudio (avec les packages Rmarkdown et bookdown).","code":""},{"path":"commencer.html","id":"autres-options","chapter":" 1 Commencer","heading":"1.3.2 Autres options","text":"Il existe plusieurs interfaces utilisateurs graphiques (GUI pour Graphical User Interface) pour R comme R Commander (certainement l’option la plus connue en sciences humaines et sociales) ou JASP, mais aussi plusieurs autres. Ces deux options sont gratuites et libre-accès. Il existe aussi d’autres options payantes. Ces logiciels visent l’utilisation de R par une approche pointer-et-cliquer (point--click) au travers les analyses plutôt que de recourir à la syntaxe. Ces options plus intuitives pour le nouvel utilisateur ont parfois des effets limitatifs pour des analyses plus avancées et ont comme effet indésirable de promouvoir la boîte noire statistique.","code":""},{"path":"commencer.html","id":"démarrer-r-ou-rstudio","chapter":" 1 Commencer","heading":"1.4 Démarrer R ou RStudio","text":"À l’ouverture de R, illustrée à la Figure 1.1, le logiciel présente la console, une interface très rudimentaire (et assez déstabilisante pour un logiciel pourtant si promu). Une fois le logiciel ouvert, l’application offre une invitation discrète à écrire des commandes. Le symbole > au bas de la console est une invite (prompt) indiquant où taper les commandes. C’est à cette ligne de commande que les expressions sont immédiatement évaluées.\nFigure 1.1: Ouverture (effrayante!) de la console R\nÀ titre d’exemple, la console peut être utilisée comme une calculatrice glorifiée.Plusieurs nouveaux utilisateurs se limitent à ouvrir R y faire quelques calculs, puis le referme et ne l’ouvre plus jamais. Ce livre tente d’emmener le lecteur un peu plus loin.Dans la Figure 1.1, R est défini en français. Cela n’pour effet que de modifier le menu déroulant (“Fichier, Edition, etc.”) au sommet du logiciel et les différentes options de ce menu. Le fonctionnement reste le même (les fonctions ne sont pas traduites, par exemple).La console R n’étant pas un éditeur de texte, il faudra enregistrer la syntaxe utilisée lors d’une séance pour la conserver. Le logiciel offre une option d’écriture de script intégré, mais n’est pas lié directement à la console. Il faudra donc se résoudre à abuser du copier-coller ou à sourcer le script (tâche plus ardue pour les nouveaux utilisateurs). Plusieurs éditeurs de texte sont utiles ou même construits pour directement travailler avec R, le plus connu étant certainement RStudio, déjà mentionné. L’environnement intégré est beaucoup plus fonctionnel.La Figure 1.2, montre l’interface de RStudio, déjà un peu moins intimidante que celle de R. À l’ouverture de RStudio, quatre types de fenêtres sont disponibles : la console (cadran inférieur gauche), les scripts (cadran supérieur gauche), l’environnement (cadran supérieur droit) et l’affichage (cadran inférieur droit). L’emplacement de ces cadrans peut être modifié selon les désirs de l’utilisateur.\nFigure 1.2: Ouverture (moins effrayante) de RStudio\nLa console RStudio est identique à la console usuelle retrouvée avec R. Elle sert les mêmes fonctions.Le script est un éditeur de texte dans lequel de la syntaxe est rédigée, sauvegardée, manipulée, et testée. S’il n’est pas ouvert ou s’il faut ouvrir un script supplémentaire, il faut procéder par le menu déroulant File > New File > R Script ou bien CTRL + Shift + N. Il peut y avoir plusieurs scripts ouverts simultanément.L’environnement global permet de connaître les variables et fonctions maison en mémoire vive. L’onglet History montre les dernières lignes de code commandées (non affichées dans la figure - il suffit de cliquer sur l’onglet à côté de Environment).Le cadran inférieur droit montre le fichier de travail R qui contiendra ordinairement les fichiers de travail actifs (scripts, jeu de données, fonctions maison, etc.). Si aucun directoire n’est demandé explicitement par R (par exemple, si un jeu de donnée doit être téléchargé), le logiciel cherche par défaut dans le fichier actif pour télécharger les fichiers demandés. Il faut s’assurer d’être dans le bon fichier, car cela cause quelques soucis à l’occasion. RStudio permet de travailler par projet, ce qui est très pratique tant sur le plan de l’organisation du programmeur que pour la gestion des directoires de travail. Pour ouvrir un projet, il faut procéder par le menu déroulant File > New Project… et suivre les indications et ajuster selon les besoins du projet.","code":"2 + 2\n> [1] 4"},{"path":"commencer.html","id":"les-scripts","chapter":" 1 Commencer","heading":"1.5 Les scripts","text":"Ce qu’il importe le plus avec R, et ce qui fait resplendir RStudio, est de conserver la syntaxe d’une session à l’autre. Le logiciel ne le fait pas très bien. Il faudra sauvegarder dans un script les expressions et le code utilisés. Ces fichiers ont souvent comme extension “.R” et permettent de conserver, voire partager la syntaxe. Il est possible d’y ajouter des commentaires pour de futures utilisations. Tout éditeur de texte permet la sauvegarde de syntaxe, certains sont mieux que d’autres pour une utilisation avec R.RStudio contient déjà un panneau contenant un script qu’il est possible de sauvegarder et de rouler directement dans la console. Ce dernier est directement lié et il est possible de rouler la syntaxe ligne par ligne avec CTRL + Enter (Windows) ou CMD + Enter (MacOS).","code":""},{"path":"programmer.html","id":"programmer","chapter":" 2 Programmer","heading":" 2 Programmer","text":"Une fois R (ou RStudio) ouvert, qu’est-il possible de réaliser? Dans les prochaines sections, les différents éléments de programmation permettant la création et la manipulation de données sont présentés afin de dépasser le cadre de l’utilisation calculatrice.","code":""},{"path":"programmer.html","id":"les-variables","chapter":" 2 Programmer","heading":"2.1 Les variables","text":"Pour manipuler des données, il faut recourir à des variables. Pour attribuer une valeur à une variable, il faut assigner cette valeur avec <- (ALT + -) ou =, par exemple,où est maintenant égale à 2. La première ligne assigne (enregistre) la valeur 2 à . La deuxième ligne, indique à la console R d’imprimer le résultat pour le voir. Par la suite, peut être utilisée dans des fonctions, des calculs ou analyses plus complexes. De surcroît, peut devenir une fonction, une chaîne de caractère (string) ou un jeu de données.Conventionnellement, les puristes de R recommandent l’usage de <- plutôt que = pour l’assignation. Il y quelques nuances computationnelles entre les deux, mais qui échappent irrémédiablement aux néophytes et même aux usagers intermédiaires. Par tradition, <- prévaudra dans ce livre.Pour nommer des variables, seuls les caractères alphanumériques peuvent être utilisés ainsi que le tiret bas _ et le .. Les variables ne peuvent commencer par un nombre.Réassigner une valeur à une variable déjà existante écrase la valeur précédente.La sortie produit 3 et non plus 2.Cette remarque est importante, car elle signifie que des fonctions sont écrasables en nommant des variables. Il faut ainsi éviter de nommer des variables avec des fonctions utilisées par R, notamment l’utilisation des noms suivants.Certains mots sont tout simplement interdits d’utilisation.","code":"a <- 2\na\n> [1] 2a <- 2\na <- 3\na\n> [1] 3\nc; q; t; C; D; I; T; F; pi; mean; var; sd; length; diff; repTRUE; FALSE; break; for; in; if; else; while; function; Inf; NA; NaN; NULL"},{"path":"programmer.html","id":"les-opérateurs-arithmétiques","chapter":" 2 Programmer","heading":"2.2 Les opérateurs arithmétiques","text":"La première utilisation d’un nouvel usager de R est généralement d’y recourir comme calculatrice. Les opérateurs arithmétiques de base comme l’addition +, la soustraction -, la multiplication *, la division / , et l’exposant ^ sont intuitivement disponibles.Évidemment, ces opérateurs fonctionnent sur des variables numériques.Les deux premières lignes assignent des valeurs à et b, puis la troisième imprime la division. L’absence de marqueur <- ou = indique à R d’imprimer la réponse dans la console. Si le résultat / b est assigné à une variable, alors aucun résultat n’est affiché, bien que la variable contienne la réponse.Il n’y aucune réponse d’affichée. Maintenant, si la variable resultat est demandée, R affiche son contenu.D’autres fonctions sont aussi très utiles. Par exemple, la racine carrée sqrt() (qui n’est rien d’autre que ^(.5)) et le logarithme naturel log(). Il suffit d’insérer une variable ou une valeur à l’intérieur d’une de ces fonctions pour en obtenir le résultat.","code":"2 + 2\n> [1] 4\n1 / 3\n> [1] 0.333\n2 * 3 + 2 ^ 2\n> [1] 10a <- 1\nb <- 10\na / b\n> [1] 0.1\nresultat <- a / bresultat\n> [1] 0.1sqrt(4)\n> [1] 2\n4^(1/2)\n> [1] 2\nlog(4)\n> [1] 1.39"},{"path":"programmer.html","id":"les-commentaires","chapter":" 2 Programmer","heading":"2.3 Les commentaires","text":"Les scripts R peuvent contenir des commentaires. Ceux-ci sont désignés par le désormais célèbre #. Une ligne de script commençant par ce symbole est ignorée par la console. Ces commentaires permettent aussi bien de préciser différentes étapes d’un script, que d’expliquer la nomenclature des variables ou encore d’expliquer une fonction, ses entrées, ses sorties. Les commentaires sont extrêmement utiles, car les annotations peuvent souvent sauver énormément de temps et d’effort lors d’utilisations ultérieures.Dans cet exemple, la première ligne est ignorée. Autrement, la console R produit une erreur, car cette ligne est pour le logiciel pur charabia!","code":"# La variable resultat est le quotient des variables a et b\nresultat <- a / b\nresultat\n> [1] 0.1"},{"path":"programmer.html","id":"les-chaînes-de-caractère","chapter":" 2 Programmer","heading":"2.4 Les chaînes de caractère","text":"La plupart du temps, les variables utilisées sont numériques, c’est-à-dire qu’elles contiennent des nombres. Parfois, les données sont des mots, c’est-à-dire, des chaînes de caractères (string). Les chaînes de caractères sont définis par le double apostrophe \"...\", où remplace les trois points par les mots désirés.1","code":"titre <- \"Bonjour tout le monde!\"\ntitre\n> [1] \"Bonjour tout le monde!\""},{"path":"programmer.html","id":"concaténer","chapter":" 2 Programmer","heading":"2.5 Concaténer","text":"Par défaut, R ne peut assigner qu’une valeur à une variable. Pour grouper des éléments ensemble, c’est-à-dire, pour créer des jeux de données, des vecteurs, des matrices, des listes, il faudra utiliser des fonction de concaténation, dont voici une liste des plus utiles avec quelques exemples, de la plus stricte (vecteur) à la plus flexible (liste).","code":""},{"path":"programmer.html","id":"créer-un-vecteur","chapter":" 2 Programmer","heading":"2.5.1 Créer un vecteur","text":"Une fonction fort utile permet de joindre des valeurs dans une seule variable. Précédemment, l’assignation d’une valeur à des variables se limitait à une chaîne de caractères ou à une valeur numérique. La fonction concaténer c() (ou combiner, créer) met plusieurs éléments (deux ou plus) dans une seule variable. Son est de vectoriser les arguments. Chaque élément est délimité par une virgule ,.Elle fonctionne également avec les chaînes de caractères.Et les deux.La fonction c() est strict sur les arguments, car elle leur accorde le même attribut. Par exemple, phrase ne contient que des chaînes de caractères. Les valeurs 1 et 2 ont perdu leur classe numérique (elles ne sont plus utilisables comme nombre2). Cela se remarque par les guillemets anglophones autour des valeurs \"1\" et \"2\" imprimées.Il faudra également faire attention aux arguments passés à la fonction c(), car celle-ci vectorise les arguments. Autrement dit, elle crée des vecteurs (une ligne en quelque sorte) avec les entrées fournies, peu importe leur structure de départ. Par exemple, un jeu de données passant par c() devient une seule ligne (un seul vecteur) de valeurs. Les fonctions cbind() et rbind() permettent de joindre des colonnes et des lignes, respectivement.","code":"valeurs <- c(-5, 5)\nvaleurs\n> [1] -5  5texte <- c(\"Bonjour\", \"tout\", \"le\", \"monde\")\ntexte\n> [1] \"Bonjour\" \"tout\"    \"le\"      \"monde\"phrase <- c(1, \"Chat\", 2, \"Souris\")\nphrase\n> [1] \"1\"      \"Chat\"   \"2\"      \"Souris\""},{"path":"programmer.html","id":"créer-une-matrice","chapter":" 2 Programmer","heading":"2.5.2 Créer une matrice","text":"La fonction matrix() crée des matrices, comme des matrices de covariances, par exemple. La fonction utilise trois arguments, une matrice de nombre à entrer dans la matrice, un nombre de colonnes et un nombre de lignes. La fonction utilise le recyclage, ce qui est utile à certaines occasions.Les matrices sont une formes de jeu de données dans lequel tous les éléments partagent le même attribut (tous numériques, caractères, logiques, etc.).Une note devancée sur l’utilisation de 1:3 et 1:16 du code précédent qui permettent de générer des séquences de nombre simplement.","code":"# Une matrice de 0 de taille 3 x 3\nmatrix(0, ncol = 3, nrow = 3)\n>      [,1] [,2] [,3]\n> [1,]    0    0    0\n> [2,]    0    0    0\n> [3,]    0    0    0\n\n# Une matrice contenant les nombres 1:3 pour une matrice 3 x 3\nmatrix(1:3, ncol = 3, nrow = 3)\n>      [,1] [,2] [,3]\n> [1,]    1    1    1\n> [2,]    2    2    2\n> [3,]    3    3    3\n\n# Si la séquence préférée est de gauche à droite plutôt\n# que de bas en haut\nmatrix(1:3, ncol = 3, nrow = 3, byrow = TRUE)\n>      [,1] [,2] [,3]\n> [1,]    1    2    3\n> [2,]    1    2    3\n> [3,]    1    2    3\n\n# Une matrice avec un nombre d'entrées égale au nombre de cellules\nmatrix(1:16, ncol = 4, nrow = 4)\n>      [,1] [,2] [,3] [,4]\n> [1,]    1    5    9   13\n> [2,]    2    6   10   14\n> [3,]    3    7   11   15\n> [4,]    4    8   12   16"},{"path":"programmer.html","id":"créer-un-jeu-de-données","chapter":" 2 Programmer","heading":"2.5.3 Créer un jeu de données","text":"Un jeu de données (data.frame) est un peu comme l’extension de la matrice. La différence étant que les éléments entre les colonnes peuvent partager des attributs différents. Ainsi chaque ligne représente une unité (un participant, un objet) et chaque colonne représente une dimension (informations ou variable) différente de cette objectif. La fonction data.frame() permet de créer de tel objet. La fonction prend comme un argument une série de vecteurs. Des noms peuvent être attribués au colonnes qui correspondent à des variables.En utilisant nom.de.variable = vecteur à l’intérieur de data.frame(), les noms des colonnes deviennent nom.de.variable. Cela permettra une plus grande flexibilité lorsqu’il faudra [gérer] et manipuler les données.Comme les matrices, les jeux de données ont aussi une restriction. Alors que les jeux de données libèrent la contrainte d’avoir des objets de même attributs entre les colonnes (variables), ils doivent être créés avec des vecteurs de même longueur. Autrement dit, chaque colonne doit avoir exactement le même nombre de lignes. Parfois, R *recycle/ pour combler les éléments. Il faut donc porter une attention particulière afin de vérifier si c’est bien l’intention de l’utilisateur ou non.","code":"# Quelques variables\nvar1 <- c(\"Éloi\", \"Laurence\")\nvar2 <- c(6, 3)\nvar3 <- c(TRUE, TRUE)\n\n# Entrer de trois vecteurs non identifiés\njd1 <- data.frame(var1, var2, var3)\n\n# Entrer de trois vecteurs identifiés\njd2 <- data.frame(nom = var1, age = var2, enfant = var3)\n\n# Comparer\njd1 ; jd2\n>       var1 var2 var3\n> 1     Éloi    6 TRUE\n> 2 Laurence    3 TRUE\n>        nom age enfant\n> 1     Éloi   6   TRUE\n> 2 Laurence   3   TRUE"},{"path":"programmer.html","id":"créer-une-liste","chapter":" 2 Programmer","heading":"2.5.4 Créer une liste","text":"Une troisième option pour stocker de informations dans une seule variable est d’avoir recourt aux listes. La liste libère à la fois l’utilisateur des objets de mêmes attributs et de même longueur. Ainsi, une liste, peut contenir des vecteurs, des matrices, des jeux de données et même d’autres listes.Pour créer une liste, il faut utiliser la fonction list(). Comme data.frame(), des noms de colonnes peuvent être donnés pour chaque liste pour faciliter la manipulation ultérieure de la liste.L’utilisation de listes est une caractéristique prédominante avec R. Par exemple, R ne peut sortir qu’une variable par fonction. Si la fonction doit retourner plusieurs éléments, ceux-ci doivent se retrouver dans une liste. Ce qui restera plus nébuleux pour le lecteur, c’est que l’optimisation de R se fait par listes. Cela sera noté aux moments appropriés.","code":"#Quelques variables\nvar1 <- c(\"chat\", \"chien\")\nvar2 <- 1:10\n\n# Entrer de deux vecteurs non identifiés\njd1 <- list(var1, var2)\n\n# Entrer de deux vecteurs identifiés\njd2 <- list(animal = var1, nombre = var2)\n\n# Comparer\njd1 ; jd2\n> [[1]]\n> [1] \"chat\"  \"chien\"\n> \n> [[2]]\n>  [1]  1  2  3  4  5  6  7  8  9 10\n> $animal\n> [1] \"chat\"  \"chien\"\n> \n> $nombre\n>  [1]  1  2  3  4  5  6  7  8  9 10"},{"path":"programmer.html","id":"référer-à-des-sous-éléments","chapter":" 2 Programmer","heading":"2.6 Référer à des sous-éléments","text":"Avec des variables contenant plusieurs valeurs, il peut être utile de référer à une seule valeur ou un ensemble de valeurs de la variable. Les crochets [] à la suite du nom d’une variable permettent d’en extraire les valeurs désirées sans tout sortir l’ensemble.Dans le premier exemple, seul un élément est demandé. Dans le deuxième exemple, la commande 1:3 produit la série de \\(1,2,3\\) et en extrait ces nombres. Dans le dernier exemple, la fonction c() est astucieusement utilisée pour extraire les éléments \\(2\\) et \\(4\\). Le quatrième exemple montre comment retirer un élément en utilisant des valeurs négatives et le cinquième exemple montre comment retirer des éléments.La section Manipulation de données montre comment référer à des sous-éléments de jeux de données, de matrices et de listes de façon plus avancées.","code":"# Un exemple de vecteur\nphrase <- c(1, \"Chat\", 2, \"Souris\")\n\n# Extraire le premier élément de la variable phrase\nphrase[1]\n> [1] \"1\"\n\n# Extraire les éléments 1, 2 et 3\nphrase[1:3]\n> [1] \"1\"    \"Chat\" \"2\"\n\n# Extraire les éléments 2 et 4\nphrase[c(2,4)]\n> [1] \"Chat\"   \"Souris\"\n\n# Ne pas extraire l'élément 1\nphrase[-1]\n> [1] \"Chat\"   \"2\"      \"Souris\"\n\n# Ne pas extraire les éléments 1 et 3\nphrase[-c(1, 3)]\n> [1] \"Chat\"   \"Souris\""},{"path":"programmer.html","id":"les-packages","chapter":" 2 Programmer","heading":"2.7 Les packages","text":"L’utilisation de packages (souvent nommées bibliothèques, modules, paquets ou paquetage en français - ici, l’usage de package est maintenu) est l’attrait principal de R. Pour éviter l’anglicisme, Antidote (Druide informatique INC., 2022) suggère forfait, achat groupé ou progiciel (ce dernier étant certainement le terme approprié).Les packages sont de regroupement de fonctions. C’est certainement l’aspect qui le plus contribué au succès et à sa dissémination de R. Il s’agit de la mise en commun d’un effort collaboratif afin de créer des fonctions et de les partager librement entre les usagers. Le téléchargement de base de R offre déjà quelques packages rudimentaires (comme base qui offre des fonctions comme sum() ou stat qui offre des fonctions comme mean() et var()), mais qui suffisent rarement lorsque des analyses plus avancées ou plus spécialisées sont nécessaires.L’une des forces des packages est qu’ils sont fournis généralement avec un bon manuel d’utilisation. Plusieurs contributeurs leur sont associés (avec un responsable). Ils sont maintenus régulièrement. Le soutien des responsables est parfois aisé à obtenir et les auteurs de ces packages sont motivés à maintenir les packages opérationnels et aux bénéfices de tous. La faiblesse des packages est qu’il s’agit malheureusement de généralement. Il arrive que certains packages produisent des erreurs de calcul, qu’ils soient laissés en désuétude par leurs auteurs, qu’ils aient migrés sous une autre forme, ou que de meilleures options soient disponibles sans aucune notice. Cela va sans dire, ce problème concerne les logiciels traditionnels également. Il s’agit toutefois d’un enjeu moindre, car les packages sont souvent recommandés par des collègues, des autorités dans leur domaine respectif ou des ouvrages de référence, ce qui aura comme tendance de promouvoir les meilleurs packages. Pas toujours. Il faut rester critique et ne pas de laisser tromper par une boîte noire.Une dernière faiblesse : les packages agissent parfois en boîte noire, c’est-à-dire qu’ils court-circuitent la réflexion de l’utilisateur qui leur fait confiance. Il peut être parfois difficile de savoir ce que les fonctions produisent exactement. Au contraire des logiciels traditionnels, ces boîtes noires peuvent dans la plupart des cas être accessibles directement, elles sont liés en plus à des articles scientifiques ou de la documentation qui permet dans comprendre les tenants et aboutissants.","code":""},{"path":"programmer.html","id":"installer-des-packages","chapter":" 2 Programmer","heading":"2.7.1 Installer des packages","text":"Pour installer un package, il faut utiliser la fonctionoù les \"...\" doivent être remplacé par le nom du package. Il est important de bien inscrire le nom du package entre guillemet anglophone. Il est aussi possible de sélectionnerTools;\nInstall Packages…puis de nommer le package sous l’onglet package. Avec R il faudra auparavant choisir un miroir (sélectionner un pays), ce qui n’est pas nécessaire avec RStudio. Une fois téléchargé, il n’est plus nécessaire de refaire cette étape à nouveau, à l’exception de potentielles et ultérieures mises à jour lorsqu’elles devront être effectuées.","code":"\ninstall.packages(\"...\")"},{"path":"programmer.html","id":"appeler-un-package","chapter":" 2 Programmer","heading":"2.7.2 Appeler un package","text":"Ce qui n’est pas des plus intuitif avec R, c’est qu’une fois le package téléchargé, il n’est pas directement utilisable. Il faut d’abord l’appeler avec la fonction library().Cette étape doit être faite à chaque ouverture de R. Cela permet de ne pas mettre en mémoire trop de package simultanément. Il importe d’indiquer tous les packages utilisés en début de script sans quoi des erreurs, comme l’absence de fonctions, sont produites.Une technique à laquelle l’utilisateur peut avoir recourt lorsqu’il souhaite n’utiliser qu’une fonction spécifique d’un package est l’utilisation des :: débutant par le nom du package suivi par le nom de la fonction, comme MASS::mvrnorm(). La fonction s’utilise de façon usuelle. En utilisant ::, il n’est pas nécessaire d’appeler le package avec la fonction library(). Il faut toute fois que le package soit bel et bien installer.","code":"\nlibrary(\"...\")"},{"path":"programmer.html","id":"les-fonctions","chapter":" 2 Programmer","heading":"2.8 Les fonctions","text":"R offre une multitude de fonctions et permet également à l’usager de bâtir ses propres fonctions (fonctions maison). Elles permettent d’automatiser des calculs (généralement, mais peut faire beaucoup plus!). Tout au long de cet ouvrage, les fonctions sont identifiées par l’ajout de parenthèse à leur fin, comme ceci : function(). Ces fonctions ont généralement la forme suivante.Ici, nom est le nom auquel la fonction sera référée par la suite, function est la fonction R qui permet de créer la fonction maison, argument1 et argument2 sont les arguments (les entrées) fournis à la fonction et à partir desquels les calculs sont réalisés, et les accolades {} définissent le début et la fin de la fonction dans le script.Il est bien utile de créer ses propres fonctions bien que R possède une pléthore de fonctions et de packages en contenant encore plus. Toutes les fonctions, qu’elles soient maisons ou déjà intégrées, respectent le même fonctionnement, ce pour quoi il est utile de s’y pencher. Les fonctions maison permettent d’automatiser certains calculs qui sont propres à résoudre les problèmes spécifiques de l’utilisateur et d’être réutilisé ultérieurement.Voici un exemple trivial de fonction, soit la somme de deux nombres.Par défaut, une fonction retourne la dernière ligne calculée si elle n’est pas assignée à une variable. Si le résultat d’une fonction est assigné, la fonction ne retourne pas le résultat dans la console, mais assigne bel et bien la variable.L’utilisation de return() à la fin de la fonction est une bonne pratique, car elle permet d’éviter des problèmes ou des ambiguïtés.","code":"\nnom <- function(argument1, argument2, ...) {\n  \n  # Calcul à réaliser\n  \n}addition <- function(a, b) {\n  \n  a + b\n  \n}\n\naddition(2,3)\n> [1] 5addition2 <- function(a, b) {\n  # Le résultat est assigné à une variable\n  somme <- a + b\n}\n\n# Ne produit pas de sortie\naddition2(100, 241)\n\n# Comme il y a assignation, total n'est pas affichée\ntotal <- addition2(100, 241)\n\n# En roulant total, la sortie affiche bien la sortie de addition2()\ntotal\n> [1] 341addition3 <- function(a, b) {\n  \n  # Le résultat est assigné à une variable\n  somme <- a + b\n  \n  return(somme)\n}\n\n# Les deux fonctions produisent une sortie\naddition3(4, 6)\n> [1] 10\ntotal <- addition3(4, 6)\ntotal\n> [1] 10"},{"path":"programmer.html","id":"définir-une-boucle","chapter":" 2 Programmer","heading":"2.9 Définir une boucle","text":"Pour automatiser certains calculs, il peut être utile de recourir à une boucle (loop) qui permet de répéter plusieurs fois une même opération. Voici l’anatomie d’une boucle.L’élément est la fonction déclarant la boucle. Les renseignements sur les itérations se retrouvent entre les parenthèses. La variable prendra successivement tous les éléments dans () le vecteur à gauche (vec). Tout le contenu de la boucle (ce qui est répété) se retrouve entre les accolades {}, c’est ce qui est calculé à chaque boucle. Dans cet exemple, la boucle se répète \\(k\\) fois, soit de \\(1,2,3,...,k\\), à cause de l’expression 1:k qui correspond à générer un vecteur de \\(1\\) à \\(k\\) (voir la section La séquence). La variable quant à elle change de valeur à chaque itération. Elle prend tour à tour les valeurs \\(1,2,3,...,k\\). La variable peut judicieusement être utilisée dans la boucle afin de profiter ce comportement, notamment pour le classement des résultats. Lorsque la boucle atteint \\(k\\), elle se termine.Il est aussi possible de rédiger la boucle en utilisant uniquement k. Alors, prendra toutes les valeurs contenues dans k. La longueur du vecteur k définit le nombre d’itérations.","code":"\nfor(i in vec){\n  # Calcul désiré\n}\nfor(i in k){\n  # Calcul désiré\n}"},{"path":"programmer.html","id":"les-clauses-conditionnelles","chapter":" 2 Programmer","heading":"2.10 Les clauses conditionnelles","text":"Pour réaliser des opérations sous certaines conditions ou opérer des décisions automatiques, il est possible d’utiliser des arguments conditionnels avec des opérateurs logiques. Par exemple, sélectionner des unités ayant certaines caractéristiques, comme les participants ayant 18 ans et moins, les personnes ayant un trouble du spectre de l’autiste, ou encore par sexe. Il est aussi possible d’utiliser les opérateurs pour définir à quelle condition telle ou telle autre fonction doit être utilisée. Il faudra alors utiliser les arguments logiques.\nTable 2.1: Symboles logiques et leur signification\nR teste si les valeurs de la variable correspondent à l’opérateur logique en les déclarant comme vraies (TRUE) ou fausses (FALSE).Cela peut être utilisé pour référer à des sous-éléments comme abordés précédemment.Ici, toutes les valeurs vraies de l’opérateur logique sont rapportées.Les opérateurs logiques servent également à définir des opérations conditionnelles. La fonction est alors utilisée. Il y trois principales formes : (Si ceci, alors cela), le  else (Si ceci, alors cela, sinon autre chose) et les échelles else else.L’anatomie d’une fonction comporte d’abord la fonction . L’argument entre parenthèses à sa plus simple expression doit être vérifié par vrai (TRUE) ou faux (FALSE). Si l’argument est vrai, alors le calcul désiré est réalisé, autrement le logiciel ignore le calcul de la fonction entre accolades {}.Il est possible d’élaborer cette logique avec la fonction else qui permet de spécifier une suite à la fonction si l’argument est faux (FALSE).Enfin, il est possible d’élaborer un arbre de décision avec toute une échelle de conditionnels.L’arbre de décision peut devenir aussi compliqué que l’utilisateur le désire : chacune des branches peut contenir autant de ramifications que nécessaire.Il peut arriver pour certaines fonctions de devoir spécifier si certains paramètres sont vrais (TRUE) ou faux (FALSE) ou de définir des variables ayant ces valeurs. Lorsque c’est le cas, il est toujours recommandé d’écrire les valeurs logiques tout au long comme TRUE et FALSE, même si R reconnaît T et F, car ces dernières peuvent être réassignées, contrairement aux premières.","code":"valeurs <- 1:6\n# Toutes les valeurs plus grandes que 3.\nvaleurs > 3\n> [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE# Toutes les valeurs plus grandes que 3.\nvaleurs[valeurs > 3]\n> [1] 4 5 6\nif(x){\n  # Opération désirée\n}x <- -2\nif(x < 0){\n  print(\"la valeur est négative\")\n}\n> [1] \"la valeur est négative\"x <- 2\nif(x < 0){\n  print(\"la valeur est négative\")\n}else{\n  print(\"la valeur est positive\")\n}\n> [1] \"la valeur est positive\"x <- 0\nif(x < 0){\n  print(\"la valeur est négative\")\n}else if(x > 0){\n  print(\"la valeur est positive\")\n}else{\n  print(\"la valeur est égale à 0\")\n}\n> [1] \"la valeur est égale à 0\""},{"path":"programmer.html","id":"obtenir-de-laide","chapter":" 2 Programmer","heading":"2.11 Obtenir de l’aide","text":"En utilisant help(nom) ou ?nom, où il faut remplacer nom par le nom d’une fonction ou d’un package, R offre de la documentation. Les fonctions d’aide retournent une page de documentation contenant généralement de l’information sur les entrées et les sorties des fonctions. Certaines sont mieux détaillées que d’autres, tout dépendant de leurs créateurs et des personnes qui maintiennent ces fonctions.Il existe également la fonction ??nom qui produit une liste de toutes fonctions R ayant partiellement l’inscription introduite à la place de nom. Aussi, example(nom) produit un exemple d’une fonction.","code":"\n# Obtenir de l'aide pour la fonction help()\n?help"},{"path":"programmer.html","id":"en-cas-de-pépins","chapter":" 2 Programmer","heading":"2.12 En cas de pépins","text":"Il arrive parfois que le code utilisé ne fonctionne pas, que des erreurs se produisent ou que des fonctions fort utiles demeurent inconnues. Même après plusieurs années d’utilisation, les utilisateurs font encore quotidiennement des erreurs (au moins une!). Un excellent outil est d’utiliser un moteur de recherche, de poser une question à l’aide de quelques mots clés bien choisis, préférablement en anglais, et en y inscrivant “R” ou “R” ou “R”. La plupart du temps, les programmeurs de packages ont une solution sur leur site ou leurs instructions de packages. Il y aussi des plateformes publiques et en ligne, comme StackOverflow, qui collectent questions et réponses sur le codage. D’autres utilisateurs peuvent avoir posé la même question et des auteurs de programmes R et d’autres usagers y ont répondu aux bénéfices de tous. Dans le cas d’une solution introuvable, ces mêmes plateformes permettent de poser de nouvelles questions. Il faudra toutefois attendre qu’un usager plus expérimenté prenne le temps d’y répondre.","code":""},{"path":"calculer.html","id":"calculer","chapter":" 3 Calculer","heading":" 3 Calculer","text":"Dans cette section, les fonctions essentielles couramment utilisées sont présentées en rafale. L’accent est mis sur la définition de la fonction (qu’est-ce qu’elle fait?) et son utilité (à quoi sert-elle?). Pour les fonctions essentielles de nature statistiques (moyennes, médianes, etc.), cette section développe une fonction maison (rédigée par l’utilisateur pour la mettre en pratique) et identifie la fonction déjà implantée en R.","code":""},{"path":"calculer.html","id":"la-longueur","chapter":" 3 Calculer","heading":"3.1 La longueur","text":"La longueur d’une variable correspond au nombre d’éléments qu’elle contient. La fonction length() permettra d’obtenir ce résultat. Ce sera particulièrement utile lorsqu’il faudra calculer, par exemple, le nombre de boucle à réaliser à partir des éléments d’un vecteur ou la taille d’échantillon (le nombre d’unités d’observation d’une variable).La somme d’une chaîne de caractères est toujours de \\(1\\), peu importe le nombre de caractères. La fonction nchar() produira le nombre de caractères.Une variable qui existe, mais qui ne contient pas de valeur aura une longueur égale à \\(0\\). Ce type de variable est utile lorsqu’il faut créer une variable dont la taille sera altérée.Pour les matrices et les jeux de données, ncol() (nombre de colonnes) et nrow() (nombre de lignes) sont plus efficaces et intuitives.","code":"x <- c(1, 2, 3)\nlength(x)\n> [1] 3\n\ny <- \"Bonjour tout le monde!\"\nlength(y)\n> [1] 1\n\nnchar(y)\n> [1] 22"},{"path":"calculer.html","id":"la-répétion","chapter":" 3 Calculer","heading":"3.2 La répétion","text":"La fonction rep() est utile pour répéter volontairement des valeurs. Il y trois possibilités de répétitions: l’argument times définit le nombre de fois que le vecteur est répété; l’argument définit le nombre de fois que chaque élément est répété; l’argument length.précise le nombre d’éléments de la sortie. Plusieurs combinaisons de ces arguments sont possibles.","code":"vec <- c(2, 4, \"chat\")\n\n# Répéter `vec` trois fois\nrep(vec, times = 3)\n> [1] \"2\"    \"4\"    \"chat\" \"2\"    \"4\"    \"chat\" \"2\"    \"4\"   \n> [9] \"chat\"\n\n# Répéter chaque élément de `vec` trois fois\nrep(vec, each = 3)\n> [1] \"2\"    \"2\"    \"2\"    \"4\"    \"4\"    \"4\"    \"chat\" \"chat\"\n> [9] \"chat\"\n\n# Répéter chaque élément de `vec` d'une longueur de 8\nrep(vec, length.out = 8)\n> [1] \"2\"    \"4\"    \"chat\" \"2\"    \"4\"    \"chat\" \"2\"    \"4\"\n\n#  Répéter chaque élément 3 fois à 2 reprises\nrep(vec, times = 2, each = 3)\n>  [1] \"2\"    \"2\"    \"2\"    \"4\"    \"4\"    \"4\"    \"chat\" \"chat\"\n>  [9] \"chat\" \"2\"    \"2\"    \"2\"    \"4\"    \"4\"    \"4\"    \"chat\"\n> [17] \"chat\" \"chat\""},{"path":"calculer.html","id":"la-séquence","chapter":" 3 Calculer","heading":"3.3 La séquence","text":"Une première fonction pour créer des séquences de nombres est l’utilisation de : avec un nombre avant et après la ponctuation.Pour plus de malléabilité, la fonction seq() génère une séquence régulière de valeurs. Les arguments sont seq(= , = , = ) traduisibles par de , à, par. Les arguments par défaut seront très utiles pour simplifier l’écriture; La fonction commence ou termine la séquence par 1 et fera des bonds de 1 entre les valeurs. Un autre argument est la longueur de la sortie length.qui spécifie le nombre d’éléments que devra comporter le vecteur de sortie.","code":"# 1:3 équivaut à c(1, 2, 3)\n1:3\n> [1] 1 2 3# Une séquence de 1 (défaut, from = 1) à 10\nseq(10)\n>  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Une séquence de 1 (défaut, from = 1) à -10\nseq(-10)\n>  [1]   1   0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10\n\n# Une séquence de -10 (défaut, from = 1) à 1\nseq(from = -10, to = 1)\n>  [1] -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1\n\n# Une séquence de nombres paires (from = 2, to = 10, by = 2)\nseq(2, 10, 2)\n> [1]  2  4  6  8 10\n\n# Une séquence de nombres paires \nseq(from = 2, by = 2, length.out = 5)\n> [1]  2  4  6  8 10"},{"path":"calculer.html","id":"la-somme","chapter":" 3 Calculer","heading":"3.4 La somme","text":"Il est possible de calculer des sommes de variables pour en obtenir le total. En tant qu’humain, le calcul d’une série de nombre correspond à prendre chaque nombre et de les additionner un à un. La fonction suivante reproduit assez bien ce qu’un humain ferait (avec ses quelques caprices de programmation tel que devoir déclarer l’existence de la variable de total et spécifier le nombre d’éléments à calculer).À noter que l’utilisation de la boucle est à des fins illustratives seulement. En termes de rendement computationnel, elle est bien peu efficace. Il faudra privilégier la fonction sum() pour calculer le total de son entrée.Il faut prendre garde : R calcule le total de tous les éléments de l’entrée sans égard aux lignes et aux colonnes. Autrement dit, il vectorise les entrées. Si deux variables étaient entrées par inadvertance, alors R calculerait la somme de ces deux variables plutôt que de retourner deux totaux. À cette fin, les fonctions rowSums() et colSums() seront utiles lorsqu’il faudra calculer des sommes sur des lignes (row) ou des colonnes (col).","code":"somme <- function(x){\n  # La taille du vecteur `x`\n  n <- length(x)\n  \n  # Définir une variable nulle\n  total <- 0\n  \n  # Boucle pour additionner chaque élément\n  for(i in 1:n){\n    \n    # Prendre le ie élément et l'additionner\n    # au total des (i-1)e éléments précédents\n    total <- total + x[i]\n  }\n  # Retourner le total après la boucle\n  return(total)\n}\n\n# Pour tester\nx <- c(1,2,3,4,5,-6)\nsomme(x)\n> [1] 9\nsum(x)\n> [1] 9"},{"path":"calculer.html","id":"la-moyenne","chapter":" 3 Calculer","heading":"3.5 La moyenne","text":"La moyenne est une mesure de tendance centrale qui représente le centre d’équilibre d’une distribution (un centre de gravité en quelque sorte). Si le poids d’un des côtés d’une distribution de probabilité était altéré (plus lourde ou plus légère), alors la moyenne se déplacerait relativement vers cette masse.La moyenne d’un échantillon correspond à la somme de toutes les unités d’une variable divisée par le nombre de données de cette variable ou, mathématiquement, \\[\\bar{x}=\\frac{\\Sigma_{=1}^n x}{n}\\] où \\(x\\) est la variable, \\(n\\) est le nombre d’unité et \\(\\Sigma_i^n\\) représente la somme de toutes les unités de \\(x\\). R possède déjà une fonction permettant de calculer la moyenne sans effort, mean() où l’argument est la variable. Il est possible de développer une fonction maison pour calculer la moyenne commeoù sum(x) calculer la somme de toutes les unités de x, / permet la division et length(x) calcule le nombre d’unités du vecteur x. Par exemple, à partir d’une variable x, les fonctions suivantes donnent le même résultat. Par contre la fonction mean() est beaucoup plus robuste que cette dernière équation.Comme pour sum(), les fonctions rowMeans() et colMeans() seront utiles lorsqu’il faudra calculer des moyennes sur des lignes (row) ou des colonnes (col).3","code":"\nx_bar <- sum(x)/length(x)# Un vecteur\nx <- c(0, 1, 2, 3, 4, 5)\n\n# Comparaison\nmean(x)\n> [1] 2.5\nsum(x)/length(x)\n> [1] 2.5"},{"path":"calculer.html","id":"la-médiane","chapter":" 3 Calculer","heading":"3.6 La médiane","text":"La médiane d’un échantillon correspond à la valeur où \\(50\\%\\) des données se situe au-dessous et au-dessus de cette valeur. C’est la valeur au centre des autres (lorsqu’elles sont ordonnées). Quand le nombre de données est impair, le \\(\\frac{(n+1)}{2}\\)e élément est la médiane. Quand le nombre est pair, la moyenne des deux valeurs au centre correspond à la médiane. Cette statistique est intéressante comme mesure de tendance centrale, car elle est plus robuste aux valeurs aberrantes (moins sensibles) que la moyenne.Évidemment, R offre déjà une fonction median() pour réaliser le calcul. Il est toutefois possible de programmer une fonction maison. Il faut utiliser la fonction sort() pour ordonner les données (croissant par défaut).L’expression n%%2, lue \\(n \\bmod 2\\), joue astucieusement le rôle de vérifier si n est impaire. La formule générale \\(x \\bmod y\\) représente une opération binaire associant à deux entiers naturels le reste de la division du premier par le second. Par exemple, \\(60 \\bmod 7\\), noter 60%%7 dans R, donne \\(4\\) soit le reste de \\(7*8 + 4 = 60\\). Le logiciel le confirme.Il s’agit d’une technique de programmation très pratique. Dans le cas de n%%2, la formule donne \\(1\\) dans le cas d’un nombre impair ou \\(0\\) dans le cas d’un nombre pair, puis teste ce résultat pour déterminer s’il réalise s[(n+1)/2] lorsque n%%2==1(TRUE) , ce qui correspond à choisir l’élément au centre d’un vecteur de taille impair, ou bien mean(s[n/2+0:1] lorsque n%%2==0(FALSE) , ce qui correspond à choisir les deux éléments au centre d’un vecteur pair et d’en faire la moyenne. Il s’agit de l’une des nombreuses façons selon lesquelles il est possible de programmer la médiane.","code":"mediane <- function(x) {\n  # Longueur du vecteur\n  n <- length(x)\n  \n  # Ordonner le vecteur\n  s <- sort(x)\n  # Vérifier si la longueur est paire ou impaire et\n  # alors calculer la valeur médiane correspondante\n  ifelse(n%%2 == 1, s[(n + 1) / 2], mean(s[n / 2 + 0:1]))\n}\n\n# Un vecteur\nx <- c(42, 23, 53, 77, 93, 20, 37, 24, 60, 62)\n\n# Comparaison\nmedian(x)\n> [1] 47.5\nmediane(x)\n> [1] 47.560%%7\n> [1] 4"},{"path":"calculer.html","id":"la-variance","chapter":" 3 Calculer","heading":"3.7 La variance","text":"La variance d’un échantillon est une mesure de dispersion. Elle représente la somme des écarts (distances) par rapport à la moyenne au carré divisée par la taille d’échantillon moins \\(1\\). Mathématiquement, il s’agit de l’équation (3.1).\\[\ns^2 = \\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2\n\\tag{3.1}\n\\]\nIl est assez aisé d’élaborer une fonction pour réaliser se calculer avec les fonctions déjà abordées.La variance peut aussi être calculée plus efficacement avec la fonction R var().","code":"\nvariance <- function(x){\n  # Longueur du vecteur\n  n <- length(x)\n  \n  # Moyenne du vecteur\n  xbar <- mean(x)\n  \n  # La variance\n  variance <- sum((x - xbar) ^ 2)/(n - 1)\n  return(variance)\n}# Un vecteur\nx <- c(26, 6, 40, 36, 14, 3, 21, 48, 43, 2)\n\n# Comparaison\nvariance(x)\n> [1] 300\nvar(x)\n> [1] 300"},{"path":"calculer.html","id":"lécart-type","chapter":" 3 Calculer","heading":"3.8 L’écart type","text":"L’écart type d’un échantillon représente la racine carrée de la variance. Elle une interprétation plus intuitive en tant que mesure de la moyenne des écarts par rapport à la moyenne. Si le calcul avait été entrepris avec les distances par rapport à la moyenne (au lieu des écarts au carré), alors la somme serait toujours de 0, un résultat tout à fait bancal. En prenant la racine carrée des écarts au carré, ce qui constitue une mesure de distance euclidienne, l’écart type devient une mesure de l’étalement de la dispersion autour du centre d’équilibre.\\[\ns =\\sqrt{s^2}= \\sqrt{\\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2}\n\\]\nAvec R, la fonction de base est sd(). Il est possible de récupérer la fonction maison précédemment rédigée.","code":"ecart.type <- function(x){\n  # La racine carrée de la variance\n  et <- sqrt(variance(x))\n  return(et)\n}\n\n# Comparaison\necart.type(x)\n> [1] 17.3\nsd(x)\n> [1] 17.3"},{"path":"calculer.html","id":"les-graines","chapter":" 3 Calculer","heading":"3.9 Les graines","text":"Par souci de reproductibilité, il est possible de déclarer une valeur de départ aux variables pseudoaléatoires, ce que l’nomme une graine ou seed en anglais. Cela permet de toujours d’obtenir les mêmes valeurs à plusieurs reprises, ce qui est très utile lors d’élaboration de simulations complexes ou lorsque des étudiants essaient de répliquer résultat tiré d’un ouvrage pédagogique.Il suffit de spécifier cette commande (en remplaçant nombre par un nombre) en début de syntaxe pour définir la séquence de nombre. Cette fonction sera utilisée à plusieurs reprises dans le de reproduire les mêmes sorties.Cette fonction est présentée, car elle reviendra régulièrement dans ce livre pour qu’il soit possible de reproduire et obtenir exactement les mêmes résultats.","code":"\nset.seed(\"nombre\")"},{"path":"calculer.html","id":"les-distributions","chapter":" 3 Calculer","heading":"3.10 Les distributions","text":"Il existe plusieurs distributions statistiques déjà programmées avec R. Voici les principales utilisées dans cet ouvrage.\nTable 3.1: Noms des distributions, fonctions et leurs arguments\nLes libellés ci-dessus ne commanderont pas de fonction. Il faut joindre en préfixe à ces distributions l’une des quatre lettres suivantes : d, p,q, ou r. La plus simple est certainement r (random) qui génère n valeurs aléatoires de la distribution demandée selon les paramètres spécifiés. Les fonctions q (quantile) prennent un argument de 0 à 1 (100%), soit un percentile et retourne la valeur de la distribution. La fonction p (probabilité) retourne la probabilité cumulative (du minimum jusqu’à la valeur) d’une valeur de cette distribution. Enfin, la lettre d (densité) permet, notamment, d’obtenir les valeurs de densité de la distribution.Voici un exemple avec la distribution normale.Ces quatre lettres peuvent être associées à toutes les distributions énumérées et bien d’autres. Elles respectent toutes ce cadre.Afin d’illustrer ce que font ces variables, la Figure 3.1 montre dnorm(), pnorm() et qnorm(). La fonction rnorm() n’est pas illustrée. Cette dernière retourne des valeurs de l’axe des \\(x\\) en respectant les probabilités d’une courbe normale. La fonction dnorm() prend en argument une valeur de l’axe des \\(x\\) et retourne la valeur de la courbe normale (la densité) correspondante, soit la courbe illustrée. En d’autres termes, elle retourne la hauteur de la courbe (ligne pointillée). Les fonctions pnorm() et qnorm() sont interreliées. La fonction pnorm() prend une valeur de l’axe des \\(x\\) et retourne sa probabilité (de \\(-\\infty\\) à \\(x\\)), soit la zone grise de la Figure 3.1. La fonction qnorm(), quant à elle, prend une probabilité et retourne la valeur sur l’axe des \\(x\\) correspondant.\nFigure 3.1: Illustration des fonctions liées à la distribution normale\nCes fonctions entreront en jeu dans le chapitre Inférer.","code":"set.seed(9876)\n\n# Génère 5 valeurs aléatoires en fonction des paramètres\nrnorm(n = 5, mean = 10, sd = .5)\n> [1] 10.51  9.42  9.90  9.95 10.01\n\n# Retourne les valeurs associés à ces probabilités\nqnorm(p = c(.025,.975))\n> [1] -1.96  1.96\n\n# Retourne la probabilité d'obtenir un score de 1.645 et moins\npnorm(q = c(.5, 1.645, 1.96))\n> [1] 0.691 0.950 0.975\n\n# La valeur de la densité de la distribution\ndnorm(x = c(0, 1))\n> [1] 0.399 0.242"},{"path":"exercice-rudiments.html","id":"exercice-rudiments","chapter":"Exercices","heading":"Exercices","text":"Quel est le résultat de mean <- c(1, 2, 3)? Pourquoi?Quel est le résultat de mean <- c(1, 2, 3)? Pourquoi?Quelle est la différence entre # Caractère et \"Caractère\"?Quelle est la différence entre # Caractère et \"Caractère\"?Créer un vecteur contenant les valeurs \\(4, 10, 32\\). Calculer la moyenne et l’écart type de ce vecteur.Créer un vecteur contenant les valeurs \\(4, 10, 32\\). Calculer la moyenne et l’écart type de ce vecteur.Créer un vecteur contenant les valeurs de \\(4\\) à \\(11\\). Sélectionner la deuxième valeur de ce vecteur, puis additionner 100 à cette valeur et remplacer la dans le vecteur.Créer un vecteur contenant les valeurs de \\(4\\) à \\(11\\). Sélectionner la deuxième valeur de ce vecteur, puis additionner 100 à cette valeur et remplacer la dans le vecteur.Générer 10 valeurs aléatoires distribuées normalement avec une moyenne de 50 et un écart type de 4. Calculer la moyenne, la médiane et la variance.Générer 10 valeurs aléatoires distribuées normalement avec une moyenne de 50 et un écart type de 4. Calculer la moyenne, la médiane et la variance.Créer un jeu de données contenant quatre sujets avec, pour chacun, leur nom de famille, leur âge et un score d’appréciation tiré d’une distribution uniforme allant de 0 à 100.Créer un jeu de données contenant quatre sujets avec, pour chacun, leur nom de famille, leur âge et un score d’appréciation tiré d’une distribution uniforme allant de 0 à 100.Rédiger une fonction calculant l’hypoténuse d’un triangle rectangle. Rappel: le théorème de Pythagore est \\(c^2=^2+b^2\\).Rédiger une fonction calculant l’hypoténuse d’un triangle rectangle. Rappel: le théorème de Pythagore est \\(c^2=^2+b^2\\).Rédiger une fonction calculant un score-\\(z\\) pour une variable. Rappel: un score-\\(z\\), correspond à \\(z=\\frac{x-\\mu}{\\sigma}\\).Rédiger une fonction calculant un score-\\(z\\) pour une variable. Rappel: un score-\\(z\\), correspond à \\(z=\\frac{x-\\mu}{\\sigma}\\).Rédiger une fonction calculant la médiane d’une variable (ne recopiez pas celle de ce livre).Rédiger une fonction calculant la médiane d’une variable (ne recopiez pas celle de ce livre).Rédiger une fonction qui pivote une liste de \\(k\\) éléments par \\(n\\). Par exemple, une liste de six (\\(k=6\\) comme \\([1,2,3,4,5,6]\\)) pivoté de deux (\\(n=2\\)) devient (\\([3,4,5,6,1,2]\\)).Rédiger une fonction qui pivote une liste de \\(k\\) éléments par \\(n\\). Par exemple, une liste de six (\\(k=6\\) comme \\([1,2,3,4,5,6]\\)) pivoté de deux (\\(n=2\\)) devient (\\([3,4,5,6,1,2]\\)).Rédiger une fonction pour générer une séquence de Fibonacci (chaque nombre est la somme des deux précédents) jusqu’à une certaine valeur, soit \\(1, 1, 2, 3, 5, 8,...\\).Rédiger une fonction pour générer une séquence de Fibonacci (chaque nombre est la somme des deux précédents) jusqu’à une certaine valeur, soit \\(1, 1, 2, 3, 5, 8,...\\).","code":""},{"path":"entrer.html","id":"entrer","chapter":" 4 Entrer","heading":" 4 Entrer","text":"S’il y bien une caractéristique de R qui rebute les nouveaux utilisateurs, c’est certainement que le logiciel ne soit pas conçu pour la saisie de données ou, du moins, que cela ne soit pas mis en avant-plan. Lorsque le logiciel s’ouvre, que ce soit R ou RStudio, l’aspect table de données n’existe pas. Un tout nouvel utilisateur habitué aux logiciels traditionnels reste pantois : où sont les données?Conceptuellement, R importe des données pour les manipuler. Elles ne sont pas entrées dans le logiciel. Il existe toutefois quelques méthodes pour ce faire, dont en voici une courte liste. Ces méthodes sont présentées; elles ne sont pas recommandées.","code":""},{"path":"entrer.html","id":"data.entry-r-base","chapter":" 4 Entrer","heading":"4.1 data.entry() (R-base)","text":"De base, R offre la possibilité d’entrer des données dans un tableur avec la commande jd = data.entry(). Si une base de données est demandée comme argument (p. ex., data.entry(data = jd)), alors le jeu de données s’ouvre. Il est aussi possible d’ouvrir le fichier avec des variables déjà créées avec R. Si un tableur vierge est désiré, alors il faut taper data.entry(1) dans la console pour ouvrir le tableur avec une seule valeur (1), L’utilisateur peut alors modifier les noms de colonnes et entrer les données comme dans un logiciel traditionnel. La Figure 4.1 montre l’interface bien moins attrayante que celles des compétiteurs.\nFigure 4.1: Ouverture du tableur R\nLorsque l’entrée de données est terminée, l’utilisateur doit sauvegarder le jeu de données ou l’environnement de travail qui pourront être importés pour de futures utilisations ou entrées (voir la section Sauvegarder un jeu de données). En général, l’utilisateur qui entre manuellement ces données préférera certainement un autre tableur, puis importer ses données, mais R est certainement en mesure de faire ce travail.","code":""},{"path":"entrer.html","id":"data_edit-dataeditr","chapter":" 4 Entrer","heading":"4.2 data_edit() (DataEditR)","text":"Comme il arrive régulièrement, un problème avec R se résout avec un package. Il existe un package qui permet de faire l’entrée de données en tableur avec R. Il s’agit du package DataEditR (Hammill, 2021), une interface utilisateur graphique. Il résout l’un de plus grands défis lorsqu’un utilisateur migre des tableurs traditionnels vers R : une feuille de calcul interactive où il est possible de pointer et cliquer pour modifier, ajouter, supprimer des donnes vers un mode strict de syntaxe.Pour démarrer, il faut d’abord installer le package, puis l’appeler. Pour commencer à entrer des données, la syntaxe data_edit() est suffisante. Pour ouvrir un jeu de données, il suffit de l’ajouter en argument data_edit(jd).Une fois l’interface ouvert, il est possible de manipuler le jeu de données avec les options affichés et avec le clic droit qui permettra notamment d’ajouter des lignes et des colonnes.Il est recommandé de ne laisser que les données brutes, toutes les modifications et manipulations doivent rester en syntaxe R dans un script associé au jeu de données. Lorsque les entrées sont terminées, sauvegarder la base de données, préférablement en extension .csv. Il est aussi possible de sortir le tableur en tableau de données en assignant la fonction à une variable comme jd = data_edit().\nFigure 4.2: Ouverture du tableur de DataEditR\nEn général, l’utilisateur doit importer ces données dans l’environnement R. Il faut même le faire avec data_edit() à chaque ouverture d’un nouvelle séance, pour poursuivre l’entrée ou réaliser des manipulations manuelles.","code":"\n# Pour installer le package\ninstall.packages(\"DataEditR\")\n\n# Pour rendre la package accessible\nlibrary(\"DataEditR\")\n\n# La fonction\ndata_edit()"},{"path":"importer.html","id":"importer","chapter":" 5 Importer","heading":" 5 Importer","text":"Un jeu de données est essentiel pour réaliser des analyses statistiques et des représentations graphiques. Après l’ouverture de R, l’usager remarque, stupéfait, que le logiciel n’est pas un tableur (tableau de données), contrairement aux logiciels traditionnels. Cette caractéristique l’peut-être même frappé à la première ouverture!Les jeux de données sont traités de la même façon que les scripts. Il est d’usage de les conserver dans un fichier externe et d’y recourir pendant la séance de travail.Un jeu de données porte généralement les extensions “.Rdata”, lorsqu’elles proviennent de R, ou d’extensions “.dat” et “.txt”. Évidemment, R permet une grande flexibilité, il est ainsi possible d’exporter et d’importer dans d’autres extensions. Les extensions “.Rdata” sont des environnements R, elles contiennent potentiellement plusieurs variables, comme une séance de travail complète. Elles ont aussi l’avantage que, si l’utilisateur double-clique sur un fichier d’extension “.Rdata”, celui-ci s’ouvre automatiquement dans l’environnement R.Il faut importer le jeu de données à chaque nouvelle séance utilisation. Contrairement aux logiciels traditionnels, il n’est pas nécessaire, ni même recommander!, de modifier, d’altérer ou de manipuler un fichier contenant des données. Cela n’est pas nécessaire dans la mesure où les syntaxes décrivant ces manipulations sont conservées. Il devient alors impossible d’endommager, de corrompre, d’altérer ou d’archiver erronément le fichier de données. Les données originales restent intactes. Il suffit de les importer à chaque nouvelle séance, de commander la syntaxe qui lui est associée pour obtenir de nouveau la version propre du jeu de données.Il est recommandé de ne jamais manipuler les fichiers de données une fois toutes les vérifications réalisées (absence d’erreur dans les données). Il n’est jamais nécessaire de modifier ces fichiers avec R. Également, un même jeu de données peut être associé à plusieurs syntaxes de nettoyage, de manipulation et d’analyse, mais tous les collaborateurs partagent le même jeu de données, ce qui facilite les échanges entre eux.","code":""},{"path":"importer.html","id":"jeux-de-données-provenant-de-r-et-de-packages","chapter":" 5 Importer","heading":"5.1 Jeux de données provenant de R et de packages","text":"Plusieurs packages offrent, en plus des fonctions, des jeux de données. Mieux encore! R offre des jeux de données inclus avec le logiciel. La fonction data() permet de voir la liste des jeux de données disponibles. Taper simplement le nom du jeu de données permet de l’utiliser comme s’il avait été déclaré auparavant.La fonction head() introduite ici donne simplement un aperçu des six premières lignes du jeu de données pour ne pas afficher le jeu de données complet (ce qui prend beaucoup d’espace inutilement dans la console).Pour consulter tous les jeux de données des packages importés, il est possible d’utiliser cette ligne de code.Pour utiliser ces jeux, il faut rendre actif le package associé avec la fonction library().","code":"head(cars)\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6     9   10\ndata(package = .packages((all.available = TRUE)))"},{"path":"importer.html","id":"créer-un-jeu-de-données-artificielles","chapter":" 5 Importer","heading":"5.2 Créer un jeu de données artificielles","text":"Une façon rudimentaire et efficace d’obtenir des données avec R est de les créer à l’aide des fonctions génératrices de données pseudoaléatoires vue à la section Les distributions et les joindre ensemble (voir section Créer un jeu de données).Et voilà un jeu de données simple et sauvegardé dans le dossier de travail auquel il est possible de se référer.Ici, deux nouvelles fonctions sont employées : round() arrondie les valeurs à l’unité et save() permet de sauvegarder un jeu de données crée un le jeu de données.","code":"\n# Pour la reproductibilité\nset.seed(142)\n\n# Nombre d'unité\nn <- 30\n\n# Identifiant (séquence de 1 à n)\nid <- 1:n\n\n# Variables\nsexe <- rbinom(n, size = 1, prob = .5)\nQI <-  round(rnorm(30, mean = 100, sd = 15) - 5 * sexe)\n# Être \"1\" soustrait 5 points au QI en moyenne\n# Arrondi avec round()\n\n# Création du jeu de données\njd <- data.frame(id = id, \n                 sexe = sexe,\n                 QI = QI)\n\n# Enregistrement\nsave(jd, file = \"donnees.Rdata\")"},{"path":"importer.html","id":"sauvegarder-un-jeu-de-données","chapter":" 5 Importer","heading":"5.3 Sauvegarder un jeu de données","text":"Si un jeu de données est créé directement avec R, par exemple, les jeux de données artificiels, il est possible de les sauvegarder avec la fonction save() qui enregistre une variable dans un fichier.La fonction save() deux arguments principaux : un nom de variable à enregistrer et un nom de fichier d’extension .Rdata, les deux entre guillemets anglophones.Il est possible à la fin d’une session de travail de sauvegarder l’environnement complet dans un fichier save.image(). Ainsi, toutes les variables et fonctions maison sont conservées pour une future utilisation.Il y aussi la famille de fonction write() pour enregistrer le jeu de données en différentes extension, comme, write.csv(), write.csv2(), write.table(). Celles-ci fonctionne comme la fonction save().","code":"\nsave(\"variable\", file = \"fichier.Rdata\")\nsave.image(file = \"SessionTravail.Rdata\")"},{"path":"importer.html","id":"voir-la-base-de-données","chapter":" 5 Importer","heading":"5.4 Voir la base de données","text":"Parfois des valeurs s’ajoutent lors de l’exportation ou l’importation des données. Des logiciels traditionnels font parfois ce mauvais tour. Une vérification de la base de données est par conséquent impérative, surtout lors de la première utilisation du jeu de données. Deux méthodes de vérification sont suggérées. D’abord, ouvrir le fichier avec un éditeur de texte de base, comme bloc-notes, pour s’assurer qu’aucun caractère indésirable ne s’est ajouté à l’insu de l’utilisateur. Ensuite, voir avec la fonction View() dans R si la base de données s’affiche correctement et que les colonnes et les lignes correspondent à ce qui est attendu.Il est possible de voir les données en utilisant la fonction View() et en y insérant le nom de la variable associée au jeu de données. Le logiciel affiche un tableur avec les données dont il est impossible de modifier les valeurs. Cette fonction est utile pour s’assurer que le jeu de données est en ordre, bien importé, ou le consulter.","code":"\nView(jd)"},{"path":"importer.html","id":"emplacement-du-jeu-de-données","chapter":" 5 Importer","heading":"5.5 Emplacement du jeu de données","text":"Idéalement, le fichier contenant le jeu de données se retrouve déjà dans le directoire de travail (ou dan le projet R en cours). Dans ce contexte, il suffit de référer seulement au nom du fichier.Si le jeu de données est sur le web, il peut être importé en précisant l’URL.S’il est dans un fichier sur l’ordinateur, mais pas dans le directoire de travail, il s’agit essentiellement la même méthode.Pour ces deux derniers exemples d’importations, noter bien l’utilisation du / (barre oblique ou slash) ou du plus robuste // plutôt que le \\ (barre oblique inverse) généralement utilisée par les ordinateurs.Si l’utilisateur ne connaît pas exactement la trajectoire du fichier, il peut se résoudre à passer par l’explorateur de fichiers (Windows ou Apple) pour déterminer l’emplacement du fichier de jeu de données. Il faut alors utiliser la fonction file.choose() sans aucun argument à l’intérieur de la fonction d’importation.L’utilisateur doit identifier manuellement (pointer et cliquer) où se trouve le fichier. Il se promène de fichier en fichier jusqu’à ce qu’il arrive au bon jeu de données, un peu comme le font les logiciels traditionnels lorsque l’utilisateur souhaite sauvegarder un fichier à un certain endroit. Une fois la trajectoire du fichier identifiée, la variable chemin contient cette trajectoire qui peut alors être utiliser dans les fonctions d’importation ou bien manipuler comme n’importe quelle chaîne de caractères.","code":"\njd <- read.table(\"fichier.txt\")\njd <- read.table(\"https://site/ou/trouver/le/fichier.txt\")\njd <- read.table(\"C://site//ou//trouver//le//fichier.txt\")\nchemin <- file.choose()"},{"path":"importer.html","id":"importer-avec-r","chapter":" 5 Importer","heading":"5.6 Importer avec R","text":"Dans la plupart des situations, les analyses et les graphiques sont réalisés à partir d’un jeu de données se trouvant dans un fichier. Ce lui-ci est importé en R pour être manipulé. Les jeux de données peuvent se trouver dans un fichier dans l’ordinateur, mais aussi sur le web. Ils peuvent être en différents types de format.Pour la description de l’importation, le présent ouvrage prend pour acquis que le fichier de données se retrouve dans le directoire de travail (ce qui est l’idéal en général). La création de trajectoires pour différents emplacements est présentée dans la section Emplacement du jeu de données.Les fonctions de base permettent d’importer la plupart des jeux de données, particulièrement s’ils ont été exportés dans un format compatible. Pour le cas où ces fichiers ne sont pas importables, des packages pallient ce besoin. Dans cet ouvrage, seule une présentation sommaire de ces options est discutée, l’utilisateur est recommandé à la documentation de ces packages pour plus d’informations.La prochaine section décrit les fonctions pour importer les bases de données “manuellement”. En plus de ces méthodes, RStudio possède une interface permettant l’importation des données.","code":""},{"path":"importer.html","id":"la-fonction-de-base","chapter":" 5 Importer","heading":"5.6.1 La fonction de base","text":"La fonction de base read.table() permet d’importer la plupart des jeux de données. Ceux-ci ont certaines caractéristiques qu’il faudra préciser comme argument à la fonction read.table() pour assurer une importation adéquate. Ces caractéristiques sont header = FALSE, sep = \"\", et fill = !blank.lines.skip (les éléments à droite sont les options par défaut).Parfois, certains fichiers sauvegardent le nom des variables en tête de colonne (première ligne). Par défaut, R assume qu’il s’agit de valeurs. L’argument header = TRUE ajouté à read.table() précise à R lors de l’importation que ces libellés sont des noms de colonnes.Si un autre symbole est utilisé pour délimiter (séparer) des valeurs dans le fichier, comme ;ou ,, l’argument sep = \";\" ou sep = \",\" précise le séparateur.Si les lignes du fichier sont de tailles inégales, R assume qu’il s’agit de valeur, et ces blancs de texte sont ajoutés comme valeurs (\"\"). Pour gérer cette situation, l’argument fill = FALSE règle la situation.","code":""},{"path":"importer.html","id":"fichiers-dextension-.txt","chapter":" 5 Importer","heading":"5.6.2 Fichiers d’extension .txt","text":"Un fichier d’extension .txt est un fichier texte délimité par des tabulations (tab-delimited text files) et est importé à l’aide de la fonction read.table().","code":"\njd <- read.table(\"fichier.txt\")"},{"path":"importer.html","id":"fichiers-dextension-.dat","chapter":" 5 Importer","heading":"5.6.3 Fichiers d’extension .dat","text":"Un fichier d’extension .dat est un fichier générique de données et est importé à l’aide de la fonction read.table().","code":"\njd <- read.table(\"fichier.dat\")"},{"path":"importer.html","id":"fichiers-dextension-.csv","chapter":" 5 Importer","heading":"5.6.4 Fichiers d’extension .csv","text":"Un fichier d’extension .csv use généralement de séparateur comme \";\" (lorsque le système numérique de la langue d’origine utilise la virgule - comme le français par exemple) ou \",\" (pour les autres langues qui n’utilise pas la virgule) et ont généralement les noms de variables en première ligne. Ainsi, la fonction read.table() est utilisable pourvu que le séparateur soit précisé et la présence d’en-tête également.Il existe aussi la fonction read.csv() (nombres décimaux délimités par un point) et read.csv2() (nombres décimaux délimités par une virgule) pour importer des fichiers d’extension .csv. Il s’agit exactement de read.table() à l’exception des arguments par défaut, mais précisant par défaut header = TRUE et fill = TRUE et détecte s’il s’agit de \";\" ou \",\".","code":"\njd <- read.table(\"fichier.csv\", sep = \";\", header = TRUE)"},{"path":"importer.html","id":"fichiers-délimités","chapter":" 5 Importer","heading":"5.6.5 Fichiers délimités","text":"Pour les fichiers recourant à un autre caractère qu’une tabulation, qu’une \",\", ou un \";\" pour délimiter les valeurs, il faut spécifier le caractère dans read.table() importe le fichier.Comme pour read.csv() et read.csv2(), les fonctions read.delim() et read.delim() pourraient être utilisées.","code":"\njd <- read.table(\"fichier.txt\", sep = \"$\")"},{"path":"importer.html","id":"fichiers-dextension-.sav-.dta-.syd-et-.mtp","chapter":" 5 Importer","heading":"5.6.6 Fichiers d’extension .sav, .dta, .syd et .mtp","text":"Comme le lecteur s’en doute peut-être, R de base ne permet pas d’importer des fichiers spécifiques d’autres logiciels. Par contre, avec les années se sont développés des packages permettant de pallier la situation. Le package foreign (R Core Team, 2020) permet d’importer des fichiers issus de IBM SPSS (.sav), Stata (.dta) et Systat (.syd) et Minitab (mtp) avec, respectivement les fonctions read.spss(), read.data(), read.systat() et read.mtp(). La logique d’importation est la même pour ces quatre fonctions.Pour read.spss(), il y deux arguments qui sont importants à souligner. Par défaut, la fonctionne ne retourne pas un data frame et utilise les libellés de valeurs (value labels). Dans la plupart des cas, l’utilisateur désire probablement obtenir un jeu de données de type data.frame et les valeurs sous-jacentes aux libellés de valeurs. L’utilisateur peut alors changer ces arguments .data.frame = TRUE (par défaut, FALSE) et use.value.labels = FALSE(par défaut, TRUE).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\nlibrary(foreign)\n\n# SPSS\njd <- read.spss(\"fichier.sav\", \n                to.data.frame = TRUE,\n                use.value.labels = FALSE)\n\n# Stata\njd <- read.dta(\"fichier.dta\")\n\n# Systat\njd <- read.systat(\"fichier.syd\") \n\n# Minitab\njd <- read.mtp(\"fichier.mtp\")"},{"path":"importer.html","id":"fichiers-dextension-.xls-et-xlsx","chapter":" 5 Importer","heading":"5.6.7 Fichiers d’extension .xls et xlsx","text":"Il n’existe pas de fonction de base pour importer des fichiers Microsoft Excel (extensions .xls et .xlsx). Par contre, il existe plusieurs packages qui permettent de la faire, comme readxl (Wickham & Bryan, 2022). Le package readlxl permet d’utiliser la fonction read_excel() pour importer le fichier.La fonction read_excel() possède un argument sheet = qui permet de préciser la feuille qu’il faut importer ou range = (p. ex. range = A1:B20 qui permet d’importer un rectangle de plage de données (du coin supérieur gauche A1 au coin inférieur droit B20).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n# Excel\nlibrary(readxl)\njd = read_excel(\"fichier.xls\")"},{"path":"importer.html","id":"fichiers-dextension-.html","chapter":" 5 Importer","heading":"5.6.8 Fichiers d’extension .html","text":"Il n’existe pas de fonction de base pour importer des fichiers d’extension .html, (HTML, HyperText Markup Language). Le package XML fournit une solution possible avec la fonction readHMTLTable() (Temple Lang, 2022).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n# HTML\nlibrary(XML)\njd <- readHMTLTable(\"fichier.html\")"},{"path":"importer.html","id":"fichiers-dextension-.json","chapter":" 5 Importer","heading":"5.6.9 Fichiers d’extension .json","text":"Il n’existe pas de fonction de base pour importer des fichiers d’extension .json, (JavaScript Object Notation). Comme le lecteur pourra s’y attendre, il existe un package pour rectifier la situation : le package rjson et sa fonction fromJSON() (Couture-Beil, 2022).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n#JSON\nlibrary(rjson)\njd <- fromJSON(\"fichier.json\")"},{"path":"importer.html","id":"fichiers-dextension-.sas7bdat","chapter":" 5 Importer","heading":"5.6.10 Fichiers d’extension .sas7bdat","text":"Il n’existe pas de fonction de base pour importer des fichiers d’extension .sas7bdat, (Statistical Analysis System). Il existe le package sas7bdat pour importer des données de SAS vers R avec la fonction read.sas7bdat() (Shotwell, 2022).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n# SAS\nlibrary(sas7bdat)\njd <- read.sas7bdat(\"fichier.sas7dbat\")"},{"path":"importer.html","id":"importation-avec-rstudio","chapter":" 5 Importer","heading":"5.7 Importation avec RStudio","text":"RStudio offre une interface simple pour télécharger directement un jeu de de données IBM SPSS, Microsoft Excel, SAS, STATA, et des extensions “.txt” et “.readr”. Il y même un outil de visualisation pour s’assurer que le tout est en ordre. Pour procéder, il faut faire *File > Import dataset > “format de fichier* où”format de fichier” remplace Text, SPSS, Excel et les autres. Il faut suivre les instructions. En indiquant le chemin du fichier, R importe le fichier et fournie une syntaxe afin de reproduire l’importation pour de futurs usages.","code":""},{"path":"importer.html","id":"exporter-dautres-logiciels","chapter":" 5 Importer","heading":"5.8 Exporter d’autres logiciels","text":"","code":""},{"path":"importer.html","id":"exporter-de-ibm-spss","chapter":" 5 Importer","heading":"5.8.1 Exporter de IBM SPSS","text":"Il est possible d’exporter des données de IBM SPSS pour une utilisation avec R. Il faut quelques manipulations préalables. En ayant le fichier de données IBM SPSS ouvert, il faut cliquer sur “Enregistrer sous” sous le menu déroulant “Fichier”. Par défaut, IBM SPSS choisie toutes les variables, mais il est possible de sélectionner seulement les variables d’intérêt en décochant les variables qu’il n’est pas nécessaire de conserver. Ensuite, il faut sélectionner le type de fichier de sauvegarde, préférablement “Tabulé (*.dat)“. IBM SPSS offre également la possibilité d’enregistrer les noms de variables (première option à cocher) et les libellés de valeur. Il suffit maintenant de nommer le fichier et de cliquer sur l’onglet”Enregister”.En s’assurant que nouveau fichier se trouve dans le directoire actif de R, il suffit de télécharger le fichier.L’option header est FALSE si les noms de variables n’ont pas été conservés (première ligne du fichier). La variable jd contient la base de données et est prête à être manipulée.","code":"\njd <- read.table(file = \"donnees.tab\", header = TRUE)"},{"path":"importer.html","id":"exporter-de-microsoft-excel","chapter":" 5 Importer","heading":"5.8.2 Exporter de Microsoft Excel","text":"Il est possible d’exporter des données de Microsoft Excel pour une utilisation avec R. Il faut sélectionner l’onglet “Fichier”, puis Enregistrer sous”. Dans le menu déroulant, sélectionner comme type de fichier “Texte Unicode (*.txt)“. Intituler le fichier, puis cliquer sur”Enregister”.Microsoft Excel sauvegarde l’entièreté de la page active. Il est donc pertinent de créer une feuille Microsoft Excel ne contenant que les informations à conserver.Par la suite, en s’assurant que nouveau fichier se trouve dans le directoire actif de R, il suffit de télécharger le fichier avec read.table() (voir la prochaine section) et les arguments convenant au jeu de données.","code":""},{"path":"importer.html","id":"quelques-conseils","chapter":" 5 Importer","heading":"5.9 Quelques conseils","text":"Voici quelques conseils pour la gestion le jeu de données.Éviter les noms trop longs ou trop courts et dépourvus de signification. Cela augmente le risque d’erreur. L’utilisation de huit caractères ou moins est une bonne recommandation (quoique ce n’est pas une règle!). Pour conserver plus de renseignements, utiliser les commentaires de la syntaxe.Éviter les noms trop longs ou trop courts et dépourvus de signification. Cela augmente le risque d’erreur. L’utilisation de huit caractères ou moins est une bonne recommandation (quoique ce n’est pas une règle!). Pour conserver plus de renseignements, utiliser les commentaires de la syntaxe.Éviter les espaces entre les mots. Cela peut être interprété erronément comme deux éléments. À la place, collez les mots et distinguer-les avec des majuscules (MaFonction), utiliser le tiret bas (ma_fonction) ou un point (ma.fonction).Éviter les espaces entre les mots. Cela peut être interprété erronément comme deux éléments. À la place, collez les mots et distinguer-les avec des majuscules (MaFonction), utiliser le tiret bas (ma_fonction) ou un point (ma.fonction).Éviter les espaces ou les vides dans les données. Cela peut être interprété comme des données (absentes) ou non.Éviter les espaces ou les vides dans les données. Cela peut être interprété comme des données (absentes) ou non.Éviter les symboles suivants ?, $, %, ^, &, *, (, ), -, #, ?, , , <, >, /, |, \\, [, ], { et } qui peuvent erronément être interprétés comme de la syntaxe autant dans les noms de variables que dans les données.Éviter les symboles suivants ?, $, %, ^, &, *, (, ), -, #, ?, , , <, >, /, |, \\, [, ], { et } qui peuvent erronément être interprétés comme de la syntaxe autant dans les noms de variables que dans les données.Vérifier que les valeurs manquantes sont identifiées NA.Vérifier que les valeurs manquantes sont identifiées NA.Si les données proviennent d’un autre logiciel, vérifier la présence de commentaires qui pourraient occasionner des lignes ou colonnes supplémentaires et ainsi corrompre le jeu de données.Si les données proviennent d’un autre logiciel, vérifier la présence de commentaires qui pourraient occasionner des lignes ou colonnes supplémentaires et ainsi corrompre le jeu de données.Vérifier que l’exportation et l’importation se sont bien déroulées.Vérifier que l’exportation et l’importation se sont bien déroulées.","code":""},{"path":"manipuler.html","id":"manipuler","chapter":" 6 Manipuler","heading":" 6 Manipuler","text":"Avec R, il ne faut jamais manipuler directement le fichier contenant les données. Cette pratique est déconseillée. Il vaut mieux préserver le fichier original intact, ce qui évite de nombreuses complications, comme la compatibilité entre les versions, la reproductibilité des bases, la maintenance, etc. Toutes les manipulations doivent être conservées dans un fichier script. Cela favorise le suivi des modifications apportées en comparant tout simplement les traces dans entre les syntaxes, mais aussi le partage entre collègues.En pratique, l’expérimentateur aura le jeu de données officiel (final) avec lequel travailler. Il doit l’importer à chaque début de séance, tel que précisé dans la section Importer. Par la suite, il ne lui reste qu’à mettre en place le nettoyage et la préparation du jeu de données ou bien, si cela est déjà fait, de rouler les scripts des séances précédentes, ce qui se fait facilement en quelques cliques.Il existe plusieurs méthodes pour gèrer des données; il ne faut pas s’étonner de voir d’autres ouvrages aborder la gestion de données d’une autre façon. Au final, la meilleure méthode est celle qui m’est l’utilisateur à son aise.Dans la première section, les manipulations rudimentaires d’un jeu de données sont expliquées. Dans la seconde section, la philosophie tidyverse est présentée, car elle permet une manipulation assez intuitive des jeux de données4.","code":""},{"path":"manipuler.html","id":"manipulation-de-données","chapter":" 6 Manipuler","heading":"6.1 Manipulation de données","text":"Les tableaux ont généralement deux dimensions (lignes par colonnes). Différents éléments ou groupes d’éléments peuvent être extraits des jeux de données. Plusieurs méthodes peuvent être utilisées en fonction des besoins. Le jeu de données cars (disponible avec R) sera utilisé à des fins illustratives.Le jeu de données contient 50 unités d’observation (lignes) et deux variables (colonnes), soit la vitesse (speed) et la distance (dist).","code":"head(cars)\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6     9   10"},{"path":"manipuler.html","id":"référer-à-une-variable-dans-un-jeu-de-données","chapter":" 6 Manipuler","heading":"6.1.1 Référer à une variable dans un jeu de données","text":"Il est possible de référer à une variable soit en utilisant l’emplacement de la variable par rapport aux autres en utilisant les crochets ou en utilisant le signe $ puis le nom de la variable après le libellé de le jeu de données. L’opération est fort simple avec le symbole $.Précédemment utilisés pour extraire des valeurs dans une variable unidimensionnelle (voir Référer à des sous-éléments), les [] peuvent extraire des données sur un tableau en deux dimensions (ligne par colonne). Il faut spécifier la ou les lignes désirées, puis la ou les colonnes désirées entre crochets. Laissez une des dimensions en blanc (vide) indique au logiciel de rapporter toutes les valeurs. Par exemple, pour obtenir le même résultat, dist est la deuxième colonne. Il faut référer entre crochets à la colonne \\(2\\) et comme toutes les lignes sont désirées, cette dimension reste vide.Il est possible de faire la même chose avec les lignes.Ici, toutes les variables de la 4e unité sont rapportée. Remarquer bien l’absence d’argument après la virgule. La fonction head() n’est pas nécessaire ici, car il y peu d’informations à extraire.Si certaines valeurs spécifiques étaient désirées, comme la valeur de la 4e unité pour la 2e variable.Enfin, à l’intérieur d’un jeu de données, les variables peuvent être commandées avec le signe de $ placé après le nom de la variable suivi du nom de la variable ou encore en identifiant les noms de variables entre crochets.","code":"# Avec $\nhead(cars$dist)\n> [1]  2 10  4 22 16 10# Entre crochets\nhead(cars[,2])\n> [1]  2 10  4 22 16 10# Entre crochets\ncars[4,]\n>   speed dist\n> 4     7   22# Entre crochets\ncars[4, 2]\n> [1] 22# Utilisation du signe $\nhead(cars$speed)\n> [1] 4 4 7 7 8 9\n\n# Nommer entre crochets\nhead(cars[\"speed\"])\n>   speed\n> 1     4\n> 2     4\n> 3     7\n> 4     7\n> 5     8\n> 6     9"},{"path":"manipuler.html","id":"référer-à-un-sous-ensemble-dunité.","chapter":" 6 Manipuler","heading":"6.1.2 Référer à un sous-ensemble d’unité.","text":"Pour référer à des unités ayant certaines caractéristiques, la fonction subset() peut s’avérer utile. Les arguments sont un jeu de données, le deuxième est un opérateur logique (voir Les clauses conditionnelles) en lien avec une variable du jeu de données.Cette fonction est utile s’il faut extraire les données d’un certain sexe ou les participants plus jeune ou plus vieux qu’un certain âge, par exemple.","code":"# Extraire les données pour toutes les unités ayant une vitesse égale à 24\nsubset(cars, speed == 24)\n>    speed dist\n> 46    24   70\n> 47    24   92\n> 48    24   93\n> 49    24  120"},{"path":"manipuler.html","id":"nommer-des-variables-dans-un-jeu-de-données","chapter":" 6 Manipuler","heading":"6.1.3 Nommer des variables dans un jeu de données","text":"Il est possible d’attribuer ou de modifier des noms à des colonnes ou des lignes d’un tableau de données. Les fonctions colnames() et rownames() sont alors utilisées. Contrairement aux autres fonctions, celles-ci se retrouvent à gauche de l’équation.Il importe de fournir autant de noms qu’il y de colonnes (ou lignes), et ce, en chaîne de caractères.","code":"colnames(cars) <-  c(\"vitesse\", \"distance\")\nhead(cars)\n>   vitesse distance\n> 1       4        2\n> 2       4       10\n> 3       7        4\n> 4       7       22\n> 5       8       16\n> 6       9       10"},{"path":"manipuler.html","id":"données-manquantes","chapter":" 6 Manipuler","heading":"6.1.4 Données manquantes","text":"Les devis de recherche et les jeux de données empiriques sont rarement parfaits et peuvent souvent contenir des données manquantes. R reconnaît les données manquantes lorsqu’elles sont identifiées comme NA (available). Plusieurs méthodes permettent de gérer les données manquantes. La méthode la plus simple est d’éliminer les unités ayant une donnée manquante, soit la suppression par liste (listwise suppression). Les fonctions natives de R recourrent à l’argument na.rm = TRUE. Si cela est impossible, la fonction na.omit() permet de créer des jeux de données sans les valeurs manquantes. Certaines fonctions, comme mean() ont des arguments pour gérer les données manquantes.","code":"# Un vecteur\nvaleurs <-  c(10, 12, 14, NA, 18)\n\n# La présence de NA empêche la moyenne d'être calculée\nmean(valeurs)\n> [1] NA\n\n# L'argument \"na.rm = TRUE\" gère les NA\nmean(valeurs, na.rm = TRUE)\n> [1] 13.5\n\n# na.omit omet les valeurs NA dans la nouvelle variable.\nvaleurs.nettoyees <-  na.omit(valeurs)\nmean(valeurs.nettoyees)\n> [1] 13.5"},{"path":"manipuler.html","id":"le-tidyverse","chapter":" 6 Manipuler","heading":"6.2 Le tidyverse","text":"Le nom tidyverse (Wickham et al., 2019) est une contraction de tidy (bien rangé) et de universe. Le tidyverse est fondé sur le concept de tidy data, développé par Hadley (Wickham, 2014). Il repose sur une philosophie d’organisation des données facilitant la gestion, la préparation et le nettoyage préalable aux analyses quantitatives.Plusieurs packages respectent cette philosophie et font partie intégrante du tidyverse, comme ggplot2 (présentation graphique), dplyr (manipulation de données), readr (importation de données), tibble (nouvelle catégorie de data frame), mais bien d’autres également. Ces packages font partie intégrante de l’univers tidy et sont téléchargés simultanément avec le package tidyverse.Pour utiliser le package tidyverse, il faut d’abord l’installer puis l’appeler.","code":"\n# Installer le package\ninstall.packages(\"tidyverse\")\n\n# Rendre le package accessible\nlibrary(tidyverse)"},{"path":"manipuler.html","id":"les-fonctions-utiles","chapter":" 6 Manipuler","heading":"6.3 Les fonctions utiles","text":"Un des avantages et originalité d’utiliser le tidyverse est d’obtenir l’opérateur %>% (appelée pipe en anglais que l’peut traduire par tuyau) qui provient originellement du package magrittr (Bache & Wickham, 2020) et est importé par dplyr. L’opérateur favorise la lisibilité et la productivité, car il est plus facile de suivre le flux de plusieurs fonctions à travers ces tuyaux que de revenir en arrière lorsque plusieurs fonctions sont imbriquées. En fait, il favorise la lecture par verbes, soit par action (fonction), dans une séquence temporelle intuitive. Si les arguments sont placés en une seule ligne, non seulement la ligne est-elle longue et complexe, voire illisible, mais, en plus, les éléments les plus à gauche (les premiers à la lecture) sont les derniers opérés. Si chacune des fonctions était en ligne, alors il faudrait écraser ou créer des variables temporaires inutiles tout simplement pour arriver à réaliser les fonctions. La philosophie tidyverse, par l’usage de %>%, évite tous ses problèmes.L’opérateur %>% s’ajoute à la fin d’une ligne syntaxe. Son fonctionnement se traduit par l’argument de la ligne à gauche est introduit dans la fonction de droite, et ce, du haut vers le bas. Il peut être commandé plus rapidement avec le raccourci Ctrl + Shift + M sur Windows. En plus de l’opérateur %>% , dplyr offre de nouvelles fonctions pour gérer un jeu de données. Quelques-unes des plus importantes sont décrites ici. Par la suite, une mise en situation permet de mieux comprendre leur fonctionnement.","code":""},{"path":"manipuler.html","id":"sélectionner-des-variables","chapter":" 6 Manipuler","heading":"6.3.1 Sélectionner des variables","text":"Pour sélectionner des données d’un très grand jeu de données, la fonction select() permet de choisir les variables à conserver. Pour utiliser la fonction, il suffit d’indiquer les variables par leur nom de colonne dans la fonction. Aucun besoin de guillemets anglophones.","code":""},{"path":"manipuler.html","id":"sélectionner-des-participants","chapter":" 6 Manipuler","heading":"6.3.2 Sélectionner des participants","text":"Pour filtrer les participants selon les caractéristiques désirées, la fonction filter() permet de sélectionner les unités satisfaisant les conditions spécifiées. Pour utiliser la fonction, il faut indiquer le ou les arguments conditionnels à respecter et sur quelle variable.Dans ce contexte la fonction na.() peut être utile pour retirer une valeur aberrante.","code":""},{"path":"manipuler.html","id":"transformer-et-créer-des-variables","chapter":" 6 Manipuler","heading":"6.3.3 Transformer et créer des variables","text":"Pour créer ou transformer des variables, la fonction mutate() permettra de créer de nouvelles variables à partir des valeurs déjà dans le jeu de données. Il suffit d’indiquer dans la fonction, le calcul qui doit être opérer.","code":""},{"path":"manipuler.html","id":"sommariser-les-informations-pertinentes","chapter":" 6 Manipuler","heading":"6.3.4 Sommariser les informations pertinentes","text":"Pour obtenir des informations sur le jeu de données ainsi créées, la fonction summarise() permettra notamment d’obtenir des statistiques d’intérêt. En ajoutant, dans la fonction, les fonctions désirées, comme mean() ou sd(), avec les variables sur lesquelles elles devraient être opérées ou encore n() pour connaître la taille des groupes.S’il y des groupes ou des catégories, le sommaire peut être divisé avec la fonction group_by() où la variable nominale est spécifiée.","code":""},{"path":"manipuler.html","id":"autres-fonctions","chapter":" 6 Manipuler","heading":"6.3.5 Autres fonctions","text":"Il existe plusieurs autres fonctions possibles. Notamment, slice() permet de choisir les unités désirées en passant comme argument la base de données et le ou les numéros de ligne; sample_slice() qui est très similaire, retourne des lignes aléatoires; rename(), similaire à select(), permet de renommer les variables; arrange reclasse par ordre croissant en fonction d’une variable placée en argument. Et il y en plusieurs autres.","code":""},{"path":"manipuler.html","id":"mise-en-situation","chapter":" 6 Manipuler","heading":"6.4 Mise en situation","text":"Pour mettre en pratique la philosophie tidyverse, voici un exemple tiré du jeu de données starwars (disponible du tidyverse). Ce jeu de données possède de nombreuses caractéristiques (diversité de variables, de mesures, données manquantes) qui en font un jeu de données similaires à ce qu’un expérimentateur pourrait obtenir.Sans plus de préliminaire, la fonction head() donne un aperçu du jeu de donnéesPour obtenir de l’information sur ce jeu de données.Voici la description du jeu de données (traduction libre),Les données d’origine, issues de SWAPI, l’API de Star Wars, https://swapi.dev/, ont été révisées pour tenir compte des recherches supplémentaires sur la détermination du genre et du sexe des personnages.Peu utile comme descripteur, une inspection des données est plus informative. Pour afficher le jeu de données dans un nouvel onglet.Le fichier contient, le nom de 87 personnages mesurés sur 14 variables, soitle nom;le nom;la taille (cm);la taille (cm);le poids (kg);le poids (kg);la couleur des cheveux, de la peau et des yeux (trois variables);la couleur des cheveux, de la peau et des yeux (trois variables);l’année de naissance;l’année de naissance;le sexe biologique (mâle, femelle, hermaphrodite ou aucun);le sexe biologique (mâle, femelle, hermaphrodite ou aucun);le genre;le genre;la planète natale;la planète natale;l’espèce;l’espèce;une liste de films où le personnage apparaît;une liste de films où le personnage apparaît;une liste des véhicules que le personnage piloté;une liste des véhicules que le personnage piloté;une liste des vaisseaux que le personnage piloté.une liste des vaisseaux que le personnage piloté.À partir de ce jeu de données, l’objectif est de cette mise en situation est de comparer les hommes et les femmes humaines par rapport à leur indice de masse corporelle (IMC) ou body mass index (BMI). Le calcul de l’IMC consiste à diviser le poids par la taille au carré (kg/m2).Les étapes à considérer sont les suivantes : sélectionner les variables pertinentes, filtrer en retirant les unités d’espèces non humaines, tenir compte des données manquantes, corriger la taille des unités qui devrait être en mètre et non en centimètre (divisé par 100) et créer l’indice de masse corporelle.Les étapes de la syntaxe se lisent comme suit :La première ligne starwars %>% indique l’objet sur lequel il faut passer les fonctions subséquentes et la sortie est assignée à jd;La première ligne starwars %>% indique l’objet sur lequel il faut passer les fonctions subséquentes et la sortie est assignée à jd;puis, select(sex, mass, height, species) %>% indique les variables à conserver pour les fonctions subséquentes;puis, select(sex, mass, height, species) %>% indique les variables à conserver pour les fonctions subséquentes;puis, filter(species == \"Human\") filtre les unités qui sont humains et passe aux fonctions subséquentes;puis, filter(species == \"Human\") filtre les unités qui sont humains et passe aux fonctions subséquentes;puis, na.omit() %>% retire les valeurs manquantes des unités dans le jeu de données et passe aux fonctions subséquentes;puis, na.omit() %>% retire les valeurs manquantes des unités dans le jeu de données et passe aux fonctions subséquentes;puis, mutate(height = height  / 100) %>%, transforme la variable height et passe à la dernière fonction;puis, mutate(height = height  / 100) %>%, transforme la variable height et passe à la dernière fonction;enfin, mutate(IMC = mass / height^2) crée la variable d’IMC.enfin, mutate(IMC = mass / height^2) crée la variable d’IMC.Si une méthode plus traditionnelle avait été utilisée, la syntaxe pourrait ressembler à ceci.Le jeu de données est créé en autant de ligne de syntaxe. Par contre, la lecture n’est pas aussi intuitive qu’avec l’utilisation de l’opérateur %>% et des fonctions associées select(), filter(), mutate(). Il ne faut pas trop penser à quoi ressemblerait ces manipulations en une seule ligne de syntaxe.Une fois le jeu de données prêt, il est possible d’obtenir les informations sommaires. Ici, la moyenne, l’écart type, la valeur minimale et maximale ainsi que le nombre de données sont demandés en fonction du sexe. À cette étape, l’avantage d’embrasser la philosophie tidyverse apparaît, en quelques lignes rudimentaires, les cinq statistiques demandées sont affichées, et ce, par groupes.Le jeu de données issu de ces opérations peut être utilisée normalement pour réaliser des analyses statistiques. Il existe des packages afin de demeurer dans le tidyverse comme rstatix où il est possible de faire des test-\\(t\\) avec test_t() ou des corrélations avec cor_test(), par exemple. Voir la documentation complète du package pour une vue d’ensemble de ce qu’il est possible d’accomplir avec rstatix. Cela dit, l’utilisateur préférera probablement utiliser d’autres méthodes lorsque des analyses statistiques seront nécessaires.","code":"starwars[,1:6]\n> # A tibble: 87 x 6\n>    name         height  mass hair_color skin_color eye_color\n>    <chr>         <int> <dbl> <chr>      <chr>      <chr>    \n>  1 Luke Skywal~    172    77 blond      fair       blue     \n>  2 C-3PO           167    75 <NA>       gold       yellow   \n>  3 R2-D2            96    32 <NA>       white, bl~ red      \n>  4 Darth Vader     202   136 none       white      yellow   \n>  5 Leia Organa     150    49 brown      light      brown    \n>  6 Owen Lars       178   120 brown, gr~ light      blue     \n>  7 Beru Whites~    165    75 brown      light      blue     \n>  8 R5-D4            97    32 <NA>       white, red red      \n>  9 Biggs Darkl~    183    84 black      light      brown    \n> 10 Obi-Wan Ken~    182    77 auburn, w~ fair       blue-gray\n> # ... with 77 more rows\n?starwars\nView(starwars)jd <-  starwars %>% \n  select(sex, mass, height, species) %>% \n  filter(species == \"Human\") %>% \n  na.omit() %>% \n  mutate(height = height  / 100) %>% \n  mutate(IMC = mass / height^2)  \njd\n> # A tibble: 22 x 5\n>    sex     mass height species   IMC\n>    <chr>  <dbl>  <dbl> <chr>   <dbl>\n>  1 male      77   1.72 Human    26.0\n>  2 male     136   2.02 Human    33.3\n>  3 female    49   1.5  Human    21.8\n>  4 male     120   1.78 Human    37.9\n>  5 female    75   1.65 Human    27.5\n>  6 male      84   1.83 Human    25.1\n>  7 male      77   1.82 Human    23.2\n>  8 male      84   1.88 Human    23.8\n>  9 male      80   1.8  Human    24.7\n> 10 male      77   1.7  Human    26.6\n> # ... with 12 more rowsjd <- starwars[, c(\"sex\", \"mass\", \"height\", \"species\")]  # select()\njd <- jd[jd[, \"species\"] == \"Human\",]                    # filter()\njd <- na.omit(jd)                                        # na.omit()\njd[,\"height\"] <- jd[,\"height\"] / 100                     # mutate()\njd[,\"IMC\"] <- jd[,\"mass\"] / jd[,\"height\"]^2              # mutate()\njd\n> # A tibble: 22 x 5\n>    sex     mass height species   IMC\n>    <chr>  <dbl>  <dbl> <chr>   <dbl>\n>  1 male      77   1.72 Human    26.0\n>  2 male     136   2.02 Human    33.3\n>  3 female    49   1.5  Human    21.8\n>  4 male     120   1.78 Human    37.9\n>  5 female    75   1.65 Human    27.5\n>  6 male      84   1.83 Human    25.1\n>  7 male      77   1.82 Human    23.2\n>  8 male      84   1.88 Human    23.8\n>  9 male      80   1.8  Human    24.7\n> 10 male      77   1.7  Human    26.6\n> # ... with 12 more rowsjd %>% \n  group_by(sex) %>% \n  summarise(mean(IMC), sd(IMC), min(IMC), max(IMC), length(IMC)) \n> # A tibble: 2 x 6\n>   sex    `mean(IMC)` `sd(IMC)` `min(IMC)` `max(IMC)`\n>   <chr>        <dbl>     <dbl>      <dbl>      <dbl>\n> 1 female        22.0      5.51       16.5       27.5\n> 2 male          26.0      4.29       21.5       37.9\n> # ... with 1 more variable: `length(IMC)` <int>library(rstatix)\n\n# Test-t sur l'IMC en fonction du sexe\njd %>% \n  t_test(IMC ~ sex)\n> # A tibble: 1 x 8\n>   .y.   group1 group2    n1    n2 statistic    df     p\n> * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n> 1 IMC   female male       3    19     -1.23  2.40 0.326\n\n# Analyse de corrélations\njd %>% \n  select(IMC, mass, height) %>% \n  cor_test() \n> # A tibble: 9 x 8\n>   var1   var2     cor statistic         p conf.low conf.high\n>   <chr>  <chr>  <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n> 1 IMC    IMC     1      2.12e+8 5.26e-155    1.00      1    \n> 2 IMC    mass    0.85   7.32e+0 4.47e-  7    0.674     0.938\n> 3 IMC    height  0.18   8.13e-1 4.26e-  1   -0.262     0.558\n> 4 mass   IMC     0.85   7.32e+0 4.47e-  7    0.674     0.938\n> 5 mass   mass    1      3.00e+8 5.13e-158    1         1    \n> 6 mass   height  0.65   3.84e+0 1.02e-  3    0.317     0.842\n> 7 height IMC     0.18   8.13e-1 4.26e-  1   -0.262     0.558\n> 8 height mass    0.65   3.84e+0 1.02e-  3    0.317     0.842\n> 9 height height  1    Inf       0            1         1    \n> # ... with 1 more variable: method <chr>"},{"path":"visualiser.html","id":"visualiser","chapter":" 7 Visualiser","heading":" 7 Visualiser","text":"La visualisation de données est l’un des deux objectifs fondamentaux de R (l’autre étant évidemment de faire des statistiques). Il existe plusieurs méthodes et packages pour produire rapidement et simplement des graphiques. Beaucoup de matériel se retrouve en ligne pour maîtriser les graphiques, mais surtout les personnaliser. L’objectif, bien modeste, de cette section n’est pas de rendre le lecteur maître de la production de figures, mais bien de lui faire faire ses premiers pas et de l’outiller pour qu’il puisse produire simplement et rapidement des graphiques de qualité.L’exemple de cette section est basé sur celle de la section Manipuler. Voici la syntaxe pour obtenir le jeu de données de nouveau.","code":"\njd <- starwars %>% \n  select(name, sex, mass, height, species) %>% \n  filter(species == \"Human\") %>% \n  na.omit() %>% \n  mutate(height = height  / 100) %>% \n  mutate(IMC = mass / height^2)  "},{"path":"visualiser.html","id":"ggplot2","chapter":" 7 Visualiser","heading":"7.1 ggplot2","text":"Le package ggplot2 est une extension du tidyverse avec lequel il est possible de créer simplement et rapidement des graphiques. Ces graphiques sont de qualité de publications, idéale pour les articles scientifiques. Le package fournit un langage graphique pour la création intuitive de graphiques compliqués. Il permet à l’utilisateur de créer des graphiques qui représentent des données numériques et catégorielles univariées et multivariées.La logique de ggplot2 repose sur la grammaire des graphiques (Grammar Graphics), l’idée selon laquelle toutes les figures peuvent être construites à partir des mêmes composantes. Il s’agit de la deuxième version du package. Voilà pour l’appellation ggplot2.Dans la grammaire de graphique, une figure possède huit niveaux, dont les trois principaux sont les suivants :data, les données utilisées;data, les données utilisées;mapping (aesthetic), cartographier les variables, c’est-à-dire, établir la carte des variables (abscisses, ordonnées, couleur, forme, taille, etc.);mapping (aesthetic), cartographier les variables, c’est-à-dire, établir la carte des variables (abscisses, ordonnées, couleur, forme, taille, etc.);geometric représentation, la représentation géométrique ou le type de représentation graphique, par exemple, diagramme de dispersion, histogramme, boîte à moustache, etc.geometric représentation, la représentation géométrique ou le type de représentation graphique, par exemple, diagramme de dispersion, histogramme, boîte à moustache, etc.Les cinq autres son, statistics, facet, coordinate space, labels, theme permettent de personnaliser la figure.Les composantes les plus importantes sont les trois premières, soit les données, la cartographie et la représentation géométrique. Ce sont les éléments de base pour débuter le graphique. Les autres composantes viennent bonifier la figure tout en l’ajustant au besoin de l’utilisateur.La fonction ggplot() met en place la figure. Le résultat d’utiliser la fonction ggplot() seule est illustrée à la Figure 7.1\nFigure 7.1: La fonction ggplot() seule - Rien\nIl est aussi possible de piper (prononcé avec un fort accent anglophone) les données dans la fonction.Pour afficher des graphiques, il faut ajouter +, puis une représentation géométrique ainsi que la cartographie (mapping). La cartographie (aes(mapping = ), où aes désigne l’esthétisme, aesthetic) peut se trouver dans ggplot() ou dans la représentation géométrique. Si elle est dans ggplot, elle est passée aux autres niveaux.Voici une liste des représentations géométriques possibles :geom_line() crée une ligne qui lie toutes les valeurs, très utiles pour une série temporelle (abscisse = temps, ordonnée = variable dépendante);geom_line() crée une ligne qui lie toutes les valeurs, très utiles pour une série temporelle (abscisse = temps, ordonnée = variable dépendante);geom_point() crée un diagramme de dispersion ou un nuage de point, très utile pour les corrélations;geom_point() crée un diagramme de dispersion ou un nuage de point, très utile pour les corrélations;geom_bar() crée un diagramme à bâton, idéal pour présenter des proportions, des fréquences ou des données comptées;geom_bar() crée un diagramme à bâton, idéal pour présenter des proportions, des fréquences ou des données comptées;geom_histogram() crée un histogramme des variables;geom_histogram() crée un histogramme des variables;geom_box() crée une boîte à moustache, idéal pour identifier des valeurs aberrantes et comparer la variabilité entre des groupes;geom_box() crée une boîte à moustache, idéal pour identifier des valeurs aberrantes et comparer la variabilité entre des groupes;geom_smooth() crée la ligne de prédiction des données avec des intervalles de confiances, la plupart des utilisateurs voudront certainement ces arguments method = lm (par défaut) ou sans l’erreur standard (se = FALSE);geom_smooth() crée la ligne de prédiction des données avec des intervalles de confiances, la plupart des utilisateurs voudront certainement ces arguments method = lm (par défaut) ou sans l’erreur standard (se = FALSE);geom_errorbar() ajoute des barres d’erreur ou des intervalles de confiances spécifiées.geom_errorbar() ajoute des barres d’erreur ou des intervalles de confiances spécifiées.Certaines cartographies sont d’ailleurs compatibles, geom_smooth() et geom_point(), par exemple.La Figure 7.2 montre un diagramme de dispersion construit à partir du jeu de données jd pipé dans la fonction ggplot(). Dans cette fonction, la cartographie est passée mapping = aes(x = mass, y = height) à un second niveau, geom_point) par le + et la représentation est produite.\nFigure 7.2: Diagramme de dispersion\nVoici une liste d’exemples de différentes représentations visuelles des données.","code":"\nggplot(data = jd)\njd %>% \n  ggplot()\njd %>% \n  ggplot(mapping = aes(x = mass, y = height)) + \n  geom_point()"},{"path":"visualiser.html","id":"diagramme-de-dispersion","chapter":" 7 Visualiser","heading":"7.2 Diagramme de dispersion","text":"Pour réaliser un diagramme de dispersion, la fonction se nomme geom_point(). La cartographie identifie la variable à l’axe des \\(x\\) (horizontal) et des \\(y\\) (vertical). Dans cet exemple, il s’agit du poids (\\(x\\)) et de la taille (\\(y\\)). La cartographie ne se limite pas aux axes par contre. Dans cet exemple, la forme shape est aussi un dimension manipulée. Il peut s’agir de color et même de size. Dans la syntaxe ci-dessous, l’argument size est placé à l’extérieur de mapping. Il s’agit alors d’une constante (elle change la taille des points), c’est-à-dire qu’elle ne varie pas relativement à une variable.\nFigure 7.3: Le lien entre le poids et la taille en fonction du sexe\nLa Figure 7.4 montre le résultat si `size``est ajouté au mapping pour identifier l’IMC. Les unités avec un plus grand IMC obtiennent un plus gros pointeur.\nFigure 7.4: Le lien entre le poids et la taille en fonction de l’IMC et du sexe\npeut y ajouter la droite de régression, comme la Figure 7.5 le montre. Sans geom_point(), la figure ne produit la droite. Les arguments de geom_smooth() indique l’utilisation du modèle linéaire, method = lm, et l’absence des intervalles de confiance, se = FALSE. Dans cette syntaxe, comme le mapping est ajouté à ggplot directement, il se généralise directement à geom_point() et geom_smooth()\nFigure 7.5: Le lien entre le poids et la taille en fonction de l’IMC\n","code":"\njd %>% \n  ggplot() + \n  geom_point(mapping = aes(x = mass, y = height, shape = sex), size = 2) \njd %>% \n  ggplot() + \n  geom_point(mapping = aes(x = mass, y = height, shape = sex, size = IMC)) jd %>% \n  ggplot(mapping = aes(x = mass, y = height)) + \n  geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\")\n> `geom_smooth()` using formula 'y ~ x'"},{"path":"visualiser.html","id":"boîte-à-moustache","chapter":" 7 Visualiser","heading":"7.3 Boîte à moustache","text":"La boîte à moustaches (box--whisker plot) est une figure permettant de voir la variabilité des données. Elle résume seulement quelques indicateurs de position soit la médiane, les quartiles, le minimum, et le maximum. Ce diagramme est utilisé principalement pour détecter des valeurs aberrantes et comparer la variabilité entre les groupes. C’est la représentation géométrique geom_boxplot() qui permettra de créer des boîtes à moustache. La cartographie prend en argument un variable nominale en x et une variable continue en y. La Figure 7.6 montre un exemple de boîte à moustache.\nFigure 7.6: Boîte à moustache de l’IMC en fonction du sexe\nUne fonction intéressante est la fonction coord_flip() qui tourne (flip) les axes, les coordonnées. L’axe \\(x\\) prend la place de \\(y\\); \\(y\\) prend la place de \\(x\\). Elle peut être pratique pour améliorer la qualité visuelle de certains graphiques.","code":"\nggplot(data = jd) + \n  geom_boxplot(mapping = aes(x = sex, y = IMC)) +\n  coord_flip()"},{"path":"visualiser.html","id":"histogramme","chapter":" 7 Visualiser","heading":"7.4 Histogramme","text":"Un histogramme permet de représenter la répartition empirique d’une variable. Il donne un aperçu de la distribution sous-jacente, soit comment les données sont distribuées. Cette figure permet de voir la forme de la distribution et permet de voir si elle ne démontre pas d’anomalie. La représentation graphique geom_histogram() produit des histogrammes. S’il faut en produire pour différentes variables, une stratégie simple est de les produire en série.Des techniques plus avancées permettent de créer la Figure 7.7 d’un seul coup.\nFigure 7.7: Histogrammes des variables continues\nEnfin, s’il est désiré de comparer deux distributions de groupes différents, l’argument fill dans la cartographie indique à la fonction de différencier les valeurs selon le remplissage des histogrammes.\nFigure 7.8: Histogrammes de l’IMC par rapport au sexe\nDans la Figure 7.8, l’argument position = \"identity\" indique de traiter les deux groupes comme différents, autrement les colonnes s’additionnent dans le graphique. L’argument alpha = .7 permet une transparence entre les couleurs, autrement, les valeurs derrière les autres ne paraissent pas. La valeur de alpha va de 0 (transparent) à 1 (opaque) et fonctionne dans la plupart des contextes, surtout ceux liés à ggplot2.","code":"\n# Trois histogrammes en trois figures\nggplot(data = jd) + \n  geom_histogram(mapping = aes(x = height))\n\nggplot(data = jd) + \n  geom_histogram(mapping = aes(x = mass))\n\nggplot(data = jd) + \n  geom_histogram(mapping = aes(x = IMC))\n# Trois histogrammes en une seule figure\n# en optimisant avec le tidyverse\njd %>%\n  keep(is.numeric) %>% \n  gather() %>% \n  ggplot(aes(value)) +\n  facet_wrap(~ key, scales = \"free\") +\n  geom_histogram()jd %>% \n  ggplot(mapping = aes(x = IMC, fill = sex)) + \n  geom_histogram(position = \"identity\", alpha = .7) + \n  scale_fill_grey()\n> `stat_bin()` using `bins = 30`. Pick better value with\n> `binwidth`."},{"path":"visualiser.html","id":"les-barres-derreurs","chapter":" 7 Visualiser","heading":"7.5 Les barres d’erreurs","text":"Les barres d’erreur sont une représentation géométrique à part entière. La fonction pour les commandées est geom_errorbar(). Elle nécessite deux arguments, soit l’intervalle de confiance maximale et minimale autour des moyennes à afficher.La Figure 7.9 illustre les différences entre moyennes avec des barres d’erreur à partir de la base de données ToothGrowth, une étude de l’effet de la vitamine C (dose) selon leur administration (jus ou supplément supp) sur la longueur des dents des cochons d’inde. Il y deux facteurs et une variable continue.La première étape est de tirer les statistiques sommaires, moyennes, écart type, tailles des groupes. La syntaxe tire profit de groupe_by() pour tirer les groupes et en faire le sommaire. Le sommaire summarise permet d’obtenir les statistiques, notamment la moyenne, l’erreur standard (se) pour en calculer l’intervalle autour de la moyenne ci.\nFigure 7.9: Les effets de la vitamine C sur les cochons d’inde\nUne fois ces statistiques calculées et enregistrées dans le nouveau jeu de données jd, il est possible de créer le graphique avec les représentations géométriques désirées. Remarquez comment spécifier la cartographie dans le niveau ggplot() rend la syntaxe moins compliquée. Cette syntaxe produit un graphique avec dose à l’axe des \\(x\\), supp comme pointeurs et les moyennes de len (longueur moyenne des dents). La fonction geom_errorbar() indique où placer les limites inférieures et supérieures des intervalles. Les arguments size = 5 et width = .05 sont ajoutés simplment pour l’esthétisme. L’argument .groups = \"drop\" de summarise permet d’éviter une avertissement expliquant qu’une variable de groupement est utilisé pour regrouper les résultats à la fin. Ajouter ou retirer cet argument ne change pas les calculs, ni la Figure 7.9.","code":"\njd = ToothGrowth %>% \n  group_by(dose, supp) %>% \n  summarise(mlen = mean(len),\n            sdlen = sd(len),\n            nlen = n(), \n            se = sd(len)/sqrt(n()), \n            ci = qt(.975, df = n()-1) * se,\n            .groups = \"drop\")\n\njd %>% \n  ggplot(aes(x = dose,\n             y = mlen, \n             shape = supp),\n         size = 5) + \n    geom_errorbar(aes(ymin = mlen - ci,\n                      ymax = mlen + ci), \n                  width = .05) +\n    geom_line() +\n    geom_point()"},{"path":"visualiser.html","id":"pour-aller-plus-loin","chapter":" 7 Visualiser","heading":"7.6 Pour aller plus loin","text":"Il existe une multitudes de livres, de sites web, de tutoriels en ligne et d’atelier pour donner l’occasion au lecteur d’aller plus loin dans sa conception graphique. Voici quelques ouvrages de références : Le R Graphics Cookbook (Chang) repérable à https://r-graphics.org/, ggplot2: elegant graphics data analysis (Wickham) repérable à https://ggplot2-book.org/ ou R Graphics (Murrel) repérable à https://www.stat.auckland.ac.nz/~paul/RG2e/.","code":""},{"path":"exercice-gestion.html","id":"exercice-gestion","chapter":"Exercices","heading":"Exercices","text":"TODOENTRERÀ l’aide de data_edit() du package DataEditR, créez un jeu données contenant trois participants ayant les caractéristiques suivantes, nom = Alexandre, Samuel et Vincent et age = 20, 22 et 31.IMPORTERMANIPULERPrenez le jeu de données cars, sélectionner la variable dist et transformer la en mètre, plutôt qu’en pieds. Rappel: un mètre égale 3.2808 pieds.VISUALISERPrenez le jeu de données mtcars et produisez un diagramme de dispersion montrant la puissance brute (en chevaux) (hp) par rapport à consommation en km/l (basé sur mpg) tout en soulignant l’effet du nombre de cylindres (cyl). Attention la fonction as_factor permettra d’utiliser cyl en facteur.Prenez le jeu de données mtcars et produisez un diagramme de dispersion montrant la puissance brute (en chevaux) (hp) par rapport à consommation en km/l (basé sur mpg) tout en soulignant l’effet du nombre de cylindres (cyl). Attention la fonction as_factor permettra d’utiliser cyl en facteur.Prenez le jeu de données mtcars et produisez un histogramme montrant la variabilité de la consommation mpg par rapport à la transmission (). Attention la fonction as_factor permettra d’utiliser en facteur.Prenez le jeu de données mtcars et produisez un histogramme montrant la variabilité de la consommation mpg par rapport à la transmission (). Attention la fonction as_factor permettra d’utiliser en facteur.Prenez le jeu de données msleep et produisez un diagramme à bâton pour observer la fréquence de différents type de régime (vore). Attention aux données manquantes.Prenez le jeu de données msleep et produisez un diagramme à bâton pour observer la fréquence de différents type de régime (vore). Attention aux données manquantes.Prendez le jeu de données msleep et produisez une boîte à moustache pour observer le temps total de sommeil (sleep_total) par rapport aux régimes (vore). Attention aux données manquantes.Prendez le jeu de données msleep et produisez une boîte à moustache pour observer le temps total de sommeil (sleep_total) par rapport aux régimes (vore). Attention aux données manquantes.","code":"\n# Installer et appeler le package DataEditR, si ce n'est fait\njd <- DataEditR::data_edit()"},{"path":"inférer.html","id":"inférer","chapter":" 8 Inférer","heading":" 8 Inférer","text":"Le principal de toute inférence statistique est de tirer des conclusions sur une population à partir d’un échantillon (un fragment beaucoup plus petit de la population). Comme il est rarement possible de collecter des données sur l’ensemble de la population, l’expérimentateur choisi, idéalement, un échantillon représentatif tiré aléatoirement. Une fois l’échantillon recruté et mesuré, l’expérimentateur dérive des indices statistiques. Un indice statistique synthétise par une estimation basée sur l’échantillon de l’information sur le paramètre de la population. Cet indice possède un comportement, une distribution d’échantillonnage qui détermine les différentes valeurs qu’il peut prendre. En obtenant ces indices, l’expérimentateur tente de connaître le paramètre de la population. S’il s’intéresse à la relation entre l’anxiété et un cours de méthodes quantitatives, l’expérimentateur voudra savoir d’une part si cette relation n’est pas nulle, mais aussi sa force, en termes de tailles d’effet.Cette tâche peut apparaître difficile considérant le peu d’informations sur la population, sa distribution de probabilité, les paramètres et la relative petite taille de l’échantillon par rapport à la population. Pour aider l’expérimentateur, les statisticiens ont le théorème central limite. Pour eux, il est certainement l’équivalent de la théorie de l’évolution pour le biologiste ou la théorie de la relativité générale pour le physicien. Ce théorème permet de connaître comment et sous quelles conditions se comportent les variables aléatoires.","code":""},{"path":"inférer.html","id":"le-théorème-central-limite","chapter":" 8 Inférer","heading":"8.1 Le théorème central limite","text":"Les valeurs d’un échantillon sont, pour le statisticien, des variables aléatoires. Une variable aléatoire, c’est un peu comme piger dans une boîte à l’aveuglette pour obtenir une valeur. La boîte est impénétrable, personne ne sait par quel processus elle accorde telle ou telle autre valeur. Pour le statisticien, ce qui importe c’est que chaque valeur possède une chance égale aux autres d’être sélectionnée et qu’elles soient indépendantes entre elles (le fait d’en choisir une soit sans conséquence sur la probabilité des autres).Pour le non-initié aux fonctions permettant de créer des nombres pseudoaléatoires, une fonction R comme rnorm() ou runif() (r suivi d’un nom de distribution, voir Les distributions) joue parfaitement le rôle de cette boîte. Si l’usager demande une valeur, la fonction retourne une valeur aléatoire (imprévisible à chaque fois) sans connaître comment cette valeur est produite.Le statisticien s’intéresse à inférer comment ces valeurs sont générées. Il postule ainsi que les valeurs aléatoires suivent une distribution de probabilité. Connaître cette distribution est très important, car c’est elle qui permet de répondre à des questions comme : quelle est la probabilité d’obtenir un résultat aussi rare que \\(x\\)? Ou quelle sont les valeurs attendues pour \\(95\\%\\) des tirages? Questions tout à fait pertinentes pour l’expérimentateur. Une des distributions les plus connues est certainement la distribution normale, celle qui est derrière la fonction rnorm() d’ailleurs. Mais, il y en beaucoup, beaucoup d’autres.Lorsque plus d’une variable sont issues d’une même boîte (distribution), elles sont identiquement distribuées. Si ces variables aléatoires sont combinées, que ce soit en termes de produit, de quotient, d’addition, de soustraction, le résultat est une nouvelle variable aléatoire qui possède sa propre distribution nommée distribution d’échantillonnage. Sur le plan de la syntaxe R, il s’agit de réaliser des opérations mathématiques avec des variables aléatoires identiquement distribuées.Entre en jeu le théorème central limite: plus des variables aléatoires identiquement distribuées sont additionnées ensemble, plus la distribution de probabilité de cette somme se rapproche d’une distribution normale.Par exemple, la fonction rlnorm() génère des variables issues d’une distribution log normale. Elle la forme illustré à la Figure 8.1 (qui n’rien de normal à première vue).\nFigure 8.1: Distribution log normale\nEn calculant la somme de plusieurs variables aléatoires de cette distribution, pour diverses valeurs de tailles d’échantillons (nombre de variables échantillonnées), les résultats tendent de plus en plus vers une distribution normale. Le code ci-dessous présente la démarche utilisée et la Figure 8.2 en fait la démonstration graphique en présentant les distributions d’échantillonnage obtenues.La Figure 8.2 montre que la distribution d’échantillonnage de la somme des variables converge vers une distribution normale à mesure que la taille d’échantillon \\(n\\) augmente. Cela est vrai pour n’importe quelle distribution de probabilité de la population. Le théorème central en dit plus que simplement la forme de la distribution. Elle affirme également qu’une distribution de probabilité d’une population ayant une moyenne \\(\\mu\\) et un écart type \\(\\sigma\\) échantillonnées sur \\(n\\) unités, génére une distribution d’échantillonnage des totaux (indicé \\(t\\)) ayant une espérance (la moyenne) de \\(n\\mu_t\\) et un écart type de \\(n\\sigma_t^2\\).\nFigure 8.2: Distributions des totaux de n variables log normales\nLes expérimentateurs ne connaissent pas les distributions sous-jacentes aux valeurs des unités issues de la population. Par contre, à l’aide des statisticiens et du théorème central limite, ils savent comment se comportent les sommes des variables. Les expérimentateurs s’intéressent toutefois rarement aux sommes de variable… à l’exception de la moyenne qui est une somme de variable divisée par la constante \\(n\\)5. Dans le cas de la moyenne, le théorème central limite stipule qu’une distribution de probabilité ayant une moyenne \\(\\mu\\) et un écart type \\(\\sigma\\) dont l’échantillon est constitué de \\(n\\) unités, génére une distribution d’échantillonnage des moyennes avec une espérance de \\(\\mu_{\\bar{x}}\\) et un écart type de \\(\\sigma_{\\bar{x}}/\\sqrt{n}\\).Dans la mesure où l’expérimentateur connaît la distribution de la population (extrêmement rare, mais permet de mieux illustrer la théorie) ou qu’il recoure à une distribution d’échantillonnage connue, il peut inférer la probabilité d’une variable aléatoire par rapport à ce qui est attendu simplement par hasard. Il juge alors si cette variable est trop rare par rapport à l’hypothèse de base (nulle).La théorie traditionnelle des tests d’hypothèses repose sur l’idée selon laquelle compare la vraisemblance d’une variable aléatoire estimée auprès d’un échantillon par rapport à une hypothèse nulle (l’absence d’effet). En épistémologie des sciences, il n’est pas possible de montrer l’exactitude d’une hypothèse, seulement son inexactitude. Cela rappelle le principe du falsificationnisme selon lequel ne peut prouver une hypothèse, ne peut que la falsifier. En statistiques, c’est la rareté d’une donnée qui agira comme indice d’inexactitude. Si la variable aléatoire est trop rare pour l’hypothèse nulle, celle-ci est rejetée : d’autres hypothèses doivent être considérées pour expliquer ce résultat. Autrement, l’hypothèse nulle n’est pas rejetée, les preuves sont insuffisantes pour informer l’expérimentateur sur l’hypothèse nulle.","code":"runif(n = 1)\n> [1] 0.87# Pour répliquer\nset.seed(1)\n\n# Création de deux variables identiquement distribuées\na <- runif(n = 1)\nb <- runif(n = 1)\na ; b\n> [1] 0.266\n> [1] 0.372\n\n# Une nouvelle variable aléatoire\ntotal <- a + b\n# Cette fonction sort les nombres, mais pas les graphiques.\n# Différentes tailles d'échantillons\nN <- seq(10, 90, by = 10)\n# Nombre de tirage pour chaque élément de N\nreps <- 1000\n\n# Une boucle pour tester toutes les possibilités\nfor(n in N){\n  total <- as.numeric()\n  for(i in 1:reps){\n    # Faire la somme de n valeurs tirés d'une distribution log normale\n    total[i] <- sum(rlnorm(n))\n  }\n  # hist(total) \n}"},{"path":"inférer.html","id":"inférence-avec-la-distribution-normale-sur-une-unité","chapter":" 8 Inférer","heading":"8.2 Inférence avec la distribution normale sur une unité","text":"Un excellent exemple en sciences humaines et sociales où la distribution de probabilité de la population est connue est le quotient intellectuel (QI). Le QI d’une population occidentale est distribué normalement (établi intentionnellement par les psychométriciens) avec une moyenne de 100 (\\(\\mu=100\\)) et un écart type de \\(\\sigma = 15\\). Ces valeurs sont totalement arbitraires, il est tout aussi convenable de parler d’une moyenne de 0 et d’un écart type de 1 (la distribution peut être standardisée) quoiqu’il est contre-intuitif de parle d’un QI de 0. (Qui voudrait avoir une intelligence de 0?)Dans la population, bien que la moyenne et la variance peuvent être connues, sélectionner une unité au hasard génère une variable aléatoire. Chaque individu de la population une probabilité très faible d’être sélectionné et est indépendant des autres individus de la population. Il est très difficile de prédire le score exact d’une personne. Toutefois, il est possible d’avoir un idée de la variabilité des scores. La Figure 8.3 montre la distribution normale par rapport à la moyenne, \\(\\mu\\) pour différentes valeurs d’écart type, \\(\\sigma\\). Elle montre que 68.269% devrait se retrouvé entre plus ou moins un écart types ou encore que 95.45% devrait se retrouvé entre plus ou moins deux écarts types. Ajuster au QI, il s’agit de 85 à 115 et de 70 à 130 respectivement\nFigure 8.3: La distribution normale du QI\nUne autre façon de fonctionner est de prendre une personne au hasard et de mesurer son QI. Le score obtenu est une valeur aléatoire. Comme la distribution est connue avec ses paramètres, il est possible de juger de la vraisemblance de ce score (est-il rare?) par rapport à la population.Voici un exemple où ces informations sont pertinentes. Un groupe d’expérimentateurs mettent en place un outil d’évaluation qui teste si un individu donné est un humain ou un reptilien (une race d’extra-terrestre). Leur outil n’est pas si sophistiqué. En fait, il se base sur le QI, car les expérimentateurs ont remarqué que les reptiliens ont un QI beaucoup plus élevé que le QI humain.La distribution normale du QI humain joue le rôle d’hypothèse nulle, les personnes mesurées sont admises humaines jusqu’à preuve du contraire (une présomption d’innocence en quelque sorte), un QI trop élevé suggérant la culpabilité.Les expérimentateurs émettent l’hypothèse que les 5 % personnes ayant le plus haut QI sont vraisemblablement reptiliens. C’est le risque qu’ils sont prêts à prendre de sélectionner un humain et de le classer erronément comme reptilien.Le groupe d’expérimentateurs teste leur instrument sur Fanny. Elle un QI de 120. La Figure 8.4 illustre la distribution de l’intelligence dans la population et où se situe Fanny parmi celle-ci. Comment tester si elle est reptilienne?\nFigure 8.4: Score de Fanny sur la distribution normale\nLa première étape est d’obtenir un score-\\(z\\). Un score-\\(z\\) est une échelle standardisée des distances d’une valeur par rapport à la moyenne. Lorsqu’une échelle de mesure est transformée en score-\\(z\\), la moyenne est de 0 et l’écart type est égal à 1. Cela permet de mieux apprécier les distances et leur probabilité. Un score-\\(z\\) s’obtient en prenant la différence entre une unité (\\(x\\)) par rapport à la moyenne (\\(\\mu\\)) divisée par l’écart type (\\(\\sigma\\)). L’Équation (8.1) illustre ce calcul.\\[\\begin{equation}\nz = \\frac{x-\\mu}{\\sigma}\n\\tag{8.1}\n\\end{equation}\\]Comme un score-\\(z\\) est standardisé, la Figure 8.3 est utilisable pour tirer des conclusions, car celle-ci applicable pour toutes sortes de situations où la distribution est vraisemblablement normale.Fanny un score-\\(z\\) de 1.333. Maintenant, il faut traduire cette valeur en probabilité.L’expectative sous l’hypothèse nulle est d’observer un score pareil ou supérieur à celui de Fanny 9.121 % du temps. Cette statistique correspond à la valeur-\\(p\\), la probabilité de l’indice par rapport à sa distribution d’échantillonnage (hypothèse nulle). Comme elle ne dépasse pas le seuil de 5%, soit la limite selon laquelle le score est jugé invraisemblable, l’hypothèse nulle n’est pas rejetée (elle est humaine!).Avec le critère d’identifier erronément les 5 % humains les plus intelligents, il s’agit, du même coup, du taux de faux positif acceptable de l’étude. Un faible sacrifice à réaliser afin identifier des reptiliens parmi les humains. La zone de rejet, c’est-à-dire la zone dans laquelle l’hypothèse nulle (humain) est rejetée, correspond à la zone ombragée à droite de la distribution.La logique des tests statistiques inférentiels repose sur cette série d’étapes : choisir un indice, connaître sa distribution sous-jacente, déterminer l’hypothèse nulle (généralement l’absence d’effet), calculer la probabilité de l’indice par rapport à cette hypothèse nulle.","code":"\nfanny <- 120\nz.fanny <- (fanny - 100) / 15# La probabilité que Fanny ait un QI de -Inf à z.fanny\npnorm(z.fanny)\n> [1] 0.909\n\n# En pourcentage\npnorm(z.fanny) * 100\n> [1] 90.9"},{"path":"inférer.html","id":"inférence-avec-la-distribution-normale-sur-un-échantillon","chapter":" 8 Inférer","heading":"8.3 Inférence avec la distribution normale sur un échantillon","text":"Jusqu’à maintenant, seule une unité d’observation était traitée. L’indice et la distribution étaient également spécifiés. Dans cette section, l’exemple est étendu aux échantillons (plus d’une unité d’observation).Fanny un QI de 120. Si une autre personne est sélectionnée, cette nouvelle personne aurait inévitablement un autre score. Cette logique s’applique également aux échantillons. L’exemple ci-dessous échantillonne 10 unités d’une population de QI distribuée normalement avec les paramètres usuels.\nFigure 8.5: Scores des unités de l’échantillon\nDans la Figure 8.5, chaque unité est présentée par un trait vertical noir. La moyenne de cet échantillon est de 108.8 et l’écart type est de 9.762. Dans cet exemple, l’indice est clairement identifié, mais quelle est la distribution d’échantillonnage des moyennes? Selon le théorème central limite, la moyenne de la distribution d’échantillonnage est \\(\\mu=\\mu_{\\bar{x}}=100\\) et l’écart type est \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}=\\frac{15}{\\sqrt{10}}=4.743\\) . Maintenant, il est possible de calculer un score-\\(z\\).\\[\nz = \\frac{\\bar{x}-\\mu_{\\bar{x}}}{\\sigma/\\sqrt{n}}=\\frac{108.8-100}{15/\\sqrt{10}}=1.855\n\\]\nCela donne le code suivant.La fonction (1 - pnorm(z)) * 100, retourne la probabilité (en pourcentage) d’un résultat plus rare que l’indice obtenu auprès de l’échantillon par rapport à la population. Comme pour l’exemple de Fanny, ce chiffre est une valeur-\\(p\\), soit la probabilité de l’indice observé par rapport à l’hypothèse nulle. La Figure 8.6 montre l’emplacement de l’échantillon sur la distribution d’échantillonnage. La probabilité de cet échantillon par rapport à l’hypothèse nulle est de 3.178 %, juste en deçà du 5 % fixé. Dans la Figure 8.6, la moyenne de l’échantillon se retrouve à l’intérieur de la zone de rejet (la zone grise). La conclusion est par conséquent de rejeter l’hypothèse nulle, l’échantillon semble provenir d’une autre distribution (avec des paramètres différents) que celle postulée.\nFigure 8.6: Moyenne de l’échantillon sur la distribution normale\nQu’en est-il vraiment de ce résultat? Pour l’expérimentateur, il ne peut aller plus loin, car il ne connaît pas la boîte noire selon laquelle les valeurs de l’échantillon sont générées. Il ne peut que constater que plusieurs (9/10) unités ont un score plus élevé que 100. Par contre, comme il s’agit d’un exemple simulé, la boîte noire est connue. C’est la fonction, round(rnorm(n = 10, mean = 100, sd = 15)), une distribution normale ayant \\(\\mu=100,\\sigma=15\\) qui est utilisée pour générer les valeurs. L’utilisateur sait qu’il s’agit d’un faux positif (une erreur de Type ) : l’échantillon fait partie des 5 % des échantillons qui risquent de se faire rejeter accidentellement. Si l’utilisateur utilise une autre graine (seed()), la plupart (95%) des moyennes ne sont pas rejetées.Pour l’instant, seule une boîte noire été examinée - celle de l’hypothèse nulle. Qu’advient-il du vrai phénomène? Par exemple, si les reptiliens existent vraiment. Comme l’utilisateur est le maître du modèle, il peut spécifier les paramètres à sa convenance. Le QI des reptiliens est conceptualisé pour être distribué comme une distribution normale ayant \\(\\mu_{r}=130, \\sigma_r = 15\\) où l’indice \\(r\\) ne fait qu’indiquer qu’il s’agit des paramètres de la population reptilienne. Les paramètres humains sont désignés par \\(h\\), soit \\(\\mu_h = 100,\\sigma_h = 15\\).La Figure 8.7 présente les distributions de ces populations. Trois zones sont ajoutées pour illustrer les concepts statistiques d’erreur de type , d’erreur de type II et de puissance statistique. Comme les populations sont connues, ces concepts statistiques sont calculables.\nFigure 8.7: Distribution du QI des humains et des reptiliens\nL’erreur de type (présentée auparavant) représente la probabilité d’émettre un faux positif, souvent représentée par \\(\\alpha\\) (alpha). Elle correspond à la probabilité de rejeter l’hypothèse lorsqu’elle est vraie. Dans la Figure 8.7, il s’agit de la zone noire. Elle correspond à conclure qu’un vrai humain est un reptilien (ce qu’il n’est pas). Ce taux est fixé à l’avance par l’expérimentateur, ici, 5%. C’est le risque qu’il est prêt à prendre. Ainsi, 95% des humains sont correctement identifiés comme humains.La zone hachurée de la Figure 8.7 correspond à l’erreur de type II, souvent représentée par \\(\\beta\\) (beta), soit la probabilité de ne pas rejeter l’hypothèse nulle. Autrement dit, c’est la probabilité de ne pas trouver l’effet lorsque celui-ci est vrai. Pour cet exemple, il s’agit d’un reptilien assez sournois (avec un QI suffisamment faible pour sa population) qu’il passe inaperçu auprès des humains, ou, en d’autres termes, de conclure qu’un vrai reptilien est un humain (ce qu’il n’est pas). Cette probabilité est estimée à 36.124%, donc 36.124% des reptiliens passeront inaperçus.La puissance, la zone grise de la Figure 8.7, correspond à la probabilité de rejeter correctement l’hypothèse nulle. Il s’agit de rejeter l’hypothèse nulle lorsqu’elle est fausse. Cela signifie d’identifier un vrai reptilien correctement. C’est exactement ce que le groupe de chercheur désire réaliser. Elle est estimée à 63.876% en ne prenant qu’une mesure de QI. La puissance est l’une des statistiques qui intéressent le plus l’expérimentateur avant de réaliser son étude, car elle donne une approximation sur la probabilité de trouver un résultat significatif. Par contre, les expérimentateurs ne connaissent que rarement les paramètres de la vraie distribution de l’effet qu’il désire trouver. En langage statistique, il s’agit un paramètre de non-centralité (ou de décentralisation) ou ncp en syntaxe R. L’expérimentateur recourt alors à d’habiles approximations éclairées basées sur leurs connaissances du phénomène et en regard des études déjà publiées sur le sujet (ou un sujet similaire), s’il y en . Ces estimations souffrent tout de même d’être des variables aléatoires (elles respectent elles aussi une distribution d’échantillonnage), et non les paramètres recherchés.Avant de procéder davantage, une dernière notion est importante à présenter : la direction du test. Dans tous les exemples précédents, l’hypothèse nulle était rejetée si une valeur plus rare était obtenue. Il s’agit d’un test d’hypothèse unilatérale (d’un seul côté). Cela facilite grandement la présentation de certains concepts et calculs. Un test peut être unilatérale inférieure, trouver un résultat plus faible qu’une valeur critique; ou unilatérale supérieure, trouver un résultat plus élevé qu’une valeur critique; ou encore bilatérale, soit trouver un résultat moins élevée ou plus élevée que des valeurs critiques.Dans le cas d’un test bilatéral, l’erreur de type est divisée par 2, \\(\\alpha/2\\) pour couvrir l’espace des deux côtés de la distribution. L’espace total est identique même s’il est divisé sur les deux extrêmes. Cela implique des valeurs critiques plus élevées que si le test n’était que d’un côté et par conséquent une puissance statistique un peu plus faible, car l’effet est directionnel. La probabilité de rejeter l’hypothèse dans une direction été diminuée pour tenir compte de l’autre côté. La Figure 8.8 montre un exemple de chacun de ces types de tests pour \\(\\alpha = .05\\).\nFigure 8.8: Illustration de l’erreur de type \nLe choix entre unilatérale inférieure, supérieure ou bilatérale repose essentiellement sur la question de recherche de l’expérimentateur. Qu’attend-il ou que veut-il savoir du résultat? L’expérimentateur recourt généralement au test bilatéral afin d’identifier des résultats allant pour son hypothèse ou à l’opposé de son hypothèse. Cela permet notamment d’identifier des devis qui peuvent nuire aux participants. Par exemple, si un expérimentateur développe une intervention pour réduire la consommation de drogues chez les adolescents et n’est concerné que par la probabilité que l’intervention soit efficace, il peut manquer l’effet délétère d’une telle intervention, soit que l’intervention augmente la consommation de drogue. Il est certain que l’expérimentateur souhaite être rapidement mis au courant si ces résultats se dessinent.","code":"# Création d'un échantillon de 10 unités\nset.seed(824)\n\n# Dix valeurs arrondies avec une moyenne de 100 et un écart type de 10\nQI <- round(rnorm(n = 10, mean = 100, sd = 15))\nQI\n>  [1] 110 111 102  99 109 102  99 110 132 114z <- (mean(QI) - 100)/(15 / sqrt(10))\nz\n> [1] 1.86\nmu.h <- 100 ; sd.h <- 15\nmu.r <- 130 ; sd.r <- 15# Erreur de type I (fixé à l'avance)\nalpha <- .05 \n\n# Valeur critique à laquelle l'expérimentateur rejette H0\nv.crit <- qnorm(1 - alpha, mean = mu.h, sd = sd.h)\nv.crit\n> [1] 125\n\n# Erreur de type II\nbeta <- pnorm(v.crit, mean = mu.r, sd = sd.r)\nbeta\n> [1] 0.361\n\n# Puissance statistique\npuissance <- 1 - beta\npuissance \n> [1] 0.639"},{"path":"inférer.html","id":"la-distribution-t","chapter":" 8 Inférer","heading":"8.4 La distribution-\\(t\\)","text":"Dans la plupart des cas, l’expérimentateur ne connaît pas la variance de la population. Il recourt alors à la meilleure estimation qui lui est disponible, celle obtenue auprès de l’échantillon. De recourir à une estimation au lieu de la vraie valeur cause un réel problème: l’estimation est une variable aléatoire respectant une distribution d’échantillonnage. La distribution des variances montre une légère asymétrie positive plus flagrante pour les petites tailles d’échantillon, car les variances sont distribuées en distribution-\\(\\chi^2\\).\nFigure 8.9: Distribution de la variance en fonction de la taille d’échantillon\nUne petite simulation faite à partir d’une distribution normale standardisée par rapport à trois tailles d’échantillon et dont la variance est estimée 10^{4} fois est présentée dans la Figure 8.9. Elle montre que l’estimation n’est pas biaisée (elles sont toutes les trois centrées à 1), mais relève l’asymétrie en question pour les petites tailles d’échantillon qui tend à décroître à mesure que \\(n\\) augmente. Autrement dit, une estimation unique (pris d’un échantillon) est plus susceptible d’être sous-estimée. Ne pas tenir compte de cet aspect augmente indûment les valeurs obtenues (à cause du dénominateur plus petit). Ainsi, il est inadéquat d’utiliser une distribution normale lorsque la variance est inconnue. Il faut plutôt opter pour la distribution-\\(t\\) qui, elle, tient compte de la variabilité de l’estimation de la variance.La distribution-\\(t\\) est symétrique, comme la distribution normale, mais des queues plus larges pour tenir compte de la surestimation des valeurs dû à la sous-estimation de la variance. La distribution-\\(t\\) tend vers une distribution normale lorsque \\(n\\) augmente, ce qui est illustré à la Figure 8.10. La ligne pleine noire montre la distribution pour 5 unités, les deux autres lignes, traits et pointillés, pour 30 et 1000 unités respectivement. Il est difficile de distinguer ces deux dernières.\nFigure 8.10: Comparaison d’une distribution normale à deux distributions-\\(t\\)\nEn ayant recours à des échantillons et une estimation de la variance (la variance est inconnue de l’expérimentateur, ce qui est généralement le cas), il faut procéder avec la distribution-\\(t\\). La procédure est la même que celle utilisée avec les scores-\\(z\\) précédemment, la différence étant que la distribution-\\(t\\) est utilisée au lieu de la distribution normale, car l’écart type est estimé.","code":""},{"path":"inférer.html","id":"le-test-t-à-échantillon-unique","chapter":" 8 Inférer","heading":"8.5 Le test-\\(t\\) à échantillon unique","text":"Le test-\\(t\\) à échantillon unique permet de tester la moyenne d’échantillon par rapport à une valeur arbitraire \\(\\mu_0\\) (généralement \\(\\mu_0 = 0\\)) qui joue le rôle d’hypothèse nulle.\\[ t_{n-1} = \\frac{\\bar{x}-\\mu_0}{(s/\\sqrt{n})} \\]La distribution-\\(t\\) un degré de liberté (l’indice de \\(t\\) dans l’équation) qui lui est associé et qui est fixé à \\(dl = n - 1\\), la taille d’échantillon moins 1. Un degré est perdu à cause de l’estimation de l’écart type de l’échantillon. Qu’en est-il de la probabilité d’obtenir cette moyenne? En recourant à la distribution intégrée de R, pt() il est possible d’obtenir la probabilité d’une valeur-\\(t\\) par rapport à l’hypothèse nulle. La fonction nécessite une valeur-\\(t\\) et le degré de liberté associé, p. ex., pt(t = , df = n - 1). La valeur produite donne la probabilité d’obtenir un score plus petit jusqu’à la valeur-\\(t\\). Une astuce permet de calculer aisément la probabilité lorsque la distribution d’échantillonnage est symétrique. Utiliser une valeur absolue permet de considérer les deux côtés de la distribution simultanément. Les valeurs-\\(t\\) négatives sont alors positives. Comme un côté est supprimé, l’espace positif est doublé, le code pour tenir compte de cet astuce est : (1 - pt(abs(t), df = n - 1)) * 2 où t est la valeur-\\(t\\) obtenue.Une fois la fonction créée, il est possible de la tester en la comparant avec la fonction R de base test.t(). Le code suivant crée un échantillon de 30 unités avec une moyenne de 1 et un écart type de 1.\nFigure 8.11: Valeur-\\(t\\) de l’échantillon sur la distribution-\\(t\\)\nQu’est-ce que ces résultats signifient? La Figure 8.11 montre la distribution d’échantillonnage de l’hypothèse nulle, c’est-à-dire l’hypothèse selon laquelle la moyenne de la population est égale à 0. Les zones grises montrent les zones de rejet. Si une valeur-\\(t\\) se retrouve dans ces zones, elle est jugée comme trop rare, il faut alors rejeter l’hypothèse nulle. C’est ce qui s’est produit ici. Rien de surprenant, l’échantillon est tiré d’une distribution normale avec une moyenne de 1 et un écart type de 1.","code":"\ntestt <- function(x, mu = 0){\n  # x est une variable continue\n  # mu est une moyenne à tester comme hypothèse nulle(H0)\n  xbar <- mean(x)\n  sdx <- sd(x)\n  n <- length(x)\n  vt <- (xbar - mu) / (sdx / sqrt(n))\n  dl <- n - 1\n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  statistique <- list(valeur.t = vt, \n                      dl = dl, \n                      valeur.p = vp)\n  return(statistique)\n}# Un exemple de jeu de données\nset.seed(20)\nx = rnorm(n = 30, mean = 1, sd = 1)\n# Vérification de la fonction maison\ntestt(x)\n> $valeur.t\n> [1] 3.69\n> \n> $dl\n> [1] 29\n> \n> $valeur.p\n> [1] 0.000919\n# Comparer avec la fonction R\nt.test(x)\n> \n>   One Sample t-test\n> \n> data:  x\n> t = 4, df = 29, p-value = 9e-04\n> alternative hypothesis: true mean is not equal to 0\n> 95 percent confidence interval:\n>  0.31 1.08\n> sample estimates:\n> mean of x \n>     0.696"},{"path":"inférer.html","id":"critiques-des-tests-dhypothèses","chapter":" 8 Inférer","heading":"8.6 Critiques des tests d’hypothèses","text":"TODOLe présent ouvrage ne couvre que l’aspect traditionnel ou classique des tests d’hypothèse. Cette approche est remise en question depuis 1950 jusqu’à aujourd’hui. D’excellents ouvrages couvrent les failles et solutions des tests d’hypothèses classiques de façon plus approfondie qu’il ne l’est fait ici.L’hypothèse nulle n’est jamais susceptible d’être vraie.La conclusion statistique n’est pas celle désirée par l’expérimentateur.La significativité statistique n’implique pas la significativité pratique.Les expérimentateurs interprètent incorrectement la valeur-\\(p\\).Une valeur-\\(p\\) faible n’est pas garantie de la reproductibilité.La dichotomisation de la preuve \\(p<=\\alpha\\).Une saine utilisation de la valeur-\\(p\\) n’accomplit pas grand-chose.L’obsession de la valeur-\\(p\\) cause plus de problèmes qu’elle n’en résout : \\(p\\)-hacking, triturage de données, abus de variables, comparaison multiple, biais pour les hypothèses originales et surprenantes.","code":""},{"path":"analyser.html","id":"analyser","chapter":" 9 Analyser","heading":" 9 Analyser","text":"En continuation de l’introduction des théories des tests d’hypothèses (voir Inférer) et l’aperçu donnée par le test-\\(t\\) à échantillon unique, cette section poursuit la présentation en introduisant des analyses statistiques de bases comme les différences de moyennes, l’association linéaire et les tests pour données nominales. Les tests-\\(t\\) indépendant et dépendant, l’analyse de variables, la covariance, la corrélation ainsi que le test du \\(\\chi2\\) pour table de contingence sont présentées.","code":""},{"path":"analyser.html","id":"les-différences-de-moyennes","chapter":" 9 Analyser","heading":"9.1 Les différences de moyennes","text":"","code":""},{"path":"analyser.html","id":"le-test-t-indépendant","chapter":" 9 Analyser","heading":"9.1.1 Le test-\\(t\\) indépendant","text":"En général, l’expérimentateur ne s’intéresse pas à comparer une moyenne à une valeur arbitraire, comme c’était le cas avec le test-\\(t\\) à échantillon unique. Cela peut lui être assez trivial. Il s’intéresse plutôt à comparer une moyenne à une autre moyenne, soit une différence entre deux groupes indépendants, par exemple, quelle est la différence entre un groupe traitement et un groupe contrôle?En se basant sur le test \\(t\\) à échantillon unique, la valeur-\\(t\\) pour deux moyennes se calcule selon l’équation (9.1)\\[\\begin{equation}\nt_{n-2} = \\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{\\frac{s^2_{1}}{n_1}+\\frac{s^2_{2}}{n_2}}}\n\\tag{9.1}\n\\end{equation}\\]où l’indice de la valeur-\\(t\\) est le nombre de degrés de liberté \\(n-2\\). Comme deux variances sont estimées, deux degrés de libertés sont imputés, ce qui octroi \\(n-2\\) degrés. En plus de considérer \\(\\bar{x_2}\\) la valeur arbitaire de comparaison (l’ordre de \\(\\bar{x_2}\\) et \\(\\bar{x^1}\\) est arbitraire), les deux écarts types sont également considérés au dénominateur.Une fois le calcul réalisé, la logique du test d’hypothèse est la même, à l’exception de l’hypothèse nulle qui correspond maintenant à l’absence de différence entre les deux moyennes.Voici un exemple de programmation du test-\\(t\\) pour deux groupes indépendants.Pour générer un exemple de données, le code ci-après crée un échantillon de 15 unités réparties en deux groupes, le premier groupe (gr0) est tiré d’une distribution normale ayant une moyenne de 1 et un écart type de 1, le deuxième groupe (gr1), une moyenne de 0 et un écart type de 1. La syntaxe illustre la création de deux variables pour créer les deux groupes. Il est aussi possible d’envisager la création sur en termes d’équation linéaire, comme l’équation (9.2),\\[\\begin{equation}\ny = \\mu_0 + \\mu_1x_1 + \\epsilon\n\\tag{9.2}\n\\end{equation}\\]où \\(y\\) est le score observé des unités et les autres variables construisent ce score, \\(\\mu_0\\) correspond à la moyenne du groupe référent de population (le groupe contrôle en quelque sorte.), \\(\\mu_1\\) réfère à la différence de moyenne entre les deux groupes identifiés par \\(x_1\\) qui réfère à l’assignation au groupe, soit 0 pour le groupe contrôle et 1 pour le groupe différent. Pour ce même exemple, \\(\\mu_1 = -1\\). Par le produit \\(\\mu_1x_1\\), le groupe contrôle associé à la valeur 0 n’pas de modification de la moyenne, \\(-1*0=0\\) alors le groupe différent associé à la valeur 1, \\(-1*1=-1\\). Enfin, \\(\\epsilon\\) correspond à la variabilité entre les unités. Cette façon de programmer la création des variables illustre bien l’association linéaire qui existe même dans les différences de moyennes et sera très utile pour des modèles plus compliqués.Les données sont identiques.Une fois la fonction créée, il est possible de la tester et de la comparer avec la fonction R de base test.t().Les équations et codes précédents ne sont adéquats que si les variances sont égales entre les deux groupes. Cette notion est quelque peu trahie par la spécification dans la fonction R de l’argument var.equal = TRUE qui par défaut est FALSE. Pour un test-\\(t\\) indépendant suivant les règles de l’art, il faut appliquer une correction (approximation de Welsh) aux degrés de liberté. Les degrés de liberté deviennent moins élégants et respectent l’équation (9.3).\\[\\begin{equation}\ndl=\\frac{\\left(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}\\right)^2}{\\frac{\\left(\\frac{s^2_1}{n_1}\\right)^2}{n_1-1}+\\frac{\\left(\\frac{s^2_2}{n_2}\\right)^2}{n_2-1}}\n\\tag{9.3}\n\\end{equation}\\]En apportant cette correction au code initial.Le voici comparé à la fonction R de base.Les sorties sont identiques.\nFigure 9.1: Valeur-\\(t\\) de la différence de moyenne sur la distribution-\\(t\\)\nLa Figure 9.1 illustre où se situe la moyenne de l’échantillon par rapport à la distribution d’échantillonnage de l’hypothèse nulle. Comme la valeur se retrouve dans la zone de rejet ou, de façon équivalente, la valeur-\\(p\\) est plus petite que la valeur \\(\\alpha\\) fixée à .05, rejette l’hypothèse nulle, il y vraisemblablement une différence entre les groupes. C’est bien l’intention derrière la création des données.","code":"\ntestt.ind <- function(x1, x2){\n  # x1 est une variable continue associée au groupe 1\n  # x2 est une variable continue associée au groupe 2\n  \n  # Calcul des moyennes\n  x1bar <- mean(x1) ; x2bar <- mean(x2)\n  \n  # Calcul des variances\n  x1var <- var(x1) ; x2var <- var(x2)\n  \n  # Calcul des tailles d'échantillon\n  nx1 <- length(x1) ; nx2 <- length(x2)\n  \n  # Valeur-t, degrés de liberté et valeur-p\n  vt <- (x1bar - x2bar) / sqrt(x1var / nx1 + x2var / nx2)\n  dl <- nx1 + nx2 - 2\n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  statistique <- list(valeur.t = vt, dl = dl, valeur.p = vp)\n  return(statistique)\n}# Un exemple de jeu de données programmé de deux façons\n# Méthode 1\nset.seed(2021)\ngr0 <- rnorm(n = 15, mean = 1, sd = 1)\ngr1 <- rnorm(n = 15, mean = 0, sd = 1)\n\n# Méthode 2\nset.seed(2021)\n\nx1 <- c(rep(0, 15), rep(1, 15)) # Appartenance au groupe (0 et 1)\ne  <- rnorm(n = 30, mean = 0, sd = 1) # Erreur interindividuel\nmu0 <- 1  # La moyenne du groupe référent\nmu1 <- -1 # Le deuxième groupe a une différence de moyenne de -1\n\ny <- mu0 + mu1 * x1 + e\n\n# Présenter quelques participants; retirer les crochets pour voir tous\n(cbind(method1 = c(gr0, gr1), method2 = y))[10:20,]\n>       method1 method2\n>  [1,]  2.7300  2.7300\n>  [2,] -0.0822 -0.0822\n>  [3,]  0.7272  0.7272\n>  [4,]  1.1820  1.1820\n>  [5,]  2.5085  2.5085\n>  [6,]  2.6045  2.6045\n>  [7,] -1.8415 -1.8415\n>  [8,]  1.6233  1.6233\n>  [9,]  0.1314  0.1314\n> [10,]  1.4811  1.4811\n> [11,]  1.5133  1.5133# Vérification de la fonction maison\ntestt.ind(gr0, gr1)\n> $valeur.t\n> [1] 3.4\n> \n> $dl\n> [1] 28\n> \n> $valeur.p\n> [1] 0.00205\n\n# Comparer avec la fonction R\nt.test(gr0, gr1, var.equal = TRUE)\n> \n>   Two Sample t-test\n> \n> data:  gr0 and gr1\n> t = 3, df = 28, p-value = 0.002\n> alternative hypothesis: true difference in means is not equal to 0\n> 95 percent confidence interval:\n>  0.54 2.18\n> sample estimates:\n> mean of x mean of y \n>     1.332    -0.029\ntestt.ind2 <-function(x1, x2){\n  # x1 est une variable continue associée au groupe 1\n  # x2 est une variable continue associée au groupe 2\n  x1bar <- mean(x1) ; x2bar <- mean(x2)  # Les moyennes\n  x1var <- var(x1) ; x2var <- var(x2)    # Les variances\n  nx1 <- length(x1) ; nx2 <- length(x2)  # Les tailles\n  \n  # L'erreur type \n  et <- sqrt(x1var / nx1 + x2var / nx2)\n  \n  # Valeur-t\n  vt <- (x1bar - x2bar) / et\n  \n  # Correction des degrés de liberté\n  dl <- et^4 / \n    ((x1var^2 / (nx1^2 * (nx1 - 1))) + \n       (x2var^2 / (nx2^2 * (nx2 - 1))))\n  \n  # Valeur-p\n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  \n  statistique <- list(valeur.t = vt, dl = dl, valeur.p = vp)\n  return(statistique)\n}t.test(gr0, gr1) # Absence de l'argument var.equal = TRUE\n> \n>   Welch Two Sample t-test\n> \n> data:  gr0 and gr1\n> t = 3, df = 27, p-value = 0.002\n> alternative hypothesis: true difference in means is not equal to 0\n> 95 percent confidence interval:\n>  0.539 2.182\n> sample estimates:\n> mean of x mean of y \n>     1.332    -0.029\ntestt.ind2(gr0, gr1)\n> $valeur.t\n> [1] 3.4\n> \n> $dl\n> [1] 26.9\n> \n> $valeur.p\n> [1] 0.00213"},{"path":"analyser.html","id":"le-test-t-dépendant","chapter":" 9 Analyser","heading":"9.1.2 Le test-\\(t\\) dépendant","text":"Un autre test-\\(t\\) est celui permettant de comparer deux de temps de mesure sur les mêmes participants. L’hypothèse est de vérifier si \\(\\mu_1 = \\mu_2\\), soit la moyenne du temps 1 est égale à la moyenne du temps 2. Une habile manipulation mathématique permet de poser cette hypothèse en hypothèse nulle, \\(\\mu_1-\\mu_2=0\\). Il est intéressant de noter que la différence entre deux variables est normalement distribuée si la variance est connue ou distribuée en \\(t\\) si la variance est inconnue.Ce test est utile lorsqu’il faut tester si les participants se sont améliorés ou détériorés entre deux temps de mesure. Pour calculer la valeur-\\(t\\),\\[ t_{dl_1} = \\frac{\\bar{x_1}-\\bar{x_2}}{\\sigma_d/\\sqrt{n}} \\]avec \\(n-1\\) degrés de liberté à cause de l’estimation de l’écart type des différences. Les étapes subséquentes sont identiques au test-\\(t\\) à groupe unique.Pour créer le jeu de données, les étapes sont similaires au test-\\(t\\) indépendant pour des temps de mesure indépendants (sans corrélation) où seules les populations des temps de mesure sont définis. Il est possible de spécifier une corrélation entre les deux de mesures. Cela sera introduit dans la section La corrélation.Dans le jeu de données suivant, 25 personnes sont mesurées à deux temps de reprises. Il n’y pas de corrélation entre les temps de mesure. La variance des temps de mesure est de 1. La différence de moyenne est de 2. La syntaxe simule la situation suivante, le temps1 correspond à la mesure initiale et difference correspond à la différence entre les temps de mesure. La somme de ces deux scores donnent la mesure au temps2. Cela montre assez simplement que la différence entre temps2 - temps1 permet de retrouver le vecteur de difference.La fonction base de R est encore t.test(), mais il faudra spécifier l’argument paired = TRUE pour commander un test apparié (une autre appellation pour un test-\\(t\\) dépendant).Les sorties sont identiques.\nFigure 9.2: Valeur-\\(t\\) de moyenne de différences sur la distribution-\\(t\\)\nLa Figure 9.2 montre où se situe la différence de moyenne par rapport à la distribution d’échantillonnage de l’hypothèse nulle. Comme la valeur se retrouve dans la valeur-\\(p\\) est plus petite que la valeur \\(\\alpha\\) fixée à .05, rejette l’hypothèse nulle; il y vraisemblablement une différence entre les temps de mesure, ce qui était l’intention derrière la création des données.","code":"\ntestt.dep <- function(temps1, temps2){\n  # Temps est une variable continue mesurée \n  # à deux occasions auprès des mêmes participants\n  \n  # Calculer la différence\n  difference <- temps1 - temps2\n  dbar <- mean(difference) # La moyenne des différences\n  dvar <- var(difference)  # L'écart type des différences\n  n <- length(difference)  # Taille d'échantillon\n  \n  # Valeur-t, degrés de liberté et valeur-p\n  vt <- (dbar) / sqrt(dvar / n)\n  dl <- n - 1                   \n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  statistique <- list(valeur.t = vt, dl = dl, valeur.p = vp)\n  return(statistique)\n}\n# Un exemple de jeu de données\nset.seed(148)\ntemps1 <- rnorm(n = 25, mean = 0, sd = 2)\ndifference <- rnorm(n = 25, mean = 2, sd = 2)\ntemps2 <- temps1 + differencet.test(temps1, temps2, paired = TRUE)\n> \n>   Paired t-test\n> \n> data:  temps1 and temps2\n> t = -3, df = 24, p-value = 0.004\n> alternative hypothesis: true difference in means is not equal to 0\n> 95 percent confidence interval:\n>  -2.172 -0.465\n> sample estimates:\n> mean of the differences \n>                   -1.32\ntestt.dep(temps1, temps2)\n> $valeur.t\n> [1] -3.19\n> \n> $dl\n> [1] 24\n> \n> $valeur.p\n> [1] 0.00395"},{"path":"analyser.html","id":"lanalyse-de-variance-à-un-facteur","chapter":" 9 Analyser","heading":"9.1.3 L’analyse de variance à un facteur","text":"L’analyse de variance, souvent appelée ANOVA (ANalysis VAriance), permet de comparer une variable continue sur plusieurs groupes ou traitements. Il s’agit d’une extension du test-\\(t\\) indépendant qui compare une variable continue auprès de deux groupes. À plus de deux groupes, l’analyse de variance entre en jeu.Attention! Cette section se limite au cas où les groupes sont de tailles identiques (même nombre de participants par groupe, le symbole \\(n_k\\) pour référer à cette quantité) afin d’en simplifier la présentation.","code":""},{"path":"analyser.html","id":"la-logique-de-lanalyse-de-variance","chapter":" 9 Analyser","heading":"9.1.3.1 La logique de l’analyse de variance","text":"Comme il été présenté dans la section sur le test-\\(t\\) indépendant, le modèle sous-jacent à l’analyse de variance à un facteur est une extension de ce modèle pour \\(k\\) groupes. Chaque groupe est associé à une différence de moyennes \\(\\mu_i\\), pour \\(= 1,2, ...k\\) par rapport à la moyenne groupe référent, \\(\\mu_0\\). Les variables \\(x_i\\) définissent l’appartenance au groupe \\(\\) par la valeur 1 et 0 pour les autres groupes (codage factice ou dummy coding).\\[y = \\mu_0 + \\mu_1x_1 + \\mu_2x_2+ ... +\\mu_kx_k + \\epsilon\\]\nL’hypothèse nulle est la suivante.\n\\[\\sigma^2_1=\\sigma^2_2=...=\\sigma^2_k\\]\nL’analyse de variance est un test omnibus (global) qui ne teste pas où est la différence, mais bien s’il y au moins une différence entre les groupes. L’hypothèse nulle peut s’avérer fausse de plusieurs façons. Il peut y avoir une ou plusieurs inégalités pour rejeter l’hypothèse nulle.La logique sous-jacente est basée sur l’idée selon laquelle les moyennes des groupes proviennent d’une même population. Elle compare les hypothèses suivantes : l’hypothèse nulle : les données (les moyennes) proviennent d’une même population; et l’hypothèse opposée : les données ne proviennent pas d’une même population.Elle suggère ainsi deux types de variances (ou carré moyen, CM, dans ce contexte) : le CMI, carré moyen intergroupe, lorsque l’hypothèse nulle est vraie (la variabilité des moyennes); et le CMR, carré moyen résiduel, lorsque l’hypothèse nulle n’est ni vraie, ni fausse (la variabilité des données).Pour calculer le CMI, la variance des moyennes des groupes est\n\\[s^2_{\\bar{x}}=\\sum_{=1}^k\\frac{(\\bar{x_i}-\\overline{\\overline{x}})^2}{k-1}\\]où \\(\\overline{\\overline{x}}\\) est la grande moyenne (la moyenne de toutes les unités). Cette statistique se retrouve sur la plan des distributions d’échantillonnage (comment les moyennes se distribuent). Il faut ainsi multiplier cette valeur par \\(n_k\\), le nombre d’unités par groupes, pour obtenir une estimation de la variance de la population. Ainsi, \\(\\text{CMI} = s^2_{\\bar{x}}n_k\\).Il y plusieurs méthodes pour calculer le CMR, comme le CMR est envisagé comme la moyenne des variances de groupes, il est pratique d’en référer le calcul à sa définition.\\[\n\\text{CMR} = \\bar{s^2} = \\frac{1}{k}\\sum_{=k}^k\\sigma_k^2\n\\]\nLorsque l’hypothèse nulle est vraie, les variances des groupes sont traitées comme le résultat d’une étude sur une même population, comme si \\(k\\) petites études avaient été réalisées. L’estimation du CMI est de l’ordre d’une distribution d’échantillonnage connue par le théorème central limite définissant le comportement des moyennes lorsqu’elles proviennent d’une même population (c’est ce que pose comme hypothèse l’analyse de variance). Toutefois, la valeur du CMI est relative au CMR, la quantité de bruit dans les données.L’analyse de variance pose alors la question : la variance attribuable aux différentes moyennes des groupes est-elle supérieure à la variance résiduelle? Pour tester cette question, une possibilité est de tester le ratio \\(\\frac{\\text{CMI}}{\\text{CMR}}\\), ce qui donne une valeur-\\(F\\). Le ratio de deux variances suit une distribution-\\(F_{dl_1, dl_2}\\) avec deux degrés de liberté différents qui lui sont associés. Autrement dit, la distribution-\\(F\\) est la distribution d’échantillonnage du ratio deux variances.\\[F_{dl_1,dl_2} = \\frac{n_ks^2_{\\bar{x}}}{\\bar{s^2}} = \\frac{\\text{CMI}}{\\text{CMR}}\\]\noù \\(dl_1 = k-1\\), et \\(dl_2 = k(n_k-1)\\). L’interprétation du ratio est ainsi :si \\(\\text{CMI}>\\text{CMR}\\), alors \\(\\text{CMI}/\\text{CMR}>1\\);si \\(\\text{CMI}=\\text{CMR}\\), alors \\(\\text{CMI}/\\text{CMR}=1\\);si \\(\\text{CMI}<\\text{CMR}\\), alors \\(\\text{CMI}/\\text{CMR}<1\\).Plus les moyennes sont variables, plus la valeur du CMI est élevée. Plus les unités sont variables, plus le CMR est élevé. S’il existe au moins une différence entre les groupes, le ratio est en faveur du CMI. En fait, plus \\(F\\) est élevée, toutes autres choses étant égales (les degrés de libertés, p. ex.), plus la probabilité de rejeter l’hypothèse nulle est grande. Ainsi, lorsque, la valeur-\\(F\\) est obtenue, il est possible de calculer une valeur-\\(p\\), et le test d’hypothèse se déroule tel qu’auparavant.","code":""},{"path":"analyser.html","id":"la-création-de-données","chapter":" 9 Analyser","heading":"9.1.3.2 La création de données","text":"Dans un jeu de données commun, une variable désigne les groupes et une autre leur score sur une certaine mesure. La syntaxe suivante montre trois façons pour créer une variable facteur.Il est aussi possible de remplacer les arguments, 1:k par des chaînes de caractères (des catégories au lieu de nombres).Une bonne pratique dans le contexte des comparaisons de moyenne est de déclarer les variables catégorielles comme facteur avec .factor().Pour créer des valeurs à ces catégories, une stratégie simple est de créer des valeurs pour chacun des groupes et de les combiner.","code":"nk <- 5 # nombre d'unités par groupe\nk <- 4  # nombre de groupes\n# En ordre\ngroupe1 <- rep(1:k, each = nk)\n# En alternance\ngroupe2 <- rep(1:k, times = nk)\n# Aléatoire\nset.seed(765)\ngroupe3 <- sample(x = 1:k, size = (k * nk), replace = TRUE)\ncbind(groupe1, groupe2, groupe3)\n>       groupe1 groupe2 groupe3\n>  [1,]       1       1       4\n>  [2,]       1       2       2\n>  [3,]       1       3       4\n>  [4,]       1       4       3\n>  [5,]       1       1       3\n>  [6,]       2       2       4\n>  [7,]       2       3       3\n>  [8,]       2       4       1\n>  [9,]       2       1       4\n> [10,]       2       2       3\n> [11,]       3       3       3\n> [12,]       3       4       2\n> [13,]       3       1       3\n> [14,]       3       2       3\n> [15,]       3       3       2\n> [16,]       4       4       4\n> [17,]       4       1       4\n> [18,]       4       2       2\n> [19,]       4       3       1\n> [20,]       4       4       1categorie <- c(\"char\", \"chat\", \"cheval\", \"chevalier\", \"chien\")\nnk <- 2\ngroupe <- as.factor(rep(categorie, each = nk)) # Déclarer comme facteur\ngroupe\n>  [1] char      char      chat      chat      cheval   \n>  [6] cheval    chevalier chevalier chien     chien    \n> Levels: char chat cheval chevalier chienset.seed(2602)\nchar      <- rnorm(n = nk, mean = 15, sd = 4) \nchat      <- rnorm(n = nk, mean = 20, sd = 4) # Différences ici\ncheval    <- rnorm(n = nk, mean = 10, sd = 4) # et ici\nchevalier <- rnorm(n = nk, mean = 15, sd = 4)\nchien     <- rnorm(n = nk, mean = 15, sd = 4) \n# Combinés\nscore <- round(c(char, chat, cheval, chevalier, chien))\n# Conserver toutes les informations en un jeu de données\ndonnees <- data.frame(groupe, score)\nhead(donnees)\n>   groupe score\n> 1   char    14\n> 2   char    12\n> 3   chat    17\n> 4   chat    21\n> 5 cheval    14\n> 6 cheval    10"},{"path":"analyser.html","id":"dummy-coding","chapter":" 9 Analyser","heading":"9.1.3.3 Dummy coding","text":"La plupart du temps, les variables de regroupement, les variables identifiant l’appartenance aux groupes, sont construites avec une variable de type facteur, c’est-à-dire une colonne avec différentes valeurs ou libellés. C’est d’ailleurs ce qui été fait dans l’exemple précédent. Cette méthode d’identification de groupement implique une programmation plus intensive, surtout pour la création de valeurs. En termes de programmation, il est plus élégant de recourir à une fonction de codage factice. Cela permettra de représenter fidèlement le modèle sous-jacent. Étrangement, il n’y pas de fonction de base avec R pour du codage factice. Une fonction maison permettra d’automatiser la réassignation des groupes (en une seule variable) sur plusieurs variables désignant leur appartenance.La fonction maison retourne un codage factice pour les \\(k\\) groupes. Attention, le codage factice est fait en ordre alphabétique. Comme il est redondant d’avoir \\(k\\) groupes (identifier \\(k\\) groupes nécessite \\(k-1\\) variables), un groupe référent est désigné. Ce dernier aura 0 sur tous les scores. Pour retirer un groupe, l’utilisation des crochets et une valeur négative associés à la colonne du groupe référent feront l’affaire, p. ex. dummy.coding(x)[,-k] déclare le \\(k\\)e groupe comme le groupe référent.L’expression X %*% mu est une multiplication d’algèbre matricielle qui multiplie la matrice \\(n \\times p\\) de codage factice \\(X\\) à une matrice \\(p\\times 1\\) de moyennes \\(\\mu\\). L’opération multiple les \\(p\\) éléments d’une ligne de \\(X\\) à la colonne \\(p\\) correspondante de \\(\\mu\\). En algèbre matricielle Le résultat est une matrice \\(n \\times 1\\) qui contient les différences de moyennes pour chaque unité. Dans la même ligne de syntaxe, la moyenne de la population (groupe référent), \\(\\mu_0\\) est ajoutée et la variation individuelle, \\(\\epsilon\\).Les scores produits sont identiques. Dans cet exemple par contre, l’origine des différences entre groupes est plus évidente, spécialement en comparant le modèle sous-jacent à l’analyse de variance. La source d’erreur est visible ainsi que les différences de moyennes. La force de cette deuxième méthode est qu’elle peut facilement être automatisée pour créer des jeux de données, alors que la première est plus compliquée. La seconde méthode nécessite six arguments (\\(n_k\\), \\(\\mu_0\\), \\(\\mu_{1:k}\\), et \\(\\sigma_{e}\\) en plus de catégories et définir le groupe référent), la première aurait de la difficulté à spécifier tous les arrangements de différence de moyennes automatiquement, quoiqu’elles sont plus personnalisables.","code":"\ndummy.coding <- function(x){\n  # Retourne un codage factice de x\n  # avec les facteurs en ordre alphabétique\n  \n  tab <- table(x)    # Extraire une table\n  k <- length(tab)   # Nombre de groupes\n  n <- length(x)     # Nombre d'unités\n  X <- matrix(0, nrow = n, ncol = k) # Création d'une nouvelle variable \n  xlev <- as.factor(x)\n  for (i in 1:n) {\n    X[i, xlev[i]] <- 1 # Identification des groupes\n  }\n  colnames(X) <- names(tab)\n  return(X)\n}# Facteur de groupe\nset.seed(2602)\nk <- length(categorie)\nnk <- 2\ngroupe <- rep(categorie, each = nk)\n# Groupement\nX <- dummy.coding(groupe)[,-5] # groupe 4 \"chien\" comme référent\n# Spécifications des paramètres\nmu0 <- 15\nmu <- c(0, 5, -5, 0)\ne <- rnorm(n = k * nk, sd = 4)\n# Création des scores\nscore <- round(mu0 + X %*% mu + e)\ncbind(donnees, score)\n>       groupe score score\n> 1       char    14    14\n> 2       char    12    12\n> 3       chat    17    17\n> 4       chat    21    21\n> 5     cheval    14    14\n> 6     cheval    10    10\n> 7  chevalier    12    12\n> 8  chevalier    17    17\n> 9      chien    21    21\n> 10     chien    18    18"},{"path":"analyser.html","id":"analyse","chapter":" 9 Analyser","heading":"9.1.3.4 Analyse","text":"À toute fin pratique, un jeu de données est recréé avec les catégories et paramètres précédents, mais avec \\(n_k=20\\) unités par groupe. La fonction R de base est aov(). Elle prend comme argument une formule, de forme VD ~ VI (variable dépendante prédite par variable indépendante) et un jeu de données duquel prendre les variables. Il existe également une fonction anova(), une fonction un peu plus complexe que aov(). Pour obtenir toute l’information désirée de la sortie de la fonction, il faut demander un sommaire de la sortie avec summary().Les résultats sont identiques, les seules différences étant dues à l’arrondissement. Comme la valeur-\\(p\\) est de 5.145^{-9}, ce qui est extrêmement plus petit que l’usuel .05 (ou un autre taux d’erreur de type fixé à l’avance), l’hypothèse nulle est rejetée, il y vraisemblablement une différence entre les groupes, ce qui est déjà connu. La Figure 9.3 illustre très bien la rareté d’un tel jeu de données sous l’hypothèse nulle.\nFigure 9.3: Valeur-\\(F\\) de la comparaison des moyennes\n","code":"# Anova de base\nres <- aov(score ~ groupe, data = donnees)\nsummary(res)\n>             Df Sum Sq Mean Sq F value  Pr(>F)    \n> groupe       4    834   208.4      14 5.1e-09 ***\n> Residuals   95   1414    14.9                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Fonction maison\ngr <- table(donnees$groupe)\nk  <- length(gr)          # Nombre de groupes\nnk <- dim(donnees)[1] / k # Nombre de participants par groupe\n\n# Moyennes et variance par (`by()`) groupes\nmoyenne  <- by(donnees$score, donnees$groupe, mean)\nvariance <- by(donnees$score, donnees$groupe, var)\n\n# Statistiques\nCMI <- nk * var(moyenne)\nCMR <- mean(variance)\ndl1 <- k - 1\ndl2 <- k * (nk - 1)\nvf <- CMI / CMR \nvp <- (1 - pf(vf, df1 = dl1, df2 = dl2)) \n\n# Création du tableau\nresultats <- matrix(0,2,5) # Tableau vide\n\n# Ajouter noms\ncolnames(resultats)  <- c(\"dl\", \"SS\", \"CM\", \"F\", \"p\")\nrow.names(resultats) <- c(\"groupe\", \"residu\")\n\n# Ajouter valeurs\nresultats[1,] <- c(dl1, CMI * dl1, CMI, vf, vp)\nresultats[2,] <- c(dl2, CMR * dl2, CMR, 0, 0)\nresultats\n>        dl   SS    CM  F        p\n> groupe  4  834 208.4 14 5.15e-09\n> residu 95 1414  14.9  0 0.00e+00"},{"path":"analyser.html","id":"lassociation-linéaire","chapter":" 9 Analyser","heading":"9.2 L’association linéaire","text":"","code":""},{"path":"analyser.html","id":"la-covariance","chapter":" 9 Analyser","heading":"9.2.1 La covariance","text":"La covariance représente une mesure du degré auquel une variable augmente lorsque l’autre augmente. Elle correspond au produit moyen entre deux variables centrées (dont les moyennes sont soustraites). retrouve la covariance sous la forme de l’équation (9.4).\\[\\begin{equation}\n\\text{cov}_{xy} = \\sigma_{xy}=\\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\tag{9.4}\n\\end{equation}\\]La covariance est une extension multivariée de la variance. La variance est le produit d’une variable centrée avec elle-même (le carré de la variable), \\[\\sigma^2=\\frac{1}{n-1}\\sum_{=1}^nx_ix_i\\].En transformant \\(x\\) et \\(y\\) pour qu’elles soient centrées, l’aspect de produit entre les deux variables devient évident dans l’expression de la covariance.\n\\[\\text{cov}_{xy} = \\frac{1}{n-1}\\sum_{=1}^nxy\\]\nLa programmation de la covariance peut se traduire ainsi.La fonction de base cov() est plus efficace (plus robuste et plus simple) que la fonction maison précédente. La fonction cov() peut tenir compte de plus de deux variables.\nLa covariance indique la direction de la covariation entre les deux variables, positives ou négatives, mais n’indique pas la force de la relation. Bien qu’une covariance de 0 indique l’absence de colinéarité, une covariance de 120 n’est pas nécessairement plus forte qu’une covariance de 1. La mesure est intrinsèquement influencée par les unités l’échelle de mesure. Il serait aberrant de considérer le système métrique comme moins efficace, car ces mesures sont moins variables que les mesures impériales. (Il y plusieurs raisons de préférer le système métrique, mais ce n’en est pas une!)","code":"\ncovariance <- function(x, y){\n  # X est une data.frame ou matrice de n sujets par p variables\n  n <- length(x)\n  # centrer variables\n  xc <- x - mean(x)\n  yc <- y - mean(y)\n  prod.xy <- xc * yc\n  cov.xy <- sum(prod.xy) / (n-1)\n  return(cov.xy)\n}"},{"path":"analyser.html","id":"la-corrélation","chapter":" 9 Analyser","heading":"9.2.2 La corrélation","text":"La corrélation représente le degré d’association linéaire entre deux variables. Une façon simple de concevoir la corrélation est comme une covariance standardisée. Elle varie entre -1 et 1, ces deux dernières valeurs impliquant, respectivement, une relation parfaitement négative et positive entre les deux variables. En plus d’indiquer la direction de la relation, la corrélation suggère une force à ce lien. Lorsque la valeur de la corrélation est nulle, \\(r=0\\), un diagramme de dispersion ne montre qu’un nuage de point épars (sans structure). Plus la corrélation augmente (en terme absolu), plus le nuage tends vers une ligne droite. La Figure 9.4 montre différentes valeurs de corrélation et un exemple de nuage de points qui lui est associé.\nFigure 9.4: Illustrations de diagrammes de dispersion associé en fonction de corrélations\nEn statistiques, la corrélation est souvent représentée par la lettre grecque \\(\\rho\\) (rho) ou l’usuel \\(r\\) lorsqu’il s’agit d’une estimation à partir d’un échantillon.Il y plusieurs méthodes pour calculer la corrélation. Une première est de considérer la corrélation comme le produit de variables standardisées (autrement dit, score-\\(z\\)). Pour la covariance, la moyenne était soustraite de la variable, c’est-à-dire des variables centrées. Pour la corrélation, divise pas l’écart type également pour obtenir des variables standardisées.En admettant que \\(x\\) et \\(y\\) sont déjà standardisée, \\(z_x,z_y\\), comme c’était le cas pour la covariance, l’aspect de produit entre les deux variables est conservé.\n\\[\\text{cor}_{xy} = r_{xy}= \\frac{1}{n-1}\\sum_{=1}^nz_xz_y\\]Il est possible de calculer directement la corrélation à partir de la covariance, soit en divisant par le produit des écarts types des variables.\\[r_{xy} = \\frac{\\text{cov}_{xy}}{\\sigma_x\\sigma_y}\\]Voici trois options de programmation de la corrélation. La première et la deuxième sont basées sur la covariance. Dans la première, la covariance est calculée de nouveau dans la fonction. La deuxième quant à elle profite avantageusement de l’existence d’une fonction maison qui calcule directement la covariance. La troisième recourt au produit de variables standardisées.Évidemment, R possède une fonction de base pour le calcul d’une corrélation entre deux variables ou d’une matrice de corrélation (à partir d’un jeu de données de deux variables ou plus). Il s’agit de cor(). Ces trois fonctions maison en plus de la fonction R sont comparées ci-dessous. Pour ce faire, un jeu de données est créé possédant les caractéristiques désirées. Dans ce cas-ci, la taille d’échantillon est fixée à \\(n=10000\\) pour assurer une bonne précision dans l’échantillon et une corrélation \\(r=.7\\).Une façon simple de produire des données à partir d’une matrice de corrélation (ou de covariance) est d’utiliser la fonction mvrnorm() du package MASS. Elle évite pour l’instant d’introduire les manipulations mathématiques nécessaires (ce sera donc pour l’instant l’une de ces boîtes noires ayant la confiance de l’utilisateur, mais élucider dans le chapitre Créer).La fonction mvrnorm() pour MultiVariate Random NORMmal nécessite trois arguments :la taille d’échantillon n;la taille d’échantillon n;les moyennes mu des \\(p\\) variables, et;les moyennes mu des \\(p\\) variables, et;la matrice de covariance Sigma (la corrélation est un cas particulier de la covariance où les variables sont standardisées).la matrice de covariance Sigma (la corrélation est un cas particulier de la covariance où les variables sont standardisées).Le lecteur avisé aura noté la ressemblance de nomenclature entre mvrnorm() (multivariée) et\nrnorm() (univariée). Enfin, la dernière étape est de produire une matrice de covariance \\(p \\times p\\). La fonction matrix() prend une variable de données, dans cet exemple , c(1, r, r, 1) qu’elle répartit ligne par colonne (voir Créer une matrice. Comme \\(p=2\\), cela crée une matrice \\(2 \\times 2\\).Les résultats sont près des attentes et identiques.Pour tester la probabilité d’obtenir cette valeur, la corrélation \\(r\\) peut se transformer monotonement en valeur-\\(t\\) avec l’équation (9.5), ce qui permet de calculer des valeurs-\\(p\\).\\[\\begin{equation}\nt_{n-2} = \\frac{r}{(\\frac{\\sqrt{1-r^2}}{\\sqrt{n-2}})}\n\\tag{9.5}\n\\end{equation}\\]L’équation (9.5) se traduit simplement en code R.Dans le code ci-dessous, l’équation (9.5) précédente est subtilement réarrangée pour être plus simple et élégante quoiqu’équivalente.\\[\\begin{equation}\nt_{n-2} = \\frac{r}{(\\frac{\\sqrt{1-r^2}}{\\sqrt{n-2}})} = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\tag{9.6}\n\\end{equation}\\]L’équation (9.5) l’avantage de montrer la relation entre l’estimateur et l’erreur type alors que l’équation (9.6) est moins encombrante.","code":"\n# Option 1 : Calcul complet\ncorrelation1 <- function(x, y){\n  \n  # La taille d'échantillon\n  n <- length(x)\n  \n  # Centrer les variables\n  xc <- x - mean(x)\n  yc <- y - mean(y)\n  \n  # Produit des variables\n  prod.xy <- xc * yc\n  \n  # La somme des produits divisée par n - 1\n  cov.xy <- sum(prod.xy) / (n - 1)  # La covariance\n  \n  # Diviser par les écarts types\n  cor.xy <- cov.xy / (sd(x) * sd(y))\n  \n  return(cor.xy)\n}\n\n# Option 2 : Basé sur la covariance\ncorrelation2 <- function(x, y){\n  \n  # Calculer la covariance\n  cov.xy <- covariance(x,y)\n  \n  # Diviser par les écarts types\n  cor.xy <- cov.xy / (sd(x) * sd(y))\n  return(cor.xy)\n}\n\n# Option 3 : Basé sur des variables standardisées\ncorrelation3 <- function(x, y){\n  \n  # La taille d'échantillon\n  n <- length(x)\n  \n  # Centrer et réduire les variables\n  xs <- (x - mean(x)) / sd(x)\n  ys <- (y - mean(y)) / sd(y)\n  \n  # La somme des produits divisé par n - 1\n  cor.xy <- sum(xs * ys)/(n - 1)\n  \n  return(cor.xy)\n}# Pour la reproductibilité\nset.seed(820)\n\n# Taille d'échantillon importante pour réduire l'erreur d'échantillonnage\nn <- 10000\n\n# La corrélation désirée\nr <- .70\n\n# Création d'une matrice de corrélation\nR <- matrix(c(1, r, r, 1), nrow = 2, ncol = 2)\n\n# Création des données \ndonnees <- data.frame(MASS::mvrnorm(n = n, \n                                    mu = c(0,0), \n                                    Sigma = R))\ncolnames(donnees) <- c(\"x\", \"y\") # Ajouter des noms de colonnes\n\n# Voici la matrice de corrélation\nR\n>      [,1] [,2]\n> [1,]  1.0  0.7\n> [2,]  0.7  1.0\n\n# Voici une visualisation partielle des données\nhead(donnees)\n>        x      y\n> 1  0.635  0.914\n> 2 -0.193  0.360\n> 3 -0.602 -1.213\n> 4 -0.841 -0.986\n> 5 -0.105 -1.771\n> 6 -2.147 -1.590\n\n# Vérifications des variables\nmean(donnees$x); mean(donnees$y); sd(donnees$x); sd(donnees$y)\n> [1] -0.00578\n> [1] -0.00531\n> [1] 0.999\n> [1] 1.01\n\n# Les résultats sont près des attentes\n# Vérifications de la corrélations\ncor(donnees$x, donnees$y)\n> [1] 0.699\ncorrelation1(donnees$x, donnees$y)\n> [1] 0.699\ncorrelation2(donnees$x, donnees$y)\n> [1] 0.699\ncorrelation3(donnees$x, donnees$y)\n> [1] 0.699# Création d'une matrice de corrélation \n# Il pourrait également s'agir d'un vecteur ou d'un scalaire.\nr <- matrix(c( 1, .5, .4,\n              .5,  1, .3,\n              .4, .3,  1), \n            nrow = 3, ncol = 3)\n\n# Nombre d'unités (valeur arbitraire ici)\nn <- 10\n\n# Valeurs-t\nvt <- r * sqrt(n - 2) / sqrt(1 - r ^ 2)\n\n# Valeurs-p\nvp <- (1 - pt(abs(vt), df = n - 2)) * 2\nvt ; vp\n>      [,1]  [,2]  [,3]\n> [1,]  Inf 1.633 1.234\n> [2,] 1.63   Inf 0.889\n> [3,] 1.23 0.889   Inf\n>       [,1]  [,2]  [,3]\n> [1,] 0.000 0.141 0.252\n> [2,] 0.141 0.000 0.400\n> [3,] 0.252 0.400 0.000"},{"path":"analyser.html","id":"les-données-nominales","chapter":" 9 Analyser","heading":"9.3 Les données nominales","text":"Jusqu’à présent, deux types d’association de données ont été présenté : une variable nominale (identifiant des groupes) avec une variable continue (différences de moyenne) et deux variables continues (association linéaire). Dans cette section, l’association entre deux variables nominales est présentée. Une façon de représenter l’association entre deux variables nominales est le tableau de contingence, soit l’illustration d’une distribution d’une variable (en ligne) pour chaque catégorie de l’autre (en colonne). En voici, un exemple.\nTable 9.1: Tableau de contingence de la relation entre posséder une voiture et le climatoscepticisme\nLe tableau 9.1 présente deux variables nominales (climatosceptique, à la verticale, et posséder une voiture, à l’horizontale) ayant chacune deux catégories (oui et non). Il montre les proportions observées. Ces variables sont-elles associées? Pour traiter cette question, les proportions attendues jouent un rôle crucial. En fait, l’hypothèse nulle sous la table de contingence postule que toutes les proportions du tableau de contingence sont indépendantes. Mathématiquement, il s’agit de postuler que les proportions d’une variable (en ligne ou en colonne) n’influencent pas celles de l’autre variable. Pour obtenir les proportions théoriques, les totaux des lignes et colonnes sont calculés ainsi que le grand total. La fréquence attendue d’une cellule est obtenue en calculant, pour chaque cellule, le produit de sa colonne et de sa ligne respective divisé par le grand total.\\[\\begin{equation}\nt_{ij} = \\frac{\\text{total}_i \\times \\text{total}_j}{\\sum \\text{total}}\n\\tag{9.7}\n\\end{equation}\\]L’équation (9.7) ci-dessus illustre l’idée sous-jacente à la proportion attendue, \\(t\\) d’une cellule de ligne, \\(\\), et colonne \\(j\\).\nTable 9.2: Tableau de contingence étendu avec les totaux\n","code":""},{"path":"analyser.html","id":"le-chi2-pour-table-de-contingence","chapter":" 9 Analyser","heading":"9.3.1 Le \\(\\chi^2\\) pour table de contingence","text":"Maintenant que la question statistique et que l’hypothèse nulle sont posées, comment mesurer le degré selon lequel les données s’écartent des attentes théoriques. Le \\(\\chi^2\\) (khi-carré) permet de calculer une telle mesure. Le \\(\\chi^2\\) correspond à la distance entre une valeur observée et théorique au carré, pondérée par la valeur théorique.\n\\[ \\chi^2_v = \\sum_{=1}^{l}\\sum_{j=1}^c(\\frac{(o_{ij}-t_{ij})^2}{t_{ij}})\\]\noù \\(o\\) correspond aux valeurs observées, \\(t\\) réfère aux valeurs théoriques, \\(v\\) représente les degrés de liberté et \\(\\) et \\(j\\) les lignes et colonnes respectivement. Le degré de liberté est\n\\[v = (n_{\\text{ligne}}-1)(n_{\\text{colonne}}-1)\\]Si l’hypothèse nulle est vraie, les valeurs observées et théoriques devraient être très près. Le carré permet de calculer une distance euclidienne et le dénominateur pondère la distance. Comme l’[analyse de variance][Analyse de variance à un facteur], le test de \\(\\chi^2\\) pour table de contingence est global, il n’informe pas d’où provient la dépendance, mais bien s’il y au moins une dépendance.Il y plusieurs techniques pour créer une base de données, l’essentiel étant de lier les proportions désirées. Ici, une variable sexe est créée avec 50-50% de chance d’être l’un ou l’autre sexe. Par la suite, une proportion différente liée au sexe est utilisée pour générer la fréquence de consommation de tabac. La syntaxe sert principalement à identifier les valeurs homme et femme de la première variable pour en associer une valeur tabac.La fonction table() de R génère une table de contingence. La fonction maison et deux méthodes de base R sont présentées et produisent les mêmes résultats pour le \\(\\chi^2\\). Un autre, le test exact de Fisher est également présenté. Celui-ci est plus robuste, mais peut requérir plus intensive des ressources de l’ordinateur pour les grosses tables de contingences.Le test du \\(\\chi^2\\) est sujet à certains problèmes qu’il est important de considérer sans quoi l’interprétation peut être faussée. Si la taille d’échantillon (le nombre d’unités d’observation observées est trop faible), alors le test est biaisé. Par exemple, trois lancers de pile ou face ne sont pas suffisants pour tester une hypothèse de khi-carré, le nombre de résultats différents est de 2, ce qui ne donne pas une approximation satisfaisante de la distribution d’échantillonnage. La convention est de dire qu’une fréquence théorique est trop petite si elle est plus petite que 5. Si ce n’est pas cas, des corrections ou d’autres options doivent être considérées.Dans le présent exemple, bien que les fréquences attendues respectent les critères usuels, il peut être utile d’envisager un test plus robuste comme le test exact de Fisher ou le \\(\\chi^2\\) avec correction de continuité. Le premier se commande avec la fonction fisher.test() et le second est la fonction par défaut de chisq.test(). Pour montrer l’équivalent entre la fonction chisq.test() et la fonction maison, l’option de correction est désactivée avec l’argument correct = FALSE.","code":"\nkhicarre <- function(obs){\n  # Obs est une table de contingence\n  # Somme colonne\n  SC <- as.matrix(colSums(obs))\n  # Somme ligne\n  SL <- as.matrix(rowSums(obs))\n  # Grand total\n  TT <- sum(obs)\n  # Proportion théoriques\n  theo <- SL %*% t(SC) / TT\n  # khi carré\n  khi2 <- sum((obs - theo) ^ 2 / theo)\n  dl <- (length(SL) - 1) * (length(SC) - 1)\n  vp <- 1 - pchisq(khi2, dl)\n  # Sortie\n  statistiques <- list(khi2 = khi2, dl = dl, valeur.p = vp)\n  return(statistiques)\n}  set.seed(54)\n  n <- 100\n  \n  # Première variable\n  sexe <- sample(c(\"homme\",\"femme\"), \n                size = n, \n                replace = TRUE, \n                prob = c(.5,.5))\n  tabac <- rep(0, 100)\n  \n  # Proportions conditionnelles\n  tabac[sexe == \"femme\"] <- sample(c(\"fumeur\",\"non-fumeur\"), \n                                    size = sum(sexe == \"femme\"), \n                                    replace = TRUE, \n                                    prob = c(.2,.8))\n  \n  tabac[sexe == \"homme\"] <- sample(c(\"fumeur\",\"non-fumeur\"), \n                                    size = sum(sexe == \"homme\"), \n                                    replace = TRUE, \n                                    prob = c(.1,.9))\n  #base données\n  donnees <- data.frame(sexe, tabac)\n  table(donnees)\n>        tabac\n> sexe    fumeur non-fumeur\n>   femme      8         38\n>   homme      4         50# Table de contingence\nTC <- table(donnees)\n# Fonction maison\nkhicarre(TC)\n> $khi2\n> [1] 2.34\n> \n> $dl\n> [1] 1\n> \n> $valeur.p\n> [1] 0.126\n\n# Fonction de base\nsummary(TC)\n> Number of cases in table: 100 \n> Number of factors: 2 \n> Test for independence of all factors:\n>   Chisq = 2.3, df = 1, p-value = 0.1\n\n# Autre fonction de base\nresultat <- chisq.test(TC, correct = FALSE)\nresultat\n> \n>   Pearson's Chi-squared test\n> \n> data:  TC\n> X-squared = 2, df = 1, p-value = 0.1\n\n# Et pour plus de précision...\nresultat$statistic\n> X-squared \n>      2.34\nresultat$p.value\n> [1] 0.126\n\n# Le test exact de Fisher\nresultat.fisher <- fisher.test(TC)\nresultat.fisher\n> \n>   Fisher's Exact Test for Count Data\n> \n> data:  TC\n> p-value = 0.2\n> alternative hypothesis: true odds ratio is not equal to 1\n> 95 percent confidence interval:\n>   0.641 12.731\n> sample estimates:\n> odds ratio \n>       2.61\nresultat.fisher$p.value\n> [1] 0.216"},{"path":"simuler.html","id":"simuler","chapter":" 10 Simuler","heading":" 10 Simuler","text":"Dans plusieurs contextes statistiques, les informations cruciales nécessaires pour réaliser une analyse statistique ou tirer des résultats spécifiques ne sont pas connues (par exemple, le chapitre Analyser présente des analyses soutenues par des théorèmes). Pire, parfois certains postulats sont violés rendant les démarches usuelles caduques. Enfin, dans certains contextes, une analyse formelle pourrait s’avérer fort complexe, voire irrésolvable, alors qu’une analyse plus empirique usant des techniques de rééchantillonnage résoudra ces problèmes simplement et immédiatement.Jusqu’à présent, les situations connues des analyses statistiques ont été présentées. Maintenant, les techniques de rééchantillonnage, comme le bootstrap et de simulations Monte-Carlo seront présentés.","code":""},{"path":"simuler.html","id":"les-simulations-monte-carlo","chapter":" 10 Simuler","heading":"10.1 Les simulations Monte-Carlo","text":"Les simulations Monte-Carlo sont une famille de méthode algorithmique qui permet de calculer une valeur numérique en utilisant des procédés aléatoires. Le nom provient du prestigieux casino de Monte-Carlo à Monaco sur la Côte d’Azur, où des jeux de hasard se jouent constamment, et ayant ainsi une connotation très forte avec le hasard et les nombres aléatoires.Une simulation de Monte-Carlo prédit une étendue de résultats possibles à partir d’un ensemble de valeurs d’entrée fixes et en tenant compte de l’incertitude inhérente de certaines variables. En d’autres termes, une simulation de Monte-Carlo produit les résultats possibles (sorties) d’un modèle à partir des entrées fixes et d’autres entrées variables. Ces sorties sont calculées encore et encore (des milliers de fois, parfois plus), en utilisant à chaque fois un ensemble différent de nombres aléatoires, générant des sorties probables, mais différentes à chaque fois. Les estimations (et leur tendance) obtenues peuvent alors être interprétées.Les simulations Monte-Carlo sont particulièrement utiles, car elles permettent, en mathématiques, de calculer des intégrales très complexes; en physique, d’estimer la forme d’un signal ou la sensibilité; en finance, simulent des conditions permettant de prévoir le marché; et en psychologie, simuler des processus comportementaux ou cognitifs.Les simulations Monte-Carlo possèdent trois caractéristiques :Construire un modèle ayant des variables indépendantes (entrées) et dépendantes (sorties);Construire un modèle ayant des variables indépendantes (entrées) et dépendantes (sorties);Spécifier les caractéristiques aléatoires des variables indépendantes et définir des valeurs crédibles;Spécifier les caractéristiques aléatoires des variables indépendantes et définir des valeurs crédibles;Rouler les simulations de façon répétitive jusqu’à ce que suffisamment d’itérations soient produites et que les résultats convergent vers une solution.Rouler les simulations de façon répétitive jusqu’à ce que suffisamment d’itérations soient produites et que les résultats convergent vers une solution.Maintenant, ces étapes sont mises en contexte avec un problème.","code":""},{"path":"simuler.html","id":"le-problème-de-monty-hall","chapter":" 10 Simuler","heading":"10.1.1 Le problème de Monty-Hall","text":"Rien de mieux pour confronter le sens commun que d’être confronté à un problème contre-intuitif. Les simulations Monte-Carlo nous permettront de vérifier les résultats de façon empirique, parallèlement à ce qu’une analyse formelle fournit.Le problème de Monty Hall (attribuables à Selvin, 1975) présente un joueur devant un présentateur (Monty Hall). Trois portes sont offertes au choix du joueur. Derrière l’une d’entre elles se retrouve un magnifique prix : une voiture électrique de luxe. Derrière chacune des deux autres portes se retrouvent un prix citron : une chèvre chacune. Le joueur ne sait pas ce qu’il y derrière les portes. Le joueur peut alors choisir une porte. Évidemment, à ce moment le joueur à une chance sur trois de choisir la voiture.\nFigure 10.1: Illustration du problème de Monty Hall\nPar la suite, le présentateur, qui connaît le contenu derrière les portes, ouvre une porte qui () ne cache pas la voiture et (b) que le participant n’pas choisie. Le joueur peut alors choisir () de conserver la porte choisie ou (b) de changer de porte. Quelle option, s’il y en une, assurera le meilleur gain (choisir la voiture)? Rester ou changer? Par exemple, suivant l’illustration de la Figure 10.1, le joueur choisit la porte 1, alors le présentateur doit ouvrir la porte 2 (non choisie et ne contient pas la voiture). Le joueur doit-il changer son choix? Ici, la réponse est évidente, car la réponse est connue. Qu’en est-il alors si le joueur avait choisi la porte 3 et le présentateur ouvre l’une des deux autres porte? Le joueur doit-il changer ou rester? La question sous-jacente est qu’elles sont les probabilités de rester et changer respectivement. Sont-elles différentes?Comme le résultat n’est pas des plus intuitif (et sans divulgâcher le résultat), une petite simulation s’impose (ou une analyse formelle pour les lecteurs enclins mathématiquement). La situation sera recréée un millier de fois pour vérifier l’option (rester ou changer) qui maximise de gagner la voiture.En se référant aux trois points caractérisant une simulation Monte-Carlo susmentionné :Le problème de Monty Hall tel qu’imminemment décrit.TODOLes variables indépendantes sontle tirage aléatoire du contenu derrière les portes (distribution uniforme, chaque porte à la même probabilité d’avoir un prix ou non);le tirage aléatoire du contenu derrière les portes (distribution uniforme, chaque porte à la même probabilité d’avoir un prix ou non);le choix du joueur (distribution uniforme, pourrait être différent), et;le choix du joueur (distribution uniforme, pourrait être différent), et;la porte ouverte par le présentateur,la porte ouverte par le présentateur,l’action de rester ou de changer (les deux options sont étudiées).l’action de rester ou de changer (les deux options sont étudiées).Il y une variable dépendante :le succès (gagner le prix) ou l’échec (choisir la mauvaise porte).La simulation est reprise 1000 fois.Les étapes de la simulation se déroulent ainsi.L’attribution des portes (une gagnante ou deux perdantes) est faite aléatoirement6.L’attribution des portes (une gagnante ou deux perdantes) est faite aléatoirement6.Le joueur fait son premier choix.Le joueur fait son premier choix.Plusieurs modèles peuvent être utilisés, comme systématiquement choisir la même porte, ce qui simplifie la situation et ne change pas les probabilités, car le contenu derrière les portes est aléatoire. Pour rester vrai à la situation, le joueur fait un choix aléatoire.Le comportement du présentateur s’enclenche, il doit “ouvrir” une porte non choisie et qui ne contient pas le prix.Le comportement du présentateur s’enclenche, il doit “ouvrir” une porte non choisie et qui ne contient pas le prix.Les deux stratégies sont simulées, rester ou changer.Les deux stratégies sont simulées, rester ou changer.La simulation enregistre s’il y eu gain ou non et l’additionne au total de chacun.La simulation enregistre s’il y eu gain ou non et l’additionne au total de chacun.La simulation est reprise un nombre important de fois, ici, 1000 suffit, mais des situations compliquées peuvent demander beaucoup plus d’itérations.La simulation est reprise un nombre important de fois, ici, 1000 suffit, mais des situations compliquées peuvent demander beaucoup plus d’itérations.Une petite digression avant de poursuivre. Il faut être naïf pour croire que le code fonctionne tel que prévu.. À cause d’un inconvénient de la fonction sample(), une approche conditionnelle doit être utilisée. En fait, la fonction échantillonne les éléments de x (premier argument) jusqu’à obtenir size objets. Par contre, si une seule valeur est donnée à x et qu’elle est numérique, alors la fonction utilise les éléments de 1:x pour rééchantillonner au lieu de retourner x. Ainsi, si le choix et le prix (porte 2) sont derrière des portes différentes (porte 1 et 2, par exemple), alors une seule valeur est retournée (3), et sample() retourne 1:3 au lieu de 3. Ces particularités expliquent et justifient l’utilisation du conditionnel. En programmation, il faut toujours s’assurer que les fonctions s’accordent avec les attentes.Quelles sont les résultats de cette simulation? Le fait de rester sur le premier choix devrait obtenir 33% de chance de réussir, comme le joueur initialement une chance sur trois d’avoir la bonne réponse. Qu’en est-il pour changer? Est-ce qu’ouvrir une porte chèvre modifie les probabilités? Les résultats de la simulation montre que rester gagne 31.6% et que de changer gagne 68.4%. À long terme, il est plus avantageux de changer.Une explication simple est de dénombrer les possibilités. Dans le cas où le joueur ne change pas d’idée, il une chance sur trois, soit : choisir la chèvre 1, et garder la chèvre 1; choisir la chèvre 2, et garder la chèvre 2; et choisir la voiture, et garder la voiture. Si le joueur change d’idée après l’ouverture des portes, les chances sont maintenant de deux sur trois, soit choisir la chèvre 1, changer pour le prix; choisir la chèvre 2 et changer pour le prix; ou choisir le prix et changer pour une chèvre.Une autre façon de rendre se problème plus intuitif est de considérer le problème avec 100 portes au lieu de 3. Le présentateur ouvre les 98 portes qui ne sont pas un prix. Si le joueur choisit une porte et garde, il effectivement 1% de chance de remporter le prix. Par contre, s’il choisit une porte et que le présentateur lui ouvre toutes les autres portes chèvres, le joueur gagne à tout les coups s’il change, sauf s’il choisi le prix au premier coup. C’est un peu comme si le participant ouvrait les 99 portent, sauf celle qu’il choisit au début.Avec quelques modifications de la syntaxe précédente, le code suivant illustre le cas à 100 portes. À noter que l’usage du conditionnelle n’est plus nécessaire au bon fonctionnement de sample(), car il y maintenant plus de trois portes.La simulation montre que rester gagne 0.9% et que de changer gagne 99.1%.Des situations fort plus compliquées pourraient être apportées en simulations Monte-Carlo. Le jeu de Black Jack en est un exemple, bien qu’il soit déjà résolu pour des résultats optimaux (Millman, 1983). Il suffit d’avoir la patience de programmer le tout pour confirmer les résultats analytiques.Sur le plan pratique pour l’expérimentateur, les simulations Monte-Carlo peuvent être utiles pour calculer des tailles d’échantillons pour des modèles complexes, comme des modèles par équations structurelles compliquées, multiniveaux ou de classes latentes. Une sous-famille est d’une importance significative pour l’expérimentateur, celle qui sera abordée maintenant, le bootstrap.","code":"# Simulation du problème de Monty Hall\n# Valeurs possibles des portes\nportes <- c(\"Chèvre\", \"Chèvre\", \"Voiture\")\n# Nombre de répétitions\nnreps <- 1000\n# Pour la reproductibilité\nset.seed(7896)\n\n# Valeur initial des gains\ngain.rester <- 0\ngain.changer <- 0\n\n# Boucle pour répéter `nreps` fois le scénario\nfor(i in 1:nreps){\n  \n  # Arrangement initial des portes\n  tirage <- sample(portes)\n  \n  # Trouver le prix\n  prix <- which(tirage %in% \"Voiture\")\n  \n  # Choix aléatoire\n  choix1 <- sample(length(portes), size = 1)\n  \n  # Sélectionner la porte avec la chèvre (pas un prix) et\n  # qui n'est pas celle choisie (choix1)\n  option <- c(choix1, prix)\n  \n  if(choix1 != prix){\n    \n    # Si une porte valide\n    monty <- c(1:3)[-option]\n    \n  }else{\n    \n    # Si deux portes valides, en choisir une aléatoirement.\n    monty <- sample(c(1:3)[-option], size = 1)\n    \n  }\n  \n  # Décision : Reste à choix1\n  choix.rester <- choix1\n  \n  # Décision : Changer de choix changer\n  # Changer = ne pas prendre la porte initial, ni la porte ouverte\n  choix.changer <- c(1:3)[-c(choix1, monty)]\n  \n  # Enregistrement des gains\n  gain.rester  <- gain.rester  + (tirage[choix.rester]  == \"Voiture\")\n  gain.changer <- gain.changer + (tirage[choix.changer] == \"Voiture\")\n}\n\ncbind(gain.rester, gain.changer) / nreps\n>      gain.rester gain.changer\n> [1,]       0.316        0.684# Simulation du problème de Monty Hall\nportes <- c(\"Voiture\", rep(\"Chèvre\", 99))\nn <- length(portes)\nnreps <- 1000\nset.seed(7896)\n\n# Valeur initial des gains\ngain.rester <- 0\ngain.changer <- 0\n\nfor(i in 1:nreps){\n  # Arrangement initial des portes\n  tirage <- sample(portes)\n  \n  # Trouver le prix\n  prix <- which(tirage %in% \"Voiture\")\n  \n  # Choix aléatoire\n  choix1 <- sample(n, size = 1)\n  \n  # Sélectionner la porte avec la chèvre (pas un prix) et\n  # qui n'est pas celle choisie (choix1)\n  option <- c(choix1, prix)\n  \n  # Les autres portes\n  monty <- sample(c(1:n)[-option], size = n - 2)\n  \n  # Décision : Reste à choix1\n  choix.rester <- choix1\n  \n  # Décision : Changer de choix changer\n  # Changer = ne pas prendre la porte initial, ni la porte ouverte\n  choix.changer <- c(1:n)[-c(choix1, monty)]\n  \n  # Enregistrement des gains\n  gain.rester <- gain.rester + (tirage[choix.rester] == \"Voiture\")\n  gain.changer <- gain.changer + (tirage[choix.changer] == \"Voiture\")\n}\n\ncbind(gain.rester,gain.changer)  / nreps\n>      gain.rester gain.changer\n> [1,]       0.009        0.991"},{"path":"simuler.html","id":"le-bootstrap","chapter":" 10 Simuler","heading":"10.2 Le bootstrap","text":"Le bootstrap (dont il n’y pas d’excellente traduction en français) est une des techniques de rééchantillonnage astucieuses permettant des inférences statistiques (Efron & Tibshriani, 1979). Il fait partie de la famille des simulations Monte-Carlo, car il se base sur la réitération multiple d’une statistique à partir d’un jeu de données. Sans vouloir entrer dans les détails, il existe plusieurs types de techniques de bootstrap, qui font elles-mêmes parties d’une plus grande famille, les simulations stochastiques. Y est inclus les simulations de Monte-Carlo (dont le bootstrap fait parti) ou les méthodes numériques bayésiennes.Cet ouvrage insiste sur le bootstrap non paramétrique, impliquant que les distributions sous-jacentes aux échantillons ne soient pas spécifiées, bien qu’il existe du bootstrap paramétrique. Ce type de bootstrap est certainement la plus utile pour l’expérimentateur.Dans le bootstrap, l’échantillon initial est considéré comme une pseudopopulation. Il ne nécessite pas d’autre information que celle fournie par l’échantillon. Il permet d’obtenir une distribution d’échantillonnage et tous ses bénéfices. Alors que dans les chapitres Inférer et Analyser, il faut préciser et connaître quelle distribution d’échantillonnage correspond à quelle statistique (p. ex., le score-\\(z\\) d’une unité à la distribution normale; la moyenne d’un échantillon à la distribution \\(t\\) si la variance est inconnue), aucune de ces connaissances n’est nécessaire. L’étendue d’application du bootstrap est immense, surtout pour les concepts statistiques qui offriront plus de défis, ce qui sera vu dans des chapitres ultérieurs.Comme abordé dans le chapitre Inférer, la distribution d’échantillonnage permet :d’estimer l’indice désiré;d’estimer l’indice désiré;d’estimer son erreur standard;d’estimer son erreur standard;d’estimer ses intervalles de confiance;d’estimer ses intervalles de confiance;de réaliser un test d’hypothèse;de réaliser un test d’hypothèse;tout cela en ignorant les distributions et postulats sous-jacents qui empêchent ou limitent leurs usages. Le bootstrap n’que deux hypothèses fondamentales :l’échantillon reflète la population et chaque unité est indépendante et identiquement distribuée. Autrement dit, chaque unité provienne bel et bien d’une même boîte (population, voir Inférer) et en sont un portrait juste en plus que le tirage d’une unité n’en influence celle d’une autre.Le fondement du bootstrap repose sur les étapes suivantes :Sélectionner avec remise les unités d’un échantillon;Sélectionner avec remise les unités d’un échantillon;Calculer et enregistrer l’indice statistique désiré auprès de ce nouvel échantillon;Calculer et enregistrer l’indice statistique désiré auprès de ce nouvel échantillon;Réitérer les deux premières étapes un nombre élevé de fois.Réitérer les deux premières étapes un nombre élevé de fois.Ce processus hautement facilité par l’excellente performance des ordinateurs d’aujourd’hui, se réalise très facilement et rapidement. L’exemple suivant se base sur le rééchantillonnage (1000 fois) de la moyenne à partir d’une variable aléatoire tirée d’une distribution uniforme avec un minimum de 5 et d’une maximum de 15.La fonction sample(, replace = TRUE) rééchantillonne avec remplacement les identifiants des unités. Par simplicité, il est possible d’éliminer la ligne nouveau.X = X[id] en utilisant mean(X[id]) ou plus simplement mean(sample(X, replace = TRUE)). Cette utilisation se limite seulement au cas d’un vecteur de données. S’il s’agit d’un jeu de données avec plus d’une variable, alors mean(X[id, ]) est utilisé. Noter l’usage de la , entre crochets. De cette façon, toutes les variables des unités identifiées par id sont extraites (voir Référer à une variable dans un jeu de données).À partir des informations obtenues, des inférences statistiques sont possible. Toutes les estimations sont présentées comme un histogramme tel que l’illustre la Figure 7.8. Le code se retrouve ci-dessous. Quelques formules pour améliorer la présentation se retrouvent dans la syntaxe. L’utilisation simple de hist() pourra convenir.\nFigure 7.8: Historgamme des estimations des échantillons\nLa distribution de la Figure 7.8 se désigne comme la distribution de la population, comme cest le cas dans le chapitre [sur les inférences Inférer. Elle permet d’avoir une estimation plus robuste de l’erreur standard, qui se trouve à être l’écart type des indices rééchantillonnés. Elle est également utilisée pour construire des intervalles de confiance et permet ainsi de faire des tests d’hypothèse comme : l’intervalle de confiance à \\((1-\\alpha) \\times 100\\) % contient-elle la valeur 0? Comme il est fait avec une hypothèse nulle traditionnelle.Le cas illustré est trivial au sens où, par le théorème central limite, la distribution des moyennes est déjà connue. Si l’exemple était plutôt sur la médiane, il faudrait obligatoirement procéder par bootstrap pour calculer son erreur type et ses intervalles de confiance, car aucune formule exacte ne permet sa dérivation (il existe bien des approximations cela dit). Le bootstrap est alors des plus appropriés pour calculer l’erreur type et en tirer des intervalles de confiances, voire même en faire des tests d’hypothèses. Plusieurs statistiques auront recours au bootstrap pour dériver ces informations, la plus utilisée étant le coefficient de détermination (voir le chapitre [sur la régression Prédire).À partir des informations obtenues auprès du rééchantillonnage, il est possible d’obtenir les éléments désirés. Comme le cas est trivial, les indices statistiques seront très similaires. Cela confirmera au lecteur qu’aucun principe ésotérique ne s’est déroulé devant ses yeux en plus de confirmer que les statistiques attendues se produisent effectivement.La moyenne et l’erreur standard sont très près de la moyenne de l’échantillon et celle de la population (qui est de 10) et de l’erreur type attendue. Il est possible de créer des intervalles de confiances avec la fonction quantile qui prend en argument un vecteur de données et les probabilités désirées. Dans le cas de 95% pour une erreur de type de 5%, soit \\(\\alpha=.05\\), il s’agit de \\(.05/2 = .025\\) et \\(1-.05/2= .975\\), laissant au total 5% aux extrémités.Pour réaliser le test d’hypothèse nulle, il suffit de constater si l’intervalle de confiance contient ou non 0. Dans le cas où l’intervalle contient 0, le test n’est pas significatif; l’hypotèse nulle n’est pas rejetée; il est vraisemblable que l’absence d’effet soit vraie. S’il ne contient pas 0, l’hypothèse nulle est rejetée, le test est significatif et il est vraisemblable qu’il y ait un effet. Dans cet exemple, la moyenne est clairement différente de 0, car elle ne contient pas cette valeur.","code":"\nset.seed(158)\n# Nombre d'unités\nn <- 30\n\n# Le nombre de rééchantillonnage\nnreps <- 10000\n\n# Création de la variable\nX <- runif(n = n, min = 5, max = 15)\n\n# Création d'une variable vide pour l'enregistrement\nmoyenne.X <- numeric()\n\n# L'utilisation d'une boucle permet de réitérer les étapes\nfor (i in 1:nreps){\n  # Rééchantillonnage avec remise\n  id <- sample(n, replace = TRUE)\n  \n  # Nouvel échantillon\n  nouveau.X <- X[id]\n  \n  # Calculer et enregistrer la moyenne\n  moyenne.X[i] <- mean(nouveau.X)\n}\nhist(moyenne.X,              # Données\n     ylab = \" Frequence\",    # Changer l'axe y\n     main = \"\",              # Retirer le titre\n     breaks = 50,            # Nombre de colonnes\n     xlim = c(7,13)          # Définir l'étendu de l'axe x\n)# Voici la moyenne et l'erreur standard originales\nmean(X) ; sd(X)/sqrt(n)\n> [1] 10.2\n> [1] 0.485\n\n# La moyenne et l'erreur standard à partir des rééchantillons\nmean(moyenne.X)  ; sd(moyenne.X)\n> [1] 10.2\n> [1] 0.481# Erreur de type I\nalpha <- .05\n\n# Valeurs critiques\ncrit <- c(alpha/2, (1-alpha/2))\ntv <- qt(crit, df = n - 1)\n\n# Intervalles basés sur les indices de l'échantillon\nmean(X) + tv * sd(X)/sqrt(n)\n> [1]  9.2 11.2\n\n# Intervalles basés sur les indices du rééchantillonnage\nmean(moyenne.X) + tv * sd(moyenne.X)\n> [1]  9.22 11.18\n\n# Intervalles sur le rééchantillonnage\nquantile(moyenne.X, crit)\n>  2.5% 97.5% \n>  9.26 11.13"},{"path":"simuler.html","id":"meilleures-pratiques","chapter":" 10 Simuler","heading":"10.2.0.1 Meilleures pratiques","text":"R n’est pas particulièrement efficace pour réaliser des boucles. Il existe une famille de fonctions apply qui permet d’appliquer une fonction sur une série de vecteurs (comme une liste, un jeu de données ou une matrice). L’appel à l’aide ?apply donne beaucoup d’informations à ce sujet.La fonction principale est apply(). Elle nécessite le jeu de données (X) sur lequel appliqué la fonction (FUN) et un argument (MARGIN) pour identifier la dimension selon laquelle il faut appliquer la fonction, comme MARGIN = 1 produit l’opération par lignes; MARGIN = 2 produit l’opération par colonnes.7","code":"# Il faut un vecteur de données `vec`, un nombre de répétitions `nreps`\n# et une fonction `theta` à calculer sur le vecteur\nbootstrap <- function(vec, nreps, theta){\n  btsp.vec <- matrix(sample(vec, nreps * length(vec), replace = TRUE),\n                     nrow = nreps)\n  btsp.res <- apply(X = btsp.vec, MARGIN = 1, FUN = theta)\n  return(btsp.res)\n}\n\n# En utilisant les données de l'exemple précédent\nbtsp <- bootstrap(vec = X, nreps = nreps, theta = mean)\n\n# Pour fin de comparaison :\n# Voici la moyenne et l'erreur standard de ce bootstrap\nmean(btsp)  ; sd(btsp)\n> [1] 10.2\n> [1] 0.484\n\n# Voici la moyenne et l'erreur standard à partir du premier bootstrap\nmean(moyenne.X)  ; sd(moyenne.X)\n> [1] 10.2\n> [1] 0.481\n\n# Et voici la moyenne et l'erreur standard originales\nmean(X) ; sd(X)/sqrt(n)\n> [1] 10.2\n> [1] 0.485"},{"path":"simuler.html","id":"les-packages-1","chapter":" 10 Simuler","heading":"10.2.1 Les packages","text":"Il existe plusieurs packages pour réaliser du bootstrap dans R. Il y bootstrap (Tibshirani & Leisch, 2019) et boot (Canty & Ripley, 2021). Ajouter à cela que plusieurs fonctions R possèdent des options de bootstrap intégrées (qu’il faut commander dans les arguments). Plusieurs analyses statistiques recourant aux bootstraps sont déjà implantées. L’aperçu donné dans ce chapitre devrait convaincre le lecteur que, s’il besoin de bootstrap, il est assez rudimentaire de l’implanter soi-même.","code":""},{"path":"exercice-analyse.html","id":"exercice-analyse","chapter":"Exercices","heading":"Exercices","text":"TODOInférerAnalyser\nSimuler\n1. Créer une table de valeur-\\(t\\) critique pour \\(dl = 1,2,3 ,... ,30\\) et \\(\\alpha=.05\\) unilatérale.Comparer la puissance de la distribution-\\(t\\) par rapport à une distribution normale centrée réduite. La différence par rapport à l’hypothèse nulle est 2 et l’écart type est 1, l’\\(\\alpha = .05\\) bilatérale. Tester pour différentes valeurs de \\(n\\).Comparer la puissance de la distribution-\\(t\\) par rapport à une distribution normale centrée réduite. La différence par rapport à l’hypothèse nulle est 2 et l’écart type est 1, l’\\(\\alpha = .05\\) bilatérale. Tester pour différentes valeurs de \\(n\\).Créer un jeu de données pour test-\\(t\\) dépendant avec une corrélation de .3 entre les temps de mesure, des variances de 1, une différence de moyenne de 1 et une taille d’échantillon \\(n = 20\\). Analyser ce jeu de données : vérifier la corrélation et la différence.Créer un jeu de données pour test-\\(t\\) dépendant avec une corrélation de .3 entre les temps de mesure, des variances de 1, une différence de moyenne de 1 et une taille d’échantillon \\(n = 20\\). Analyser ce jeu de données : vérifier la corrélation et la différence.Calculer la puissance d’une corrélation de \\(\\rho = .30\\) pour une taille d’échantillon \\(n=80\\) et \\(alpha = .05\\) bilatérale.Calculer la puissance d’une corrélation de \\(\\rho = .30\\) pour une taille d’échantillon \\(n=80\\) et \\(alpha = .05\\) bilatérale.Comparer l’erreur de type du test-\\(t\\) indépendant avec équivalence versus non-équivalence des variances pour une différence de moyennes de 1 et des variances de \\(.25\\) pour le groupe 1 et \\(1\\) pour le groupe 2 pour une taille d’échantillon de 30 séparé également entre les groupes.Comparer l’erreur de type du test-\\(t\\) indépendant avec équivalence versus non-équivalence des variances pour une différence de moyennes de 1 et des variances de \\(.25\\) pour le groupe 1 et \\(1\\) pour le groupe 2 pour une taille d’échantillon de 30 séparé également entre les groupes.","code":""},{"path":"prédire.html","id":"prédire","chapter":" 11 Prédire","heading":" 11 Prédire","text":"Un objectif des chercheurs est généralement de développer des modèles afin de faire des prédictions à partir d’un échantillon sur la population. Les statistiques sont un outils idéalement pour développer ces modèles. La régression est l’une des analyses fondamentales pour réaliser cette objectif et constitue en quelques sortes les fondements de biens des méthodes récentes (comme l’apprentissage machine).L’objectif de la régression est de décrire la relation entre un variable dépendante et un ensemble de variables indépendantes. Un aperçu est donnée à la section sur l’association linéaire dans le chapitre Analyser. Ce présent chapitre débute avec la notion de covariance et l’étend jusqu’à celle de régression. Des techniques rudimentaires de création de données sont présentées. Par la suite, les mathématiques et la programmation sous-jacente au modèle linéaire sont illustrées.","code":""},{"path":"prédire.html","id":"retour-sur-lassociation-linéaire","chapter":" 11 Prédire","heading":"11.1 Retour sur l’association linéaire","text":"Une première méthode de mesure d’association est la covariance qui est représentée par l’équation (11.1).\\[\\begin{equation}\ns_{xy}=\\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\tag{11.1}\n\\end{equation}\\]L’équation (11.1) représente la somme des produits des écarts à la moyenne de deux variables.Comment généraliser cette équation à un ensemble de plus de deux variables?Pour débuter, les variables sont mises sur un même pied d’égalité. Plutôt que de recourir à des lettres différentes (\\(x\\) et \\(y\\)), il faut toutes les considérées comme des \\(x_{,j}\\), où l’indice \\(\\) identifie le \\(\\)e participant, comme pour l’équation (11.1), et l’indice \\(j\\) indique la \\(j\\)e variable parmi \\(p\\). Le calcul pour chacune des paires de variables \\(j\\) et \\(k\\) est réalisé pour les \\(p\\) variables; l’équation de la covariance devient ainsi l’équation (11.2).\\[\\begin{equation}\ns_{x_j,x_k}=\\frac{1}{n-1}\\sum_{=1}^n(x_{,j}-\\bar{x_j})(x_{,k}-\\bar{x_k})\n\\tag{11.2}\n\\end{equation}\\]Si les variables sont centrées, l’équation (11.2) devient, pour faciliter l’intuition, l’équation (11.3), soit la somme des produits entre deux variables.\\[\\begin{equation}\ns_{x_j,x_k}=\\frac{1}{n-1}\\sum_{=1}^n(x_{,j})(x_{,k})\n\\tag{11.3}\n\\end{equation}\\]Avantageusement, lorsque \\(j=k\\), les équations (11.2) et (11.3) calculent la variance de la variable correspondante. En syntaxe R, ces équations s’écrivent dans une fonction comme la suivante. Pour rappel, la fonction cov() dans laquelle une matrice de données est passée comme argument fournit la matrice de covariance.Dans le code R ci-dessus, les fonctions ncol() et nrow() extraient le nombre de dimensions de la base de données, soit le nombre de variables et le nombre d’unités. La fonction scale() permet de centrer les variables de X et de les retourner dans Xc, par défaut center = TRUE, ce pourquoi l’argument n’est pas explicité, mais ne les standardise pas, scale = FALSE. Une variable vide S est créé pour enregistrer les résultats. La boucle, quant à elle, calcule l’addition de l’équation (11.3) pour enfin diviser la somme par \\(n-1\\). Le résultat est S, la matrice de covariance.","code":"\ncovariance1 <- function(X){ \n  # X est un jeu de données\n  Xc = scale(X, scale = FALSE) # Centrées les variables\n  p = ncol(X)                  # Nombre de variables \n  n = nrow(X)                  # Nombre de sujets\n  S = numeric()                # Matrice vide pour enregistrer résultats\n  \n  for(j in 1:p) {\n    for(k in 1:p) {\n      S[j, k] = sum(Xc[ ,j] * Xc[ ,k])\n    }\n  }\n  \n  S = S / (n-1)\n  \n  # S est la matrice de covariance\n  return(covariance = S)\n}"},{"path":"prédire.html","id":"illustration-de-la-covariance","chapter":" 11 Prédire","heading":"11.1.1 Illustration de la covariance","text":"Il est relativement aisé d’exprimer graphiquement la covariance bivariée. L’équation (11.3) rappelle l’aire d’un rectangle. Pour chaque paire \\((x_i,y_i)\\), un rectangle peut être dessiner à partir du centre \\((0, 0)\\) jusqu’au \\((x_i,y_i)\\). La Figure 11.1 montre quatre exemples de ces rectangles. Lorsque la moyenne d’une variable est soustraite, les données deviennent centrées sur le point \\((0, 0)\\). L’expression \\(xy\\) ou \\(x_ix_j\\) rappelle le calcul de l’aire d’un rectangle. C’est effectivement ce qui se produit pour la covariance. L’équation calcule l’aire du rectangle formé par les points \\((0,0)\\) et \\((x_i,y_i)\\). En fait, l’équation (11.1) calcule le rectangle moyen dont la somme des produits est divisé par le nombre de rectangles moins 1, \\((n-1)\\).\nFigure 11.1: Illustration de la covariance\n\nFigure 11.2: Illustration des produits (rectangles) pour différentes valeurs de covariance\nLes Figures 11.1 et 11.2 permettent d’inférer quelques propriétés de la covariance.Comme la quantité de surface blanche (ou de noire) dépend de la taille de la figure, la covariance est directement proportionnelle aux échelles à l’abscisse et l’ordonnée.Comme la quantité de surface blanche (ou de noire) dépend de la taille de la figure, la covariance est directement proportionnelle aux échelles à l’abscisse et l’ordonnée.La covariance augmente lorsque les points s’approchent d’une ligne à pente ascendante et diminue lorsque les points s’approchent d’une ligne à pente descendante.La covariance augmente lorsque les points s’approchent d’une ligne à pente ascendante et diminue lorsque les points s’approchent d’une ligne à pente descendante.Comme les associations non linéaires peuvent créer des amalgames de rectangles positifs et négatifs, elles conduisent à des covariances imprévisibles et peu pertinentes.Comme les associations non linéaires peuvent créer des amalgames de rectangles positifs et négatifs, elles conduisent à des covariances imprévisibles et peu pertinentes.La covariance (et la corrélation) est sensible aux valeurs aberrantes. Un point éloigné de la masse créera une aire rectangulaire bien plus grande que les autres points. À lui seul, il peut créer une quantité substantielle positive ou négative de surface blanche (ou noire) dans la figure.La covariance (et la corrélation) est sensible aux valeurs aberrantes. Un point éloigné de la masse créera une aire rectangulaire bien plus grande que les autres points. À lui seul, il peut créer une quantité substantielle positive ou négative de surface blanche (ou noire) dans la figure.Si une variable est multipliée par elle-même, il s’agit de l’aire d’un carré, ce qui équivaut au calcul d’une variance. Par extension, si le produit de deux variables se rapproche davantage d’un carré que d’un rectangle (en moyenne), alors les deux variables sont fortement liées.Si une variable est multipliée par elle-même, il s’agit de l’aire d’un carré, ce qui équivaut au calcul d’une variance. Par extension, si le produit de deux variables se rapproche davantage d’un carré que d’un rectangle (en moyenne), alors les deux variables sont fortement liées.Le paramètre de la corrélation de la population peut être conceptualisé comme un carré déformé en rectangle à cause de l’erreur de mesure des axes.Le paramètre de la corrélation de la population peut être conceptualisé comme un carré déformé en rectangle à cause de l’erreur de mesure des axes.","code":""},{"path":"prédire.html","id":"la-covariance-en-termes-dalgèbre-matricielle","chapter":" 11 Prédire","heading":"11.1.2 La covariance en termes d’algèbre matricielle","text":"L’équation de la covariance peut aussi se calculer en termes d’algèbre matricielle. En plus d’accélérer le calcul des résultats, il simplifie énormément les mathématiques sous-jacente (pourvu que l’utilisateur connaisse l’algèbre matricielle).Une matrice est un ensemble de variables représenté sous une seule variable mathématique. Dans les équations mathématiques, une matrice est désignée par une lettre majuscule en gras: \\(x\\) devient \\(\\mathbf{X}\\) et \\(\\sigma\\) devient \\(\\mathbf{\\Sigma}\\). Dans une matrice, chaque colonne est une variable, chaque ligne correspond à un sujet différent mesuré sur toutes les variables. Une matrice est définie en partie par son nombre de lignes (\\(n\\), nombre d’unités) et son nombre de colonnes (\\(p\\), nombre de variables), dont voici une illustration à l’équation (11.4).\\[\\begin{equation}\n\\mathbf{X} = \\left(\\begin{array}{cccc}\nx_{1,1} & x_{1,2} & ...&x_{1,p}\\\\\nx_{2,1} & x_{2,2} & ...&x_{2,p}\\\\\n... & ...& ...& ... \\\\\nx_{n,1} & x_{n,2} & ... &x_{n,p}\\\\\n\\end{array}\\right)\n\\tag{11.4}\n\\end{equation}\\]En syntaxe R, il ne s’agit rien de plus que de concaténer des variables (mesurant les mêmes individus) ensembles par des colonnes, comme il est fait avec des jeux de données data.frame(). Un jeu de données est, à peu de chose près, une matrice (voir Concaténer). Pour créer une matrice X à partir des variables x, y, et z avec R, par exemple, la ligne X = cbind(x, y, z) joindra les trois variables ensemble. La fonction cbind() et une fonction pour concaténer des colonnes. Il existe aussi rbind() pour les lignes.Pour réaliser le calcul de la covariance, il faut multiplier la matrice des données centrées (les variables concaténées) par elle-même puis diviser par \\(n-1\\). Le symbole \\(X\\) représente la concaténation des variables. Par simplicité, l’équation utilise des variables centrées.\\[\\begin{equation}\n\\mathbf{S} = (n-1)^{-1}\\mathbf{X}^\\prime \\mathbf{X}\n\\tag{11.5}\n\\end{equation}\\]Le symbole \\(\\mathbf{S}\\) représente la matrice de variance-covariance. La diagonale de cette matrice représente les variances des données et les éléments hors diagonales sont les covariances, soulignant le lien entre la variance et la covariance. Le symbole \\(\\prime\\) (prime) correspond à l’opération de transposer une matrice, soit d’échanger les lignes par ces colonnes. Cette procédure est nécessaire pour produire la multiplication d’une matrice par elle-même. Noter que l’expression \\((n-1)^{-1}=\\frac{1}{n-1}\\).\\[\\begin{equation}\n\\begin{aligned}\n\\mathbf{S} & = (n-1)^{-1}\n\\left(\\begin{array}{cccc}\nx_{1,1} & x_{2,1} & ... & x_{n,1} \\\\\nx_{2,1} & x_{2,2} & ... & x_{n,2} \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{cc}\nx_{1,1} & x_{1,2} \\\\\nx_{2,1} & x_{2,2}\\\\\n... & ...  \\\\\nx_{n,1} & x_{n,2} \\\\\n\\end{array}\\right) \\\\\n& = (n-1)^{-1}\n\\left(\\begin{array}{cc}\n\\sum_{=1}^n(x_{,1})(x_{,1}) & \\sum_{=1}^n(x_{,1})(x_{,2})\\\\\n\\sum_{=1}^n(x_{,2})(x_{,1}) & \\sum_{=1}^n(x_{,2})(x_{,2})\n\\end{array}\\right)\n\\end{aligned}\n\\tag{11.6}\n\\end{equation}\\]L’équation (11.6) illustre l’équation (11.5) qui sont toutes les deux équivalentes à (11.3). En termes de syntaxe R, elles peuvent être traduites comme suit.La fonction t() opère la transpose (représentée par \\(\\prime\\)) et le symbole %*% signifie le produit matriciel des variables. Si l’usuel symbole de multiplication * est utilisé, R opère une multiplication cellule par cellule (avec recyclage) plutôt que l’opération matricielle.L’utilisation de l’algèbre matricielle est plus simple et efficace. Elle nécessite cinq lignes de code, élimine deux boucles, prend moins de temps à calculer, en plus de produire toutes les variances et covariances.Une matrice de covariance possède plusieurs propriétés importantes. Elle est toujours carrée soit \\(p \\times p\\) pour \\(p\\) variables et contient \\(p^2\\) éléments. Parmi ces éléments, les \\(p\\) éléments de la diagonale sont des variances, ce pourquoi elle est parfois appelée matrice de variance-covariance. Les éléments triangulaires inférieurs hors diagonale sont un parfait reflet des éléments supérieurs, p.ex. \\(\\sigma_{1,2} = \\sigma_{2,1}\\). Il y ainsi \\(\\frac{p(p-1)}{2}\\) covariances uniques dans une matrice et \\(p(p+1)/2\\) éléments uniques (variances et covariances). En plus, de ces caractéristiques, la matrice doit être positive semi-définie, ce qui est un terme mathématique impliquant, pour les fins de ce chapitre, que les variances ne peuvent être nulle8. La Figure (11.7) illustre la matrice de covariance \\(\\mathbf{\\Sigma}\\) de la population.\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\n\\begin{array}{cccc}\n\\sigma_{1,1} & \\sigma_{1,2} & ... &  \\sigma_{1,p}\\\\\n\\sigma_{2,1} & \\sigma_{2,2} & ... &  \\sigma_{2,p}\\\\\n...& ...& ... &  \\sigma_{3,p}\\\\\n\\sigma_{p,1} & \\sigma_{p,2} & ... &  \\sigma_{p,p}\\\\\n\\end{array}\n\\right)\n\\tag{11.7}\n\\end{equation}\\]","code":"\ncovariance2 = function(X){\n  # X est une jeu de données ou matrice de n sujets par p variables\n  n <- nrow(X)\n  Xc <- scale(x, scale = FALSE)\n  \n  # L'algèbre matriciel pour le produit permet de calculer\n  # le produit d'une colonne avec les autres colonnes\n  cov.X <- t(Xc) %*% Xc / (n - 1)\n  return(cov.X)\n}"},{"path":"prédire.html","id":"la-matrice-de-corrélation-en-termes-dalgèbre-matricielle","chapter":" 11 Prédire","heading":"11.1.3 La matrice de corrélation en termes d’algèbre matricielle","text":"Une matrice de corrélation est une matrice de covariance dont les variables sont standardisées. Cela implique de transformer la matrice afin que toute la diagonale soit à l’unité, c’est-à-dire que toutes les variances soient égalent à 1. Cela permet d’avoir une interprétation standardisée des corrélations, car celles-ci sont indépendantes des métriques originales. Les autres éléments conceptuels de la matrice de covariance s’appliquent pour la matrice de corrélation.Pour transformer la matrice de covariance en matrice de corrélation, trois techniques sont possibles.La première est de standardiser \\(\\mathbf{X}\\) préalablement au calcul de la covariance.La deuxième méthode est de standardiser la matrice de covariance en termes d’algèbre matricielle, où \\(\\mathbf{S}\\) est la matrice de covariance. Pour ce faire, pré et post multiplie \\(\\mathbf{S}\\) par \\(\\mathbf{D}\\), comme l’équation (11.8).\\[\\begin{equation}\n\\mathbf{R}=\\mathbf{D_{S}}\\mathbf{S}\\mathbf{D_{S}}\n\\tag{11.8}\n\\end{equation}\\]Pour obtenir \\(\\mathbf{D_S}\\), il faut :extraire de la matrice les variances de la diagonale (avec la fonction mathématique \\(\\text{diag}\\));extraire de la matrice les variances de la diagonale (avec la fonction mathématique \\(\\text{diag}\\));faire la racine carrée pour obtenir des écarts types;faire la racine carrée pour obtenir des écarts types;recréer une matrice carrée avec la fonction mathématique \\(\\text{diag}\\), et;recréer une matrice carrée avec la fonction mathématique \\(\\text{diag}\\), et;finalement inverser la matrice.finalement inverser la matrice.Autrement dit, \\(\\mathbf{D_S}\\) correspond à l’équation (11.9).\\[\\begin{equation}\n\\mathbf{D}_{\\mathbf{S}} = (\\text{diag}(\\sqrt{\\text{diag}(\\mathbf{S})})^{-1})\n\\tag{11.9}\n\\end{equation}\\]Le calcul complet de l’équation (11.8) en ajoutant (11.9) est représenté par l’équation (11.10).\\[\\begin{equation}\n\\mathbf{R} = (\\text{diag}(\\sqrt{\\text{diag}(\\mathbf{S})})^{-1}) \\mathbf{S} (\\text{diag}(\\sqrt{\\text{diag}(\\mathbf{S})})^{-1})\n\\tag{11.10}\n\\end{equation}\\]En code R, l’équation (11.10) se traduit ainsi.Lorsqu’une matrice est passée comme argument à diag(), elle extrait les éléments de la diagonale pour en faire un vecteur. Si un vecteur est passé en argument, alors diag() retourne une matrice avec les éléments du vecteur en diagonale. La fonction solve() calcule l’inverse d’une matrice, comme l’utilisateur pourrait attendre de \\(\\mathbf{X}^{-1}\\). Il s’agit certainement de l’aspect le plus biscornu de R. Enfin, l’opérateur %*% est le produit matriciel.La troisième méthode est d’utiliser la fonction de base cov2cor() pour transformer la matrice de covariance en matrice de corrélation, ce qui est plus simple et plus rapide que la deuxième option, mais qui cache ce qui se réalise vraiment, soit l’équation (11.10).","code":"\nXz <- scale(X)\nn <- nrow(x)\ncor.X <- t(Xz) %*% Xz / (n - 1)\nR = solve(diag(sqrt(diag(S)))) %*% S %*% solve(diag(sqrt(diag(S))))"},{"path":"prédire.html","id":"la-régression","chapter":" 11 Prédire","heading":"11.2 La régression","text":"Jusqu’à présent, des rudiments de la covariance et de la corrélation ont été présentés dans le d’introduire graduellement l’algèbre matricielle. L’objectif étant atteint, le regard porte maintenant sur la régression en tant que moyen de prédire une variable \\(y\\) à partir d’un ensemble de variables \\(\\mathbf{X}\\).Quelle est la différence entre les analyses de covariance et corrélation comparativement à l’analyse de régression? Bien que la logique sous-jacente soit très similaire, il faut maintenant déterminer une variable différente de l’autre, c’est-à-dire distinguer une variable dépendante, des variables indépendantes. Les variables indépendantes prédisent la variable dépendante un peu comme dans un modèle déterministe, il faut décider de la cause (variables indépendantes) et l’effet (variable dépendante), ce dernier étant généré par les premiers.Plutôt que de traiter chaque paire de variable comme c’est le cas avec l’analyse de corrélation, la régression étudie la relation entre la variable dépendante et toutes les variables indépendantes, et ce, simultanément. Ainsi, les variables indépendantes sont contrôlées entre elles pour évaluer leur effet sur la variable dépendante et un modèle de prédiction plus juste est obtenu (autant que faire ce peut avec un modèle statistique).","code":""},{"path":"prédire.html","id":"le-modèle-de-régression-simple","chapter":" 11 Prédire","heading":"11.2.1 Le modèle de régression simple","text":"L’équation pour un modèle de régression simple se résume au cas bivarié, soit la prédiction de \\(y\\) par une seule variable \\(x\\).\\[\\begin{equation}\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\tag{11.11}\n\\end{equation}\\]Dans ce modèle, \\(y\\) est la variable prédite, \\(x\\) est le seul prédicteur, \\(\\beta_0\\) est l’ordonnée à l’origine, \\(\\beta_1\\) est le coefficient de régression et \\(\\epsilon\\), l’erreur dans la variable \\(y\\) indépendante (non corrélée, ni fonctionnellement liée) de \\(x\\).Il vaut la peine de se pencher un peu plus sur ces paramètres. L’ordonnée \\(\\beta_0\\) est la moyenne de \\(y\\) lorsque \\(x=0\\), autrement dit, contrôlée par la variable indépendante. Si \\(x\\) est centrée, alors \\(\\beta_0\\) égale la moyenne de \\(y\\); si \\(x\\) et \\(y\\) sont centrées, alors \\(\\beta_0=0\\). La pente \\(\\beta_1\\) est le coefficient de régression, le degré selon lequel \\(x\\) prédit \\(y\\). Ce paramètre dépend de la corrélation entre ces deux variables. En fait, si \\(x\\) et \\(y\\) sont standardisées, alors \\(\\beta_1\\) correspond à la corrélation entre les deux9. Le paramètre \\(\\epsilon\\) est l’erreur du modèle. Par définition, l’erreur est normalement distribuée et sa moyenne est de \\(0\\). Seule l’erreur standard (l’écart type de l’erreur) change.La Figure 11.3 illustre ces paramètres. L’ordonnée \\(\\beta_0\\) est représentée en pointillé du centre \\((0,0)\\) jusqu’à la valeur de \\(y\\) lorsque \\(x = 0\\). La pente \\(\\beta_1\\) correspond à l’inclinaison de la pente (droite oblique pointillé). La droite oblique pointillée représente la prédiction de \\(y\\), soit \\(\\hat{y}\\) par \\(x\\). qui correspond à l’équation (11.11), mais en excluant \\(\\epsilon\\).\\[\\begin{equation}\n\\hat{y_i} = \\beta_0 + \\beta_1 x_i\n\\tag{11.12}\n\\end{equation}\\]L’équation (11.12) exprime toute la ligne pointillée, soit la meilleure prédiction possible. Celle-ci n’est pas parfaite et contient des erreurs, des résidus, autrement dit, ce qui n’est pas prédit par \\(x\\). Les courtes lignes verticales grises représentent les résidus des participants, soit \\(\\epsilon_i\\), la part de \\(y\\) qui n’est pas prédite par \\(x\\) pour la participant \\(\\). Les résidus se distribuent normalement avec une moyenne de \\(0\\) et un écart type de \\(\\sigma_{\\epsilon}\\). Plus \\(\\sigma_{\\epsilon}\\) est petit (tend vers 0), plus les points seront près de la droite.Une façon de connaître la qualité de la droite est le coefficient de détermination. Ce coefficient correspond à l’équation (11.13).\\[\\begin{equation}\nR^2 = 1-\\frac{\\sigma^2_{\\epsilon}}{\\sigma^2_y}\n\\tag{11.13}\n\\end{equation}\\]Le ratio \\(\\frac{\\sigma^2_{\\epsilon}}{\\sigma^2_y}\\) représente la proportion de variance de \\(y\\) non expliquée par les variables indépendantes. Cette proportion s’étend de 0 à 1. La différence \\(1-\\frac{\\sigma^2_{\\epsilon}}{\\sigma^2_y}\\) donne l’inverse de cette proportion… la variance expliquée! Allant de 0 à 1, le coefficient de détermination indique la qualité de la prédiction, 1 étant une relation parfaite.Pour aller un peu plus loin, dans un modèle bivarié, le coefficient de détermination correspond au coefficient de corrélation au carré, \\(r^2\\), la force du lien entre \\(x\\) et \\(y\\). La valeur \\(1-r^2\\) correspond donc la variance des résidus standardisés, \\(\\sigma^2_{\\epsilon}\\).\nFigure 11.3: Diagramme de dispersion\n","code":""},{"path":"prédire.html","id":"lanalyse-de-régression","chapter":" 11 Prédire","heading":"11.2.2 L’analyse de régression","text":"Lorsque le jeu de données est obtenu, il est temps de procéder à l’analyse de régression. Essentiellement, l’analyse de régression produit à peu près ceci en algèbre matricielle.Pour aider la compréhension, voici une explication avec le modèle linéaire simple assumant des variables centrées. Le modèle correspond à\\[\ny = \\beta x\n\\]où l’erreur, \\(\\epsilon\\) n’est pas explicitée. Il faut isoler \\(\\beta\\) afin de l’estimer, soit l’opération suivante,\\[\n\\beta = \\mathbb{E}(\\frac{y}{x})\n\\]où le symbole \\(\\mathbb{E}\\) signifie l’espérance (la moyenne). En multipliant par \\(\\frac{x}{x}\\) de chaque côté de l’équation, cela produit l’équation suivante.\\[\\begin{equation}\n\\beta = \\mathbb{E}(\\frac{xy}{xx}) = \\frac{\\sigma_{xy}}{\\sigma^2_x}\n\\tag{11.14}\n\\end{equation}\\]Dans l’équation (11.14), le numérateur rappelle la covariance et au dénominateur la variance de \\(x\\). Comment généraliser pour \\(k\\) variables? En algèbre matricielle et dans la mesure où les variables contenues dans \\(\\mathbf{X}\\) sont centrées, cela revient au même que de calculer l’équation (11.15).\\[\\begin{equation}\n\\hat{\\mathbf{B}} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} y\n\\tag{11.15}\n\\end{equation}\\]Comme pour l’équation (11.14), la composante \\((X^{\\prime} X)^{-1}\\) agit en dénominateur (par l’exposant \\(-1\\)) et correspond à la matrice de variance-covariance des variables de \\(X\\) ensemble, alors que \\(X^{\\prime} y\\) agit comme le numérateur, soit la covariance entre les variables de \\(X\\) avec \\(y\\).Pour l’erreur standard, il s’agit de calculer l’équation (11.16).\\[\\begin{equation}\n\\text{var}(\\hat{B}) = \\sigma^2 \\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\n\\tag{11.16}\n\\end{equation}\\]La racine carrée de ce résultat donne l’erreur standard (standard error) ou erreur type.Le ratio \\(\\frac{B}{\\text{se}_B} \\sim t_{n-p-1}\\), soit le quotient d’un estimateur par son erreur type, suit une distribution-\\(t\\) avec \\(n-p-1\\) degrés de liberté.Quelques détails sont importants à considérer pour la programmation. Afin d’ajouter l’intercepte (pour estimer \\(\\beta_0\\)), la solution la plus simple est d’ajouter un vecteur d’unité (un vecteur ne contenant que des \\(1\\)) à la matrice \\(\\mathbf{X}\\). En programmation R, l’inversion de matrice se fait par la fonction solve() et non pas avec un signe d’exposant. En syntaxe R, la régression s’écrit comme ceci.Le modèle linéaire peut aussi contenir des variables nominales dans la mesure où celle-ci sont transformées en données factices (dummy coding). En fait, une analyse de variance n’est au fond qu’une régression dans laquelle les variables nominales sont transformées en données factices, puis utilisées en variable indépendante. Les \\(\\beta\\) de la régression correspondent aux moyennes \\(\\mu\\), si le participant appartient (1) à tel ou tel autre groupe ou (0) autrement. Bien que les logiciels produisent souvent des sorties statistiques différentes en fonction de l’analyse demandée, les mathématiques sous-jacentes sont les mêmes.","code":"\nregression <- function(y, X){\n  # Taille d'échantillon et nombre de variables\n  n <- nrow(X)\n  p <- ncol(X)\n  \n  # Joindre un intercepte\n  X <- cbind(intercept = 1, X)\n  X <- as.matrix(X)\n  \n  # Calculer les coefficients de régression\n  B <- solve(t(X) %*% X) %*% t(X) %*% y\n  \n  #Calculer la variance résiduelle\n  var.e <- var(y - X %*% B)\n  \n  # Calculer les erreurs standards des beta\n  se.B <- sqrt(c(var.e) * diag(solve(t(X) %*% X)))\n  \n  # Valeurs-t\n  vt <- B / se.B\n  \n  # Valeurs-p\n  vp <- (1-pt(abs(vt), df = n - p - 1)) * 2\n  \n  # Création d'un tableau de sortie\n  resultats <- data.frame(Estimate = B, \n                          Std.Error = se.B, \n                          t.value = vt, \n                          p.value = vp) \n  return(resultats)\n}  "},{"path":"prédire.html","id":"la-création-de-données-1","chapter":" 11 Prédire","heading":"11.2.3 La création de données","text":"Une façon simple et efficace de créer des données à ce stade est la package MASS dont un aperçu été donné dans le chapitre Analyser.La matrice de covariance, \\(\\mathbf{\\Sigma}\\), pour \\(p=3\\), s’écrit comme l’équation (11.17).\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\n\\begin{array}{ccc}\n\\sigma_{1,1} & \\sigma_{1,2} & \\sigma_{1,3}\\\\\n\\sigma_{2,1} & \\sigma_{2,2} & \\sigma_{2,3}\\\\\n\\sigma_{3,1} & \\sigma_{3,2} & \\sigma_{2,3}\n\\end{array}\n\\right)\n\\tag{11.17}\n\\end{equation}\\]Il convient d’écrire \\(\\mathbf{\\Sigma}\\) (sigma majuscule) et \\(\\sigma\\) (sigma minuscule) plutôt que \\(\\mathbf{S}\\), car il s’agit de la matrice de covariance de la population. Le résultat de S = cov(donnees) est empirique et la notation \\(\\mathbf{S}\\) est plus appropriée. Comme il y \\(p=3\\) variables dans la syntaxe, il faudra préalablement spécifier \\(3*4/2 = 6\\) arguments : \\(p = 3\\) variances \\(\\sigma_{1,1},\\sigma_{2,2},\\sigma_{3,3}\\) et \\(\\frac{p(p-1)}{2}=3(2)/2 = 3\\) covariances \\(\\sigma_{1,2},\\sigma_{1,3},\\sigma_{2,3}\\).Une autre façon de créer des données en fonction d’un modèle linéaire plutôt qu’à partir de la matrice de corrélation (comme avec MASS) est de reprendre l’équation (11.11) et de spécifier les paramètres libre. D’abord, il faut remplacer les paramètres du modèle par des valeurs, \\(\\beta_0\\) et \\(\\beta_1\\), pour ensuite créer deux variables aléatoires de taille \\(n\\) (la taille d’échantillon), une première pour \\(x\\) et une seconde pour \\(\\epsilon\\). Les hypothèses sous-jacentes aux modèles linéaires assument que l’erreur (\\(\\epsilon\\)) est distribuée normalement (avec implicitement une moyenne de 0), la fonction rnorm() pourra jouer le rôle. Pour \\(x\\), il n’y pas de distribution à respecter, mais une distribution normale fait très bien l’affaire. Voici un exemple de code R. En spécifiant une taille d’échantillon très grande n = 10000, l’erreur échantillonnalle est considérablement réduite.Si l’utilisateur souhaite ajouter une autre variable, il lui suffit d’ajouter un \\(\\beta\\) supplémentaire et de créer une autre variable aléatoire.Cette méthode de création de données possède toutefois des limites. Principalement, elle ne spécifie pas les propriétés statistiques désirables, comme la corrélation entre les variables. Quelle est la corrélation entre xet y dans l’exemple précédent? Il est bien sûr possible de déterminer ces valeurs pour la population posteriori. Il faut résoudre l’équation (11.18).\\[\\begin{equation}\n\\rho_{x,y} = \\beta_1 \\frac{\\sigma_x}{\\sigma_y}\n\\tag{11.18}\n\\end{equation}\\]Certaines valeurs sont déjà connues, car préspécifiées par l’utilisateur, \\(\\beta_1 = 1\\) et \\(\\sigma_x = 1\\). Qu’en est-il de \\(\\sigma_y\\)? L’utilisateur n’pas spécifié la valeur de la variance de \\(y\\), il plutôt choisi la valeur de la variance de l’erreur résiduelle, \\(\\sigma^2_{\\epsilon}\\). La loi de la somme des variances permet de calculer cette valeur. Pour le lecteur intéressé, les réponses sont \\(\\sigma^2_y = \\beta_1^2\\sigma^2_x+\\sigma^2_{\\epsilon} = 10\\) et donc \\(\\rho_{x,y} = \\frac{\\beta_1 \\sigma_x}{\\sigma_y} = 0.316\\) (les détails sont présentés dans le chapitre Créer).La limite liée à la méthode de création de données est maintenant flagrante. En plus de ne pas connaître la corrélation entre les variables, la variance de \\(y\\) n’est pas connue priori. La stratégie de spécification est ainsi de choisir des valeurs et d’espérer qu’elles soient conformes aux attentes. Pire, s’il y avait plusieurs variables indépendantes, elles seraient toutes non corrélées entre elles, alors que l’utilisateur pourrait le vouloir autrement, mais cette première technique ne le permet pas.Pour l’utilisateur qui crée son jeu de données, ces caractéristiques sont souvent plus essentielles que de spécifier à l’avance la variance résiduelle. Pour résoudre cette situation, la solution est de spécifier un modèle standardisé, puis de le déstandardiser (ajouté des moyennes et des variances posteriori).La philosophie de modélisation de cet ouvrage repose sur l’idée selon laquelle, un modèle doit être standardisé au départ puis déstandardisé. Cette logique ne se prêtera pas à tous les contextes. Pour certains la difficulté sera immense; pour d’autres, cela ne respectera pas les objectifs. En partant d’un modèle standardisé toutefois, la matrice de corrélation est connue à l’avance et la variance est spécifiée directement par l’utilisateur. Les tailles d’effets attendues sont également assurées. Il suffit de dériver la variance résiduelle du modèle plutôt que de la spécifier.En assumant un modèle linéaire,\\[\\begin{equation}\ny = \\beta_0 + \\beta_1 x_1 + ... +\\beta_k x_k + \\epsilon\n\\tag{11.19}\n\\end{equation}\\]où l’équation (11.19) correspond à la généralisation de l’équation (11.11) pour \\(k\\) variables indépendantes, il est possible d’isoler \\(\\epsilon\\). La variance se calcule alors comme l’équation (11.20), pour le cas générale.\\[\\begin{equation}\n\\sigma^2_{\\epsilon} = \\sigma^2_y - \\mathbf{B}^{\\prime}\\mathbf{R}\\mathbf{B}\n\\tag{11.20}\n\\end{equation}\\]où \\(\\mathbf{R}\\) est la matrice de corrélation et \\(\\mathbf{B}\\) est le vecteur contenant tous les \\(\\beta\\) standardisés. Pour assurer un scénario standardisé \\(\\sigma^2_y = 1\\). La seule condition sous-jacente à l’équation (11.20) est de s’assurer que \\(\\sigma^2_{\\epsilon} > 0\\), c’est-à-dire en vérifiant que \\(\\mathbf{B}^{\\prime}\\mathbf{R}\\mathbf{B} < \\sigma^2_y\\), autrement la variance est négative, ce qui est impossible. En termes de syntaxe R, l’équation (11.20) correspond à ceci.L’avantage de cette technique est () de pouvoir spécifier les corrélations entre les variables indépendantes avec la matrice \\(\\mathbf{R}\\); (b) de déterminer à l’avance la variance de \\(y\\) et (c) que le vecteur \\(\\mathbf{B}\\) contient les \\(\\beta\\) standardisés qui sont dans ce contexte les corrélations partielles qui relient chacune des variables indépendantes à la variable dépendante (des tailles d’effet) en contrôlant pour chacune d’elles.L’utilisateur crée par la suite les données en spécifiant le vecteur \\(\\mathbf{B}\\) et en créant une variable basée sur la matrice de corrélation. Voici un exemple pour \\(k=3\\) variables centrées suivant une distribution normale multivariée avec la matrice de corrélation \\(\\mathbf{R}\\).\\[\n\\mathbf{R} = \\left(\n\\begin{array}{ccc}\n1 & .2 & .3\\\\\n.2 & 1 & .1\\\\\n.3 & .1 & 1\n\\end{array}\n\\right)\n\\]\nUne fois les données de \\(\\mathbf{X}\\) créées, avec la fonction MASS::mvrnorm(), comme il été fait précédemment, il suffit de multiplier \\(\\mathbf{X}\\) avec \\(\\mathbf{B}\\) et d’ajouter la variable aléatoire \\(\\epsilon\\) avec la variance appropriée pour obtenir la variable dépendante \\(y\\).Maintenant, il est possible de déstandardisé X et y en additionnant des moyennes ou multipliant par des écarts types à chaque variable.","code":"\n# Création de la matrice de covariance pour p = 3\nSigma <- matrix(c(s11, s12, s13,\n                  s12, s22, s23,\n                  s13, s23, s33), \n                nrow = 3, ncol = 3)\n\n# Création des données \ndonnees <- data.frame(MASS::mvrnorm(n = n, \n                                    mu = c(0, 0), \n                                    Sigma = Sigma))\nn <- 10000 # Taille d'échantillon\n# Les betas\nbeta0 <- 5\nbeta1 <- 1\n\n# Deux variables aléatoires tirées de distributions normales.\n# Les moyennes sont nulles et \n# les écarts types sont spécifiés\nx <- rnorm(n = n, sd = 1)\ne <- rnorm(n = n, sd = 3)\n\n# Création de la variable dépendante\ny <- beta0 + beta1 * x + e\n# Cacluler la variance de epsilon\nvar_e = var_y - t(B) %*% R %*% Bset.seed(42)  # Pour reproductibilité\nn <- 1000      # Taille d'échantillon\nk <- 3         # Nombre de variables indépendantes\n\n# Matrice de corrélation\nR <- matrix(c(1, .2, .3,\n             .2, 1, .1,\n             .3, .1, 1), k, k)\n# Moyennes\nmu <- rep(0, k)\n\n# Choix des betas standardisés\nB <- c(beta1 = .2, beta2 = -.5, beta3 = .3)\n\n#variance de epsilon\nvar_e <- 1 - t(B) %*% R %*% B\n\n# Créations des variables aléatoires\nX <-  MASS::mvrnorm(n = n, mu = mu, Sigma = R)\ne <- rnorm(n = n, sd = sqrt(var_e))\n\n# Création de la variable dépendante\ny <- X %*% B + e\n\n# Création du jeu de données\njd <- data.frame(y = y, X = X)\n\n# Quelques vérifications\n# Les données\nhead(jd)\n>        y    X.1    X.2     X.3\n> 1  0.635 -0.956 -2.567  0.3239\n> 2 -0.264  0.672 -0.172  0.5200\n> 3  0.341  0.885 -1.369 -0.6387\n> 4 -0.602  0.778 -1.104 -1.2678\n> 5  0.401  0.360  0.286 -1.4336\n> 6 -1.076 -0.210  0.621 -0.0399\n\n# La matrice de corrélation entre les variables indépendantes\n# Très près des valeurs choisies à la troisième décimale\ncor(X)\n>       [,1]  [,2]  [,3]\n> [1,] 1.000 0.188 0.279\n> [2,] 0.188 1.000 0.136\n> [3,] 0.279 0.136 1.000\n\n# La variance de y (encore une fois très près)\nvar(y)\n>      [,1]\n> [1,] 1.03"},{"path":"prédire.html","id":"conditions-dapplication-de-la-régression","chapter":" 11 Prédire","heading":"11.2.4 Conditions d’application de la régression","text":"La régression possède quatre hypothèses sous-jacentes :La vraie relation est linéaire;La vraie relation est linéaire;Les observations sont indépendantes.Les observations sont indépendantes.Les résidus sont normalement distribués;Les résidus sont normalement distribués;La variance résiduelle est homoscédastique.La variance résiduelle est homoscédastique.Les deux premiers points sont davantage méthodologiques que statistiques bien que leurs conséquences soient réelles. La relation entre les variables doit être linéaire. La régression tient compte des relations en ligne droite, si la relation entre deux variables suit une courbe, elle ne pourra être adéquatement analysée. La Figure 11.4 montre trois relations possibles entre deux variables. Bien que la relation soit très forte, peu importe la forme de la relation (les données suivent un parton très évident), seule celle au centre (relation linéaire) donne un résultat indiquant un lien fort. Il existe des techniques de transformation de données lorsqu’elles sont théoriquement justifiées. La relation quantitative entre l’âge et la quantité de rapport sexuel est un exemple (caricatural) de relation non linéaire : elle débute à l’adolescence, atteint son apogée à l’âge de jeune adulte, puis décroît progressivement. La seconde hypothèse est que les unités d’observation doivent être indépendantes. Techniquement, chaque unité devrait avoir une chance égale et indépendante d’être choisie. La régression est robuste à ce genre de violation, mais il faut toujours conserver cette idée en tête lorsque le devis d’étude est conceptualisé et lors des analyses. Les élèves dans une même salle de classe ne sont techniquement pas indépendants puisqu’ils sont tous corrélés. Ils ont le même enseignant, les mêmes pairs, les mêmes locaux, etc. Ce sont des variables qui peuvent toutes à leur façon avoir des effets sur les comportements des élèves. Dans ce cas, recourir à des analyses multiniveaux sera la seule possibilité pour tenir compte de cette violation et bien représenter les modèles. Un autre exemple est la corrélation entre différents temps de mesure sur une même unité (qui est corrélée avec elle-même). Dans ce cas, ce sera des analyses pour des devis temporels (les analyses multiveaux peuvent également tenir compte du temps).Les deux autres considérations, qui sont elles d’ordre statistique, concernent les résidus (l’écart entre la prédiction et les valeurs réelles de \\(y\\)). Les distributions des variables n’ont pas à être normales; elles peuvent suivre différentes distributions de probabilités. Par contre, l’erreur résiduelle, elle, doit être normalement distribuées. Il s’agit d’un postulat de l’estimation des moindres carrés. La dernière hypothèse concerne la variance résiduelle homoscédastique, c’est-à-dire que l’écart entre les résidus et les valeurs prédites restent constantes, peu importe le niveau sur la droite de régression. Si ce n’est pas le cas pour l’une ou l’autre, c’est qu’une variable théorique important est vraisemblablement négligée ou qu’une des relations n’est pas linéaire entre les variables.\nFigure 11.4: Différentes formes de relation\n","code":""},{"path":"prédire.html","id":"lanalyse-de-régression-avec-r","chapter":" 11 Prédire","heading":"11.2.5 L’analyse de régression avec R","text":"R de base offre la fonction lm() pour linear model (modèle linéaire) afin de réaliser une régression. Pour réaliser l’analyse, deux éléments sont primordiaux : le jeu de données et le modèle. Le jeu de données est en soi assez évident. Le modèle linéaire est quant à lui représenté par l’équation (11.19).\\[\\begin{equation}\ny = \\beta_0 + \\beta_1 x_1 + ... +\\beta_k x_k + \\epsilon\n\\tag{11.19}\n\\end{equation}\\]où l’équation (11.19) correspond à la généralisation de l’équation (11.11) pour \\(k\\) variables indépendantes.Pour écrire le modèle en syntaxe R, il faut remplacer les \\(x\\) par le nom des variables dans le jeu de données, utiliser le ~ (tilde) pour délimiter les variables dépendantes à gauche des variables indépendantes à droite. Les variables indépendantes sont délimitées, comme dans l’équation (11.19), par des signes d’addition +. Il est aussi de définir des effets d’interaction (modération) avec le signe de multiplication * (la section [Modérer] approfondie davantage ce sujet). Les symboles - (soustraction) et / (division) ne fonctionnent pas. L’intercepte (\\(\\beta_0\\)) est ajouté par défaut. La fonction n’exige pas de mettre la formule entre guillemets10.Pour ajouter une variable, il suffit de VD ~ VI1 + VI2; pour ajouter un effet d’interaction, il est possible de faire VD ~ VI1 * VI2. Pour ajouter une variable nominale (catégorielle), il suffit d’ajouter la variable comme n’importe quelles autres x, mais en s’assurant bien qu’elle soit désignée comme un facteur dans le jeu de données. Si ce n’était pas le cas, la fonction .factor() corrige la situation.La fonction lm() en elle-même n’imprime que peut de résultats. Pour obtenir l’information complète, il faut requérir le sommaire avec la fonction summary(). Le sommaire des résultats contient toute l’information qu’un expérimentateur peut désirer. Il y les coefficients de régression Estimate, leur erreur standard Std. Error, leur valeur-\\(t\\) t value et leur valeur-\\(p\\) Pr(>|t|). Tout cela peut être extrait avec summary(res.lm)$... en remplaçant ... par les éléments désirés. Au-dessous de la sortie imprimée, il y également le coefficient de détermination (\\(R^2\\), R-squared), les degrés de liberté et la valeur-\\(p\\) associé au modèle.Les résultats de lm() peuvent être comparés avec la fonction maison regression() expliquée auparavant à la section [L’analyse de régression].Un manuscrit rapporte les résultats à peu prêt comme ceci.Le modèle tester obtient un coefficient de détermination de \\(R^2(996) = 0.383, p < .001\\). Les trois prédicteurs sont liés significativement à la variable dépendante, respectivement \\(X_1: \\beta_1 = 0.214\\), \\(p = < .001\\), \\(X_2: \\beta_2 = 0.214\\), \\(p = < .001\\), \\(X_3: \\beta_3 = 0.31\\), \\(p = < .001\\).Évidemment, comme l’exemple est artificiel, il y peu de chose à dire sans devoir fabriquer de toutes pièces des interprétations alambiquées bien que cela fût fort bénéfique pour la carrière de certains.Pour vérifier la qualité des résultats, il faut vérifier la distribution des résidus. Pour ce faire, il faut extraire les résidus et les valeurs prédites. Pour la création de graphiques, il est plus simple d’ajouter ces scores au jeu de données. Les fonctions resid() et predict() extraient les résidus et les prédictions en y insérant comme argument le sommaire de la fonction lm() obtenu avec les données.Une fois ces valeurs extraites, le package ggplot2 permet de réaliser rapidement des graphiques (voir Visualiser), comme le diagramme de dispersion à la Figure 11.5 ou l’histogramme des résidus à la Figure 11.6. Dans les meilleures situations, les résidus sont distribués normalement dans l’histogramme et aucune tendance n’est discernable dans le diagramme de dispersion. Si ce n’est pas le cas, il faut étudier davantage la situation, par exemple, une relation non linéaire imprévue. Les Figures 11.5 et 11.6 ne signalent aucun problème, ce qui est attendu considérant la création des données employées.\nFigure 11.5: Relation entre prédicitons et résidus\n\nFigure 11.6: Histogramme des résidus\n","code":"# Créer un jeu de données à partir \n# des variables de la syntaxe précédente\nres.lm <- lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n\n# Les résultats\nres.lm\n> \n> Call:\n> lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n> \n> Coefficients:\n> (Intercept)          X.1          X.2          X.3  \n>     -0.0171       0.2139      -0.5520       0.3095\n\n# Sommaire des résultats\nsummary(res.lm)\n> \n> Call:\n> lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n> \n> Residuals:\n>     Min      1Q  Median      3Q     Max \n> -2.5227 -0.5546 -0.0119  0.5513  2.5762 \n> \n> Coefficients:\n>             Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)  -0.0171     0.0253   -0.68      0.5    \n> X.1           0.2139     0.0266    8.03  2.7e-15 ***\n> X.2          -0.5520     0.0258  -21.42  < 2e-16 ***\n> X.3           0.3095     0.0263   11.76  < 2e-16 ***\n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 0.8 on 996 degrees of freedom\n> Multiple R-squared:  0.383,   Adjusted R-squared:  0.381 \n> F-statistic:  206 on 3 and 996 DF,  p-value: <2e-16regression(y = jd$y, X = jd[ ,2:4])\n>           Estimate Std.Error t.value  p.value\n> intercept  -0.0171    0.0253  -0.678 4.98e-01\n> X.1         0.2139    0.0266   8.046 2.44e-15\n> X.2        -0.5520    0.0257 -21.449 0.00e+00\n> X.3         0.3095    0.0263  11.775 0.00e+00\n# Ajouter les résidus et scores prédits à la base de données\n# avec la fonction `resid()`\njd$residu <- resid(res.lm)\njd$predit <- predict(res.lm)\n# Diagramme de dispersion prédits par résidus\njd %>% \n  ggplot(mapping = aes(x = predit, y = residu)) + \n  geom_point() \n# Histogramme des résidus\njd %>% \n  ggplot(mapping = aes(x = residu)) + \n  geom_histogram()"},{"path":"prédire.html","id":"la-matrice-de-corrélation-partielle-et-semi-partielle","chapter":" 11 Prédire","heading":"11.3 La matrice de corrélation partielle et semi partielle","text":"En plus de la matrice de corrélation, deux autres types de corrélation peuvent intéressés le chercheur : les corrélations partielles et les corrélations semi partielles. La présentation est faite en terme d’algèbre matricielle, ce qui facilite substantiellement les calculs et la programmation.","code":""},{"path":"prédire.html","id":"les-corrélations-partielles","chapter":" 11 Prédire","heading":"11.3.1 Les corrélations partielles","text":"La corrélation partielle mesure le degré d’association symétrique entre deux variables en contrôlant pour toutes les autres variables. La contribution des autres variables est retirée sur les deux variables cibles. Conceptuellement, pour une paire de variables, il s’agit de retirer l’effet d’un ensemble de variables contrôles sur chacune d’elle, puis de corréler les résidus de la paire. L’équation (11.21) montre le calcul de la matrice de corrélation partielle.\\[\\begin{equation}\n\\mathbf{R}_{\\text{partielle}} = -\\mathbf{D}_{\\mathbf{S}^{-1}}\\mathbf{S}^{-1}\\mathbf{D}_{\\mathbf{S}^{-1}}\n\\tag{11.21}\n\\end{equation}\\]La formule pour calculer \\(\\mathbf{D}_{\\mathbf{S}^{-1}}\\) est la même que (11.9), mais où \\(\\mathbf{S}\\) est remplacée par \\(\\mathbf{S}^{-1}\\). En code R, l’équation (11.21) devient la syntaxe suivante.Par souci esthétique la diagonale de \\(\\mathbf{R}_{\\text{partielle}}\\) est souvent remplacée par l’unité, comme diag(Rp) = 1.","code":"\n# La matrice de corrélation partielle\nRp <- -cov2cor(solve(S))"},{"path":"prédire.html","id":"les-corrélation-semi-partielles","chapter":" 11 Prédire","heading":"11.3.2 Les corrélation semi partielles","text":"La corrélation semi partielle mesure le degré d’association asymétrique entre une variable indépendante et dépendante en contrôlant pour toutes les autres. Conceptuellement, il s’agit de retirer l’effet d’un ensemble de variables contrôles sur la variable dépendante, puis de d’utiliser la variable indépendante comme nouveau prédicteur. Autrement dit, il s’agit de la contribution ajouté d’une variable sur une autre.Le calcul de la matrice de corrélation semi partielle part de la matrice de corrélation partielle et applique un dénominateur pondérant les contributions des autres variables dans la matrice (Kim, 2015). La formule est représenté par l’équation (11.22)\\[\\begin{equation}\n\\frac{\\mathbf{R}_{\\text{partielle}}}{{\\sqrt{\\text{diag}(\\mathbf{S})}\\sqrt{|\\mathbf{S}^{-1}-(\\frac{((\\mathbf{S}^{-1})^2)^{\\prime}}{\\text{diag}(\\mathbf{S}^{-1})}})^{\\prime}|}}\n\\tag{11.22}\n\\end{equation}\\]Voici l’équation en code R.","code":"\n# La matrice de corrélation semi partielle\niS <- solve(S)\nRsp <- -cov2cor(iS) / \n         sqrt(diag(S)) /\n          sqrt(abs(diag(iS) - t(t(iS^2) / diag(iS))))"},{"path":"prédire.html","id":"une-comparaison-entre-partielle-et-semi-partielle","chapter":" 11 Prédire","heading":"11.3.3 Une comparaison entre partielle et semi partielle","text":"La section suivante développe un exemple afin de comparer la corrélation partielle et la corrélation semi partielle. L’équation (11.23) présente un exemple de matrice de corrélation.\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\n\\begin{array}{ccc}\n1 & .2 & 0\\\\\n.2 & 1 & .8\\\\\n0 & .8 & 1\n\\end{array}\n\\right)\n\\tag{11.23}\n\\end{equation}\\]Le code suivant calcule la matrice de corrélation partielle et semi partielle en fonction des équations (11.21) et (11.22).Les matrices Rp et Rsp se lisent comme suit : La ligne (variable indépendante) prédit la colonne (variable dépendante). Cette distinction n’est pas important pour la matrice Rp (corrélations partielles), car les variables sont symétriques, mais est très importantes pour la matrice Rsp (corrélations semi partielles), car la relation est asymétrique. Par exemple, la corrélation semi partielle de \\(x\\) prédit \\(y\\) est de 0.333, mais l’inverse est de 0.2.Plusieurs observations sont possibles.Pour une même paire de variable, une même corrélation partielle et semi partielle sont de même signe et de magnitude comparable.Pour une même paire de variable, la corrélation partielle est toujours plus grande ou égale que la corrélation semi partielle.La matrice de corrélation partielle est symétrique alors que la matrice de corrélation semi partielle ne l’est pas. Cela s’explique du fait que la corrélation semi partielle attribue un rôle (indépendant et dépendant) pour une paire de variable. La contribution des autres variables est retirée de la variable dépendante, puis c’est l’ajout de la variable indépendante qui est d’intérêt. Par exemple, l’effet de la variable x prédite par y en contrôlant par x est de 0.2. Ce lien est limité à cause de l’effet de z sur x.Une dernière observation : Les explications basées sur les diagrammes de Venn pour distinguer les corrélations partielles et semi partielles portent plus souvent à confusion (à long terme) qu’elle n’apporte d’éclaircissement (à court terme), bien qu’elle se retrouve abondamment dans les manuels.\nFigure 11.7: Diagramme représentant l’agencement des variables\nDans la Figure 11.7, tiré de l’exemple avec Sigma ci-haut, la zone \\(\\) illustre la covariance entre \\(x\\) et \\(y\\) au carré11, soit \\(\\sigma_{xy}^2 = \\), et de façon équivalente, \\(sigma^2_{yz} = .8^2\\) \\(sigma^2_{xz} = 0\\). Chaque cercle un aire de 1, par exemple, l’aire du cercle en bas à gauche est \\(+x=.2^2+.96 =1\\). Par simplicité, les autres aires sont précalculés. Un ouvrage indique souvent que la corrélation partielle au carré entre \\(x\\) vers \\(y\\) est égale à \\(/(+y)=.2^2/(.2^2+.32)=.111\\) dont la racine carré donne \\(.333\\), comme prévu. L’inverse, la corrélation partielle au carré entre \\(y\\) vers \\(x\\), devrait être \\(/(+x)\\)? mais cela donne \\(/(+x)=.2^2/(.2^2+.96)=.04\\). La racine carré donne \\(.2\\)… la corrélation semi partielle!? L’équation donne donc plutôt la corrélation semi partielle et non la partielle. En plus, des zones comme la corrélation semi partielles entre \\(x\\) vers \\(z\\), qui est de -0.267 qui au carré donne 0.071, ne sont étrangement pas illustrées. Où est la zone d’aire correspondante? Le pire est certainement que les ouvrages utilisent souvent des agencement de variables plus compliquées que celui-ci, un modèle simple avec deux variables non-corrélées, qui cache davantage ces ambiguïtés.Qu’est-ce qui explique ces divergences? Il revient au fait que les corrélations partielles et semi partielles se basent sur l’inverse de la matrice de covariance, solve(Sigma), la matrice de précisions. Elles se retrouvent ainsi dans un espace différent de celle de la matrice de covariance qui, elle, est bien illustrée dans le diagramme de Venn.","code":"# Créer une matrice de covariance\n# avec des libellées\nSigma <- matrix(c( 1, .2,  0,\n                  .2,  1, .8,\n                   0, .8,  1), \n                nrow = 3, \n                dimnames = list(col <- c(\"x\", \"y\", \"z\"), \n                                row <- col))\n\n# Calculer la matrice de corrélation partielle\nRp <- -cov2cor(solve(Sigma))\ndiag(Rp) <- 1\n\n# Calculer la matrice de corrélation semi-partielle\niS <- solve(Sigma)\nRsp <- Rp / sqrt(diag(Sigma)) /\n          sqrt(abs(diag(iS) - t(t(iS^2) / diag(iS))))\ndiag(Rsp) <- 1\n\n# Sortie\nRp\n>        x     y      z\n> x  1.000 0.333 -0.272\n> y  0.333 1.000  0.816\n> z -0.272 0.816  1.000\nRsp\n>        x     y      z\n> x  1.000 0.333 -0.267\n> y  0.200 1.000  0.800\n> z -0.163 0.816  1.000"},{"path":"créer.html","id":"créer","chapter":" 12 Créer","heading":" 12 Créer","text":"Dans le chapitre sur la régression, les variables indépendantes sont créées simultanément à partir d’une matrice de corrélation, puis des coefficients de régressions sont utilisés pour créer la variable dépendante. Dans un modèle plus élaboré, plutôt que d’ignorer ces corrélations, les liens entre les variables prédictrices peuvent être explicitement modélisés. Cela revient à décrire un modèle de façon à ce qu’une première variable cause la seconde; la première et la seconde causent la troisième et ainsi de suite.La modélisation en système d’équations devient primordial dans les prochaines sections, notamment pour les analyses de trajectoires, de médiation, de modération, et plus. C’est le fondement de la modélisation par équations structurelles (SEM).Dans ce chapitre12, des techniques de modélisation avancées sont présentées. Elles s’appliquent à des modèles de trajectoires récursifs, c’est-à-dire que la causalité s’enchaîne dans une direction, de la première variable vers la dernière13.","code":""},{"path":"créer.html","id":"la-loi-de-la-somme-des-variances","chapter":" 12 Créer","heading":"12.1 La loi de la somme des variances","text":"La loi de la somme des variances explique comment additionner la variance de variables aléatoires, quelle que soit leur distribution (Casella & Berger, 2002). Le cas le plus simple est celui de deux variables aléatoires, comme \\(x_1\\) et \\(x_2\\), et la façon dont elles s’additionnent pour former une troisième variable, \\(y\\). Elles donnent un modèle simple représenté par l’équation (12.1).\\[\\begin{equation}\ny=x_1+x_2\n\\tag{12.1}\n\\end{equation}\\]La variance de leur somme prend la forme suivante\n\\[\\begin{equation}\n\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2\n\\tag{12.2}\n\\end{equation}\\]où \\(\\sigma^2\\) est le symbole habituel de la variance. La variance de la somme (ou de la différence) de deux variables indépendantes (non-corrélées) aléatoires est leur somme. En fait, la somme de \\(p\\) variables indépendantes est la somme de leurs variances. En syntaxe R, ces équations ressemblent à ceci.Dans cet exemple, les variances de x1 et de x3 sont égales à 2 et 3 respectivement. Si ces deux variables sont additionnées pour créer y, l’équation (12.2) implique que la variance est de \\(\\sigma^2_{x_{1}} + \\sigma^2_{x_2} = 2 + 3= 5\\)Le résultat concorde.En pratique, il est rare que les variables prédictrices soient non corrélées. Ici, aucune corrélation n’été précisée entre x1 et x2. En développant le cas général où les variables sont corrélées, si la covariance entre \\(x_1\\) et \\(x_2\\) est représentée par \\(\\sigma_{x_1,x_2}\\), alors la loi de la somme des variances devient ceci.\\[\\begin{equation}\n\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2+2\\sigma_{x_1,x_2}\n\\tag{12.3}\n\\end{equation}\\]L’équation (12.2) est un cas particulier lorsque \\(\\sigma_{x_1 x_2}=0\\). En syntaxe R, pour \\(\\rho = .5\\), cela revient à programmer la syntaxe suivante.Pour rappel, la corrélation s’obtient avec \\(\\rho_{x_1,x_2}=\\sigma_{x_1,x_2}/(\\sigma_{x_1} \\sigma_{x_2})\\) et inversement la covariance est obtenue par \\(\\rho_{x_1,x_2} \\sigma_{x_1} \\sigma_{x_2}=\\sigma_{x_1,x_2}\\). En calculant la covariance d’abord, soit\\[\\sigma_{x_1,x_2} = \\rho_{x_1,x_2} \\sigma_{x_1} \\sigma_{x_2} = 0.5 \\times 1.414 \\times 1.732 = 1.225\\]\net confirmé par R.L’équation (12.3) pour la variance de y mène à\\[\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2+2\\sigma_{x_1,x_2}= 2+3 + 2 \\times 1.225  = 7.449\\]\ntrès près du résultat simulé.La loi des sommes des variances est directement reliée au théorème de Pythagore tel qu’illustré par l’équation (12.2). Deux variables non corrélées peuvent être envisagées comme formant un triangle rectangle. L’équation (12.3) est la règle du cosinus, ou la généralisation du théorème de Pythagore pour les triangles non rectangulaires, de sorte que la corrélation représente géométriquement un angle (Rodgers & Nicewander, 1988).La Figure 12.1 montre un triangle rectangle dont l’hypoténuse représente l’écart type de la somme de deux variables indépendantes. Lorsque l’angle est à 90\\(^\\circ\\), la corrélation est nulle. Il suffit de mettre les droites au carré pour retrouver l’équation (12.2). Lorsque l’angle rétréci, plus les deux lignes \\(\\sigma\\) se rapprochent et projettent sur une plus longue distance la somme des variances. Lorsqu’elles sont collées l’une sur l’autre, la corrélation est parfaite.\nFigure 12.1: Représentation des variables sous forme de triangles\nUne autre façon mathématiquement plus simple de calculer la loi de la somme des variances est de concevoir la variance de la somme de \\(p\\) variables comme la grande somme de leur matrice de covariance. La grande somme est une fonction mathématique informelle qui fait référence à la somme de tous les éléments d’une matrice. Soit \\(\\mathbf{\\Sigma}\\), la matrice de variance-covariance de deux variables aléatoires \\(x_1\\) et \\(x_2\\) (les éléments diagonaux sont des variances), alors\\[\\begin{equation}\n\\sigma_y^2 = \\text{grande somme}(\\mathbf{\\Sigma}) = \\text{grande somme}\\left(\\begin{array}{cc}\n\\sigma_{x_1}^2 & \\sigma_{x_1,x_2} \\\\\n\\sigma_{x_1,x_2} & \\sigma_{x_2}^2\n\\end{array}\\right)\n\\tag{12.4}\n\\end{equation}\\]ce qui, en faisant la somme, conduit à l’équation (12.3). Encore une fois, si \\(\\sigma_{x_1,x_2}=0\\), alors l’équation (12.4) est égale à l’équation (12.2). Cette formulation l’avantage de montrer l’origine des deux covariances dans l’équation (12.3). En code R, la syntaxe est dès plus rudimentaire.Même si la notation matricielle peut sembler peu attrayante au départ, elle devient beaucoup plus intéressante lorsque le nombre de variables, \\(p\\), augmente, car il y \\(p(p-1)/2\\) éléments hors diagonale à ajouter aux \\(p\\) variances. Elle deviendra même indispensable à la fin de ce chapitre.Pour un exemple à \\(p=3\\), l’équation (12.4) devient ceci.\\[\\begin{equation}\n\\sigma_y^2=\\text{grand somme}(\\mathbf{\\Sigma})=\\text{grand somme}\n\\left(\\begin{array}{ccc}\n\\sigma_{x_1}^2&\\sigma_{x_1,x_2}&\\sigma_{x_1 x_3} \\\\\n\\sigma_{x_1,x_2}&\\sigma_{x_2}^2&\\sigma_{x_2 x_3}\\\\\n\\sigma_{x_1 x_3}&\\sigma_{x_2 x_3}&\\sigma_{x_3}^2\n\\end{array}\n\\right)\n\\tag{12.5}\n\\end{equation}\\]Sous la forme linéaire, l’équation (12.5) devient\\[\\begin{equation}\n\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2+\\sigma_{x_3}^2+2\\sigma_{x_1,x_2}+2\\sigma_{x_1 x_3}+2\\sigma_{x_2 x_3}\n\\tag{12.6}\n\\end{equation}\\]et augmente à mesure que \\(p\\) s’accroît.Une opération matricielle équivalente à la grande somme est\\[\\begin{equation}\n\\mathbf{1}^{\\prime} \\mathbf{\\Sigma} \\mathbf{1}\n\\tag{12.7}\n\\end{equation}\\]où \\(1\\) est un vecteur de longueur \\(p\\) contenant seulement des 1. Voici en syntaxe R.Cette opération produit la somme de tous les éléments de \\(\\mathbf{\\Sigma}\\) (Sigma). Cela sera utile pour dériver un cas plus général dans la section suivante.","code":"\nset.seed(1155)            # reproductibilité\nn <- 100000               # taille élevée pour la précision\ns.x1 <- sqrt(2)           # variance de 2 \ns.x2 <- sqrt(3)           # variance de 3\n\n# Création des variables\nx1 <- rnorm(n = n, sd = s.x1) \nx2 <- rnorm(n = n, sd = s.x2)     y <- x1 + x2\nvar(y)\n> [1] 5\n\n# Vérifier par\ns.x1^2 + s.x2^2\n> [1] 5# Création de variables corrélées\nrho <- .5                          # corrélation de .5\ns.x1x2 <- rho * s.x1 * s.x2        # covariance de x et y\nS <- matrix(c(s.x1^2, s.x1x2,      # matrice de covariance\n             s.x1x2, s.x2^2), ncol =  2, nrow = 2)\n\n# Création de variables\nX <- MASS::mvrnorm(n = n, Sigma = S, mu = c(0, 0), empirical = TRUE)\nx1 <- X[,1] ; x2 <- X[,2]\n\n# La somme de deux variables corrélées\ny <- x1 + x2\nvar(y)\n> [1] 7.45\n\n# Vérifier par\ns.x1^2 + s.x2^2 + 2 * s.x1x2\n> [1] 7.45s.x1x2\n> [1] 1.22sum(S)\n> [1] 7.45Un <- c(1, 1)       # Création du vecteur 1\nt(Un) %*% S %*% Un  # Grande somme\n>      [,1]\n> [1,] 7.45"},{"path":"créer.html","id":"ajout-des-constantes-déchelle-beta","chapter":" 12 Créer","heading":"12.1.1 Ajout des constantes d’échelle \\(\\beta\\)","text":"Les équations (12.3), (12.4), (12.5) et (12.6) sont des cas particuliers d’une loi plus générale. Elles ne fonctionnent pas si des constantes d’échelle (scaling constant) sont ajoutées (qui seront plus tard des coefficients de régression) ou pour calculer la différence (un type d’échelle également).La situation où la variable \\(y\\) correspond au produit d’une constante \\(\\beta\\) et de la variable \\(x\\) comme :\\[\\begin{equation}\ny=\\beta x\n\\tag{12.8}\n\\end{equation}\\]un modèle linéaire qui ne comporte pas d’erreur (\\(\\epsilon\\) est ignoré pour l’instant).Il peut être utile de considérer \\(\\beta\\) comme, éventuellement, le degré de relation entre deux variances, mais aussi comme un pur modificateur de l’écart type (également une constante d’échelle). De la même manière, une variable aléatoire avec une moyenne de \\(0\\) et un écart type de \\(1\\), \\(x\\sim \\mathcal{N}(0,1)\\), multipliée par la valeur \\(\\beta\\), un facteur d’échelle arbitraire, devient distribuée comme \\(\\beta x \\sim \\mathcal{N}(0,\\beta)\\). Ainsi, \\(\\beta\\) modifié (ou mis à l’échelle) l’écart type de la distribution. Un cas connexe et fréquemment rencontré est lorsque les données sont normalisées en tant que score-\\(z\\) ou non normalisées (divisées ou multipliées respectivement par \\(\\sigma\\)). La contribution est un écart-type, donc \\(\\beta\\) doit être élevé au carré pour donner la variance de \\(y\\), soit \\(\\beta^2\\). Cela mène à l’équation (12.9) qui permet le calcul de la variance de \\(y\\).\\[\\begin{equation}\n\\sigma_y^2=\\beta^2 \\sigma_x^2\n\\tag{12.9}\n\\end{equation}\\]En termes de syntaxe R, cela se traduit (toujours avec le même exemple).L’équation (12.9) donne \\(\\sigma_y^2=\\beta^2 \\sigma_x^2=4^2 \\times 2= 32\\), ce qui est identique.L’étape suivante consiste à considérer un modèle avec deux variables et deux constantes d’échelles, ce qui rappelle de plus en plus la régression. Le modèle linéaire prend la forme suivante\\[\\begin{equation}\ny=\\beta_1 x_1+\\beta_2 x_2\n\\tag{12.10}\n\\end{equation}\\]qui est le même modèle que l’équation (12.1) où les constantes d’échelle \\(\\beta_i\\) sont ajoutées. Pour considérer les constantes d’échelle, la loi de la somme des variances basée sur l’équation (12.2) devient pour deux variables indépendantes\\[\n\\sigma_y^2=\\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma_{x_2}^2\n\\]\n\net quand ils covarient, comme l’équation (12.3), cela devient\\[\\begin{equation}\n\\sigma_y^2=\\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma_{x_2}^2+2\\beta_1 \\beta_2 \\sigma_{x_1,x_2}\n\\tag{12.11}\n\\end{equation}\\]Dans ces équations, les \\(\\beta_i\\) mettent à l’échelle la variance et la covariance. Ces équations se vérifient avec R (toujours avec le même exemple).tel qu’attendue par l’équation (12.12).\\[\n\\begin{aligned}\n\\sigma_y^2 &= \\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma_{x_2}^2+2\\beta_1 \\beta_2 \\sigma_{x_1,x_2} &=\n4^2 \\times 2 + 5^2 \\times 3 + 2 \\times 4 \\times 5 \\times 1.225 &= 155.99\n\\end{aligned}\n\\]Comme précédemment, la formule en termes d’algèbre matricielle est plus simple et plus élégante, soit\\[\\begin{equation}\n\\sigma_y^2 = \\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B}\n\\tag{12.12}\n\\end{equation}\\]où \\(\\mathbf{B}\\) (\\(\\beta\\) majuscule) est un vecteur contenant tous les coefficients de régression \\(\\beta_i\\) de longueur \\(p\\) prédisant \\(y\\) et s’appliquant à un nombre quelconque de prédicteurs \\(x\\). Le premier \\(\\prime\\) est le symbole de transposition, un opérateur qui pivote la matrice sur sa diagonale et, dans le cas d’un vecteur, retourne les colonnes en lignes et vice-versa (voir son implication dans l’équation (12.13) par exemple). Dans l’équation (12.12), les deux vecteurs \\(\\mathbf{B}\\) revient à élever au carré \\(\\beta_i\\). Si tous les éléments de \\(\\mathbf{B}\\) égalent 1, alors l’équation (12.12) est identique à la définition d’une grande somme, voir l’équation (12.7). En syntaxe R, l’équation (12.12) donne :un code plus général et qui s’appliquera à toutes les situations (peu importe la valeur de \\(p\\), le nombre de variables).Il est intéressant de noter que l’équation (12.12) est la même que les coefficients de détermination, \\(R^2\\) (Cohen et al., 2003), lorsque toutes les variables et les coefficients de régression sont normalisés.Dans le modèle linéaire simple décrit dans l’équation (12.10), l’équation (12.11) devient :\\[\\begin{equation}\n\\sigma_y^2 =\n\\left(\\begin{array}{cc}\n\\beta_1 & \\beta_2\n\\end{array}\\right)\n\\left( \\begin{array}{cc}\n\\sigma^2_{x_1} & \\sigma_{x_1,x_2} \\\\\n\\sigma_{x_2,x_1} & \\sigma^2_{x_2}\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\end{array} \\right)\n\\tag{12.13}\n\\end{equation}\\]ce qui est équivalent à l’équation (12.11). À ce stade, le lecteur peut avoir l’intuition que les équations (12.1) à (12.6) n’étaient qu’un cas particulier où tous les \\(\\beta=1\\).La variance de la différence de deux variables revient à affirmer que le \\(\\beta\\) des variables soustraites est de signe inverse (s’il est négatif, il devient positif ou s’il est positif, il devient négatif), c’est-à-dire \\(-\\beta\\), ce qui pour le modèle conduit à\\[\ny=\\beta_1 x_1-\\beta_2 x_2\n\\]\n\nou, de manière équivalente\\[\ny=(\\beta_1) x_1+(-\\beta_2) x_2\n\\]\n\ndonnent\\[\\begin{equation}\n\\begin{aligned}\n\\sigma_y^2 &=\n\\left(\\begin{array}{cc}\n\\beta_1 & -\\beta_2\n\\end{array}\\right)\n\\left( \\begin{array}{cc}\n\\sigma^2_{x_1} & \\sigma_{x_1,x_2} \\\\\n\\sigma_{x_2,x_1} & \\sigma^2_{x_2}\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} \\beta_1 \\\\ -\\beta_2 \\end{array} \\right) \\\\[15pt]\n& = \\beta_1^2 \\sigma_{x_1}^2+(-\\beta_2)^2 \\sigma^2_{x_2} + 2(\\beta_1)(-\\beta_2)\\sigma_{x_1,x_2} \\\\[15pt]\n& = \\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma^2_{x_2} - 2\\beta_1\\beta_2\\sigma_{x_1,x_2}\n\\end{aligned}\n\\tag{12.14}\n\\end{equation}\\]Comme prévu, la variance de la différence de deux variables aléatoires est la somme de leurs variances en soustrayant deux fois leur échelle de covariance par \\(\\beta\\). Cela se vérifie avec R.","code":"beta <- 4\n# La variance de x1 est toujours de 2\ny <- beta * x1\nvar(y)\n> [1] 32\n\n# Vérifier par\nbeta^2 * s.x1^2\n> [1] 32# En suivant, l'exemple précédent\nbeta1 <- 4\nbeta2 <- 5\ny <- beta1 * x1 + beta2 * x2\nvar(y) \n> [1] 156\n\n# Vérifier par\nbeta1^2 * s.x1^2 + beta2^2 * s.x2^2 + \n  2 * beta1 * beta2 * s.x1x2 \n> [1] 156# Joint beta1 et beta2 dans un vecteur\nbeta <- c(beta1, beta2)\n\n# S est la matrice de covariance calculée auparavant.\nbeta %*% S %*% beta \n>      [,1]\n> [1,]  156y = beta1 * x1 - beta2 * x2\nvar(y)\n> [1] 58\n\n# Vérifier par\nbeta = c(beta1, -beta2)   # Signe négatif, très important!\nbeta %*% S %*% beta       # Forme matricielle\n>      [,1]\n> [1,]   58"},{"path":"créer.html","id":"implications-pour-la-modélisation","chapter":" 12 Créer","heading":"12.2 Implications pour la modélisation","text":"La loi de la somme des variances de nombreuses implications dans la modélisation, en particulier si les données sont façonnées selon certaines caractéristiques souhaitables, comme dans les modèles linéaires. La régression linéaire est une approche permettant de modéliser des effets additifs (variables indépendantes, \\(x_i\\)) pour prédire une variable dépendante (\\(y\\)). La linéarité fait référence à la propriété d’une fonction d’être compatible avec l’addition et la mise à l’échelle. En tant que tel, il existe une relation directe entre les équations ci-dessus et le modèle linéaire :\\[\\begin{equation}\ny=\\beta_1 x_1 + ... +\\beta_p x_p+\\epsilon\n\\tag{12.15}\n\\end{equation}\\]en omettant la constante \\(\\beta_0\\), qui ne joue aucun rôle dans la variance de la variable dépendante et en ajoutant l’erreur, une variable indépendante particulière qui est supposée ne pas être liée à \\(x\\), avoir une moyenne de \\(0\\), un écart-type \\(\\sigma_\\epsilon\\). L’équation (12.15) est la forme générale du modèle linéaire de l’équation (12.1) et (12.9) dans laquelle la variance de \\(y\\) est une fonction des variances-covariances des \\(x\\) pondérées par \\(\\beta\\).","code":""},{"path":"créer.html","id":"calcul-de-la-variance-de-lerreur","chapter":" 12 Créer","heading":"12.2.1 Calcul de la variance de l’erreur","text":"Le modèle linéaire le plus simple, soit à deux variables est représentée par l’équation suivante\\[\\begin{equation}\ny=x+\\epsilon\n\\tag{12.16}\n\\end{equation}\\]où \\(\\epsilon\\) est le terme d’erreur résiduel. C’est le minimum pour construire un modèle bivarié. Il s’agit de la même somme bivariée que l’équation (12.1), mais où \\(x_2\\) est remplacé \\(\\epsilon\\) et défini comme indépendant de \\(x_1\\) (non corrélé). Sur la base de ce modèle, l’équation (12.2) conduit à l’équation suivante.\\[\n\\sigma_y^2=\\sigma_x^2+\\sigma_\\epsilon^2\n\\]\n\nDans la plupart des cas de modélisation de données, les variances \\(\\sigma_x^2\\) et \\(\\sigma_y^2\\) sont généralement spécifiées à l’avance plutôt que \\(\\sigma_\\epsilon^2\\). Le calcul priori de la variance de l’erreur est plus pertinent que d’ajuster les paramètres d’intérêt. En réarrangeant l’équation (12.16) pour isoler \\(\\epsilon\\),\\[\n\\epsilon=y-x\n\\]\n\net en termes de variance\\[\n\\sigma_\\epsilon^2=\\sigma_y^2+\\sigma_x^2-2\\sigma_{x,y}\n\\]\n\noù l’équation (12.14) pour la forme en algèbre matricielle.\\[\n\\sigma_y^2 =\n\\left(\\begin{array}{cc}\n1 & -1\n\\end{array}\\right)\n\\left( \\begin{array}{cc}\n\\sigma^2_{y} & \\sigma_{x,y} \\\\\n\\sigma_{x,y} & \\sigma^2_{x}\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right) =\n\\sigma^2_y + \\sigma^2_x - 2\\sigma_{x,y}\n\\]\n\n","code":""},{"path":"créer.html","id":"variance-des-erreurs-avec-beta","chapter":" 12 Créer","heading":"12.2.2 Variance des erreurs avec \\(\\beta\\)","text":"Une préoccupation lors de la modélisation des données est de préserver les propriétés souhaitées du modèle dans les jeux de données comme la variance de l’erreur, les paramètres de régression, les covariances, etc. Après la variance du terme d’erreur, le dernier élément à considérer est les coefficients de régression, \\(\\beta\\), autrement dit, le degré de la relation entre les variables indépendantes et dépendantes. À partir de l’équation (12.16), ajoute la pente et l’erreur :\\[\ny=\\beta x+\\epsilon\n\\]\n\nLa loi de la somme des variances pour ce modèle devient\\[\\begin{equation}\n\\sigma_y^2=\\beta^2 \\sigma_x^2+\\sigma_\\epsilon^2\n\\tag{12.17}\n\\end{equation}\\]Pour rappel, l’erreur n’est pas corrélée à la variable indépendante \\(x_i\\). Comme précédemment, la forme d’algèbre matricielle est plus simple et plus élégante :\\[\\begin{equation}\n\\sigma_y^2 = \\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B} + \\sigma_\\epsilon^2\n\\tag{12.18}\n\\end{equation}\\]où \\(\\beta\\) est un vecteur contenant tous les coefficients de régression \\(\\beta\\) prédisant \\(y\\) et s’applique à un nombre quelconque de prédicteurs \\(x\\). En profitant de l’indépendance du terme d’erreur, l’équation (12.18) est réarrangée comme :\\[\\begin{equation}\n\\sigma_\\epsilon^2=\\sigma_y^2 -\\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B}\n\\tag{12.19}\n\\end{equation}\\]ce qui donne la variance du terme d’erreur. Sous une forme linéaire (pour une seule variable indépendante), il est possible de réarranger l’équation (12.17) pour isoler \\(\\sigma_\\epsilon\\).\\[\n\\sigma_\\epsilon^2=\\sigma_y^2-\\beta^2 \\sigma_x^2\n\\]\n\n","code":""},{"path":"créer.html","id":"le-scénario-standardisé","chapter":" 12 Créer","heading":"12.2.3 Le scénario standardisé","text":"Pour conclure cette section, le scénario standardisé, c’est-à-dire lorsque les variances des variables sont égales à \\(1\\). Plus précisément, la variance de chaque variable est fixe ; seules les erreurs résiduelles doivent être ajustées pour maintenir ces propriétés. Dans ce scénario, la matrice de variance-covariance, \\(\\Sigma\\), est une matrice de corrélation, qui définit les variances des erreurs résiduelles. Pour calculer la variance des résidus, l’équation (12.19) est utilisée en remplaçant la variance des variables par \\(1\\) comme suit\\[\\begin{equation}\n\\sigma_\\epsilon^2=1 - \\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B}\n\\tag{12.20}\n\\end{equation}\\]ou, pour un seul prédicteur.\\[\\begin{equation}\n\\sigma_\\epsilon^2=1-\\beta^2 \\sigma_x^2\n\\tag{12.21}\n\\end{equation}\\]Comme mentionner pour l’équation (12.12), ces deux dernières équations rappellent le coefficient de détermination. La valeur \\(1\\) correspond au potentiel explicatif d’une variable, \\(\\sigma_\\epsilon^2\\) est la variance résiduelle et, par conséquent, \\(1-\\sigma_\\epsilon^2\\) est la variance expliquée par le modèle.","code":""},{"path":"créer.html","id":"le-scénario-non-standardisé","chapter":" 12 Créer","heading":"12.2.4 Le scénario non standardisé","text":"Dans la pratique, les scénarios sont rarement standardisés. Les variables n’ont pas toutes des variances de \\(1\\) et des moyennes de \\(0\\). Pour ajouter une touche de naturelle aux jeux de données, il est possible, une fois que le système d’équations est complètement obtenu, d’ajouter des variances différentes en multipliant une variable par l’écart-type souhaité et d’additionner une moyenne. Cela modifiera la matrice de covariance et les coefficients de régression, mais la force relative des liens et la matrice de corrélation resteront identiques. En d’autres termes, la déstandardisation est l’inverse d’un score \\(z\\). Les données créées jusqu’ici sont des scores \\(z\\) et pour les déstandardisés, il faut procéder ainsi \\(x = s(z + \\bar{x})\\), où \\(s\\) est l’écart-type et \\(\\bar{x}\\) est la moyenne de la variable déstandardisée.","code":""},{"path":"créer.html","id":"création-de-variables-en-série","chapter":" 12 Créer","heading":"12.3 Création de variables en série","text":"Jusqu’à présent, seule la création d’une variable est présentée. Le défi pour créer un système d’équations est d’obtenir la matrice de covariance des variables précédentes pour chaque variable subséquente.Puisque le sujet peut devenir compliqué rapidement, cette section est basée sur un exemple à trois variables avec un scénario standardisé (variables centrées et réduites). Le cas général sera développé par la suite.","code":""},{"path":"créer.html","id":"cas-spécifique","chapter":" 12 Créer","heading":"12.3.1 Cas spécifique","text":"Le défi commence lorsqu’il y trois ou plus variables à générer. Le cas à trois variables est tout de même abordable. La causalité est unidirectionnelle, c’est-à-dire d’une variable vers une autre, sans retour en arrière. Cela se nomme un modèle récursif, impliquant du même coup l’existence de modèles non-récursifs. Les modèles non-récursifs ont la caractéristique d’avoir une ou des boucles de causalité bidirectionnelle, comme \\(x \\leftrightarrow y\\), alors que les modèles récursif n’ont que des causalité directionnel comme \\(x \\rightarrow y\\). Les modèles non-récursifs dépassent la portée de cet chapitre.Pour un modèle à trois variables, une seule configuration récursive complète est possible. Elle est présentée à la Figure 12.2.\nFigure 12.2: Modèle à trois variables\nLa Figure 12.2 montre un modèle à trois variables. La première variable \\(x_1\\) prédit \\(x_2\\) et \\(x_3\\), en plus \\(x_2\\) prédit \\(x_3\\). Ce modèle peut aussi se représenter en matrice \\(\\mathbf{B}\\) dans laquelle se retrouvent les coefficients de régression qui relient les variables. Dans ce cas-ci, \\(\\beta_{3,1}\\) signifie que la variable 1 prédit la 3 à un degré \\(\\beta_{3,1}\\); \\(\\beta_{3,2}\\) signifie que la variable 2 prédit la 3 à un degré \\(\\beta_{3,2}\\). Le premier indice correspond à l’effet (variable dépendante) et le deuxième à la cause (variable indépendante).\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{ccc}\n0 & 0 & 0 \\\\\n\\beta_{2,1} & 0 & 0 \\\\\n\\beta_{3,1} & \\beta_{3,2} & 0 \\\\\n\\end{array}\n\\right)\n\\tag{12.22}\n\\end{equation}\\]Afin de bâtir un exemple complet avec R, la syntaxe ci-dessous montre les paramètres arbitraires pour la création de données à parti du modèle de la figure 12.2.La première variable \\(x_1\\) est exogène, c’est-à-dire qu’elle n’est prédite par aucune autre variable, et n’obtient aucune information d’aucune autre variable. Cela se perçoit notamment dans la matrice \\(\\mathbf{B}\\) avec la première ligne ne contenant que des 0. Pour créer x1, il suffit de connaître sa variance (1 dans un scénario standardisé). Il faudra en supplément un vecteur de variances (ici, toutes fixées à 1 par le scénario standardisé).À l’aide de la loi de la somme des variances, la création de \\(x_2\\) est assez simple puisqu’il n’y qu’un seul prédicteur.\\[\nx_2 = \\beta_{2,1}x_1+\\epsilon_{x_2}\n\\]En suivant l’équation (12.21), il est possible de calculer la variance résiduelle, toujours en assumant que \\(\\sigma^2_{x_1} = \\sigma^2_{x_2}=1\\).\\[\n\\sigma^2_{\\epsilon_{x_2}} = 1-\\beta_{2,1}^2\\sigma^2_{x_1}=1-\\beta_{21}^2\n\\]En code R, il est possible de procéder ainsi.Maintenant, il reste à créer la variable \\(x_3\\). Celle-ci est générée à partir de \\(x_1\\) et \\(x_2\\) selon l’équation suivante.\\[\nx_3 = \\beta_{3,1}x_1 + \\beta_{3,2}x_2 + \\epsilon_{x_3}\n\\]La variance résiduelle suit l’équation (12.20). En équation linéaire, elle s’écrit comme suit.\\[\n\\begin{aligned}\n\\sigma^2{\\epsilon_{x_3}} &  =\\sigma^2_{x_3} -( \\beta_{3,1}^2\\sigma^2_{x_1} + \\beta_{3,2}^2\\sigma^2_{x_2}  +  2\\beta_{3,1}\\beta_{3,2}\\sigma_{x_1,x_2})\n& = 1-(\\beta_{3,1}^2+\\beta_{3,2}^2  +  2\\beta_{3,1}\\beta_{3,2}\\sigma_{x_1,x_2})\n\\end{aligned}\n\\]Elle occasionne toutefois un défi, car la covariance entre \\(x_1\\) et \\(x_2\\) n’est pas explicitement connue. Dans le cas d’une variable prédite exclusivement par une autre variable qui elle est exogène (sans prédicteur), leur covariance est égale au coefficient de régression, soit \\(\\beta_{21}\\). Ce cas ne survient que dans cette situation précise. Il sera impératif de dégager une solution générale pour des modèles ayant plus de trois variables, la complexité du calcul de la covariance augmentant avec la croissance de \\(p\\). En R, \\(x_3\\) se génère ainsi.Il est possible de vérifier les caractéristiques des trois modèles.Comme il s’agit d’un scénario standardisé, la matrice de corrélation est similaire à la matrice de covariance. Notamment, les variances sont près de \\(1\\). Aussi, les résultats confirment que la covariance entre \\(x_1\\) et \\(x_2\\) est bel et bien le coefficient de régression, mais surtout, comme il était mentionné, qu’il s’agit du seul cas où cela est vrai. Les régressions effectuées par lm() retrouvent également les \\(\\beta\\) choisis pour l’exemple.","code":"\n# Pour la reproductibilité\nset.seed(1448)            \nn <- 100000\n\n# Paramètres arbitraires\nbeta21 <- .2 \nbeta31 <- .4\nbeta32 <- -.5\n# Création de la première variable\nx1 <- rnorm(n = n, sd = 1) # Variance de 1\n# Variance résiduelle\ne_x2 <- 1 - beta21^2\n\n# Création de la variable\nx2 = beta21 * x1 + rnorm(n = n,  sd = sqrt(e_x2)) # Variance de 1\n# Variance résiduelle\ne_x3 <- 1 - (beta31^2 + beta32^2 + \n               2 * beta31 * beta32 * beta21)\n\n# Création de la variable\nx3 = beta31 * x1 + beta32 * \n  x2 + rnorm(n = n, sd = sqrt(e_x3))# Création du jeu de données\nX <- data.frame(x1 = x1,\n                x2 = x2,\n                x3 = x3)\n\n# Comme il s'agit d'un scénario standardisé, la matrice de\n# corrélation est similaire à la matrice de covariance.\ncov(X)\n>       x1     x2     x3\n> x1 1.002  0.202  0.303\n> x2 0.202  1.004 -0.418\n> x3 0.303 -0.418  0.997\ncor(X)\n>       x1     x2     x3\n> x1 1.000  0.201  0.303\n> x2 0.201  1.000 -0.417\n> x3 0.303 -0.417  1.000\n\n# Retrouver beta21\nlm(x2 ~ x1, data = X)\n> \n> Call:\n> lm(formula = x2 ~ x1, data = X)\n> \n> Coefficients:\n> (Intercept)           x1  \n>     0.00148      0.20113\n\n# Retrouver beta31 et beta32\nlm(x3 ~ x1 + x2, data = X)\n> \n> Call:\n> lm(formula = x3 ~ x1 + x2, data = X)\n> \n> Coefficients:\n> (Intercept)           x1           x2  \n>     0.00578      0.40254     -0.49663"},{"path":"créer.html","id":"cas-général","chapter":" 12 Créer","heading":"12.3.2 Cas général","text":"Un cas général permet d’obtenir la matrice de covariance \\(\\mathbf{\\Sigma}\\) à partir de la matrice \\(\\mathbf{B}\\) et d’un vecteur de variance \\(\\text{diag}(\\mathbf{\\Sigma})\\). Par la suite, il est possible de créer des variables en série comme la section précédente, ou bien de revenir à ce qui se faisait dans les chapitres précédents, c’est-à-dire d’utiliser MASS:mvrnorm() pour générer des données.Pour obtenir la matrice de covariance, il est nécessaire d’avoir une matrice de coefficients de régression\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{cccc}\n0 & 0 & 0 & 0\\\\\n\\beta_{2,1} & 0 & 0 & 0\\\\\n...& ...  & 0 & 0 \\\\\n\\beta_{p,1} & ... & \\beta_{p,p-1} & 0\n\\end{array}\n\\right)\n\\tag{12.23}\n\\end{equation}\\]et d’un vecteur de variances\\[\\begin{equation}\n\\text{diag}(\\mathbf{\\Sigma}) = \\left(\\sigma^2_{x_1},...,\\sigma^2_{x_p} \\right)\n\\end{equation}\\]Ces matrices sont construites de façon générale. La matrice \\(\\mathbf{B}\\) est de dimension \\(p \\times p\\) avec des valeurs nulles comme triangle supérieur incluant la diagonale et toutes celles qui se trouvent au-dessus. Les coefficients de régresison se trouvent dans le triangle extérieur. Le vecteur de variance contient \\(p\\) valeurs qui représentent les variances de variables. Elles sont toutes à l’unité quand le scénario est standardisé.L’idée sous-jacente est qu’il est possible de calculer les covariances de la variable \\(\\) à partir de la matrice de covariance des variables précédente \\(1:(-1)\\) soit \\(\\mathbf{\\Sigma}_{1:(-1),1:(-1)}\\) et des coefficients de régression associés \\(\\mathbf{B}_{, 1:(-1)}\\) en procédant en série pour toutes variables de \\(x_2\\) (\\(=2\\)) (prédite par au moins une variable, toutes sauf la première) jusqu’à la dernière variable \\(x_p\\) (\\(=p\\)). Le calcul est décrit à l’équation (12.24), malheureusement, elle recourt à l’algèbre matricielle, mais demeure toujours beaucoup plus simple et générale que si elle était présentée en algèbre linéaire.\\[\\begin{equation}\n\\begin{aligned}\n\\mathbf{\\Sigma}_{,1:(-1)} = \\mathbf{B}_{+1,1:}\\mathbf{\\Sigma}_{1:,1:}\\\\\n\\mathbf{\\Sigma}_{1:(-1),} = \\mathbf{B}_{+1,1:}\\mathbf{\\Sigma}_{1:,1:}\n\\end{aligned}\n\\tag{12.24}\n\\end{equation}\\]Il faut à chaque étape s’assurer de faire le remplacement des valeurs obtenues sur chaque côté de la diagonale, ce pourquoi deux équations sont reproduites à l’équaiton (12.24) avec des indices différents pour \\(\\mathbf{\\Sigma}\\).Voici pour l’exemple précédent les étapes de calcul de l’équation (12.24) décrites une à une. D’abord, il faut construire une matrice, \\(\\mathbf{\\Sigma}\\) (S en code R) avec comme diagonale les variances. Puis, calculer l’équation (12.24). Ensuite, il faut remplacer le résultat obtenu des deux côtés de la diagonale de façon à ce que S demeure symétrique. Ces étapes sont répétées pour \\(=2,...,p\\) (dans cet exemple, \\(p=3\\)).Avant de procéder, une courte digression sur une façon de réaliser en moins de lignes, mais avec une syntaxe plus complexe, la réassignation des valeurs dans S. Le code est présenté dans la syntaxe ci-dessous. Il appert qu’il n’est pas aussi intéressant à programmer pour sauver deux lignes.Trêve de digression, une fois la matrice de covariance S calculée, la fonction MASS::mvrnorm() peut être utilisée pour créer des données. Les résultats sont presque identiques pour les régressions.Comme la formule est générale et qu’elle implique plusieurs itérations, il est envisageable de programmer ces calculs avec une boucle dans une fonction maison. La fonction maison beta2cov() permet, à partir d’une matrice de coefficient de régression et d’un vecteur de variance, d’obenir la matrice de covariance.Évidemment, comme il est possible de passer d’une matrice de coefficient de régression à une matrice de covariance, l’inverse est envisageable. L’équation (12.24) est réarrangée pour isoler \\(\\mathbf{B}\\) de l’équation, ce qui donne l’équation (12.25).\\[\\begin{equation}\n\\mathbf{B}_{+1, 1:} = \\mathbf{\\Sigma}^{-1}_{1:,1:}\\mathbf{\\Sigma}_{1+,1:}\n\\tag{12.25}\n\\end{equation}\\]L’équation (12.25) se transforme (relativement) aisément en fonction maison. Par rapport à beta2cov(), la boucle n’inclut pas la \\(p\\)e variable, mais bien la première.La fonction maison cov2beta() est maintenant testée sur S pour évaluer si elle retourne bien la matrice B originale.Ce qui est le cas.Une seule contrainte s’impose lors de la du calcul de la covariance à partir des coefficients de régression. Il s’agit de s’assurer que la matrice de covariance demeure positive semi-définie à chaque étape. Cela se manifeste notamment lorsque les coefficients de régression pour une variable dépendante sont si élevés que le coefficient de détermination surpasse la variance de la variable en question. Autrement dit, la variance de la variable n’est pas assez élevée pour le potentiel explicatif. Mathématiquement le problème est que \\(1-R^2_X < 0\\) ou plus généralement que \\(\\sigma^2_y-\\mathbf{B^\\prime\\Sigma B}<0\\). Dans ces cas, la variance résiduelle négative, ce qui est impossible. L’une des corrections apporté sont les suivantes, réduire les coefficients de régression \\(\\mathbf{B}\\) pour cette variable ou encore d’augmenter sa variance \\(\\sigma^2_y\\) de sorte que \\(\\sigma^2_y-\\mathbf{B^\\prime\\Sigma B}>=0\\) soit toujours vrai à chaque étape.Une dernière note, comme ces équations et syntaxes procèdent de \\(=2,...,p\\), l’ordre des variables est primordiale et altérer leur ordre des conséquences substantielles sur les résultats. Lorsque la matrice \\(\\mathbf{B}\\) est créées, il faut être sûr de l’ordre déterministe des variables, c’est-à-dire, quelle variable cause quelles autres variables, comme illustre à la Figure 12.2 par exemple. Changer ou retirer ne serait-ce qu’une variable change les coefficients de régression : les régressions ne sont plus identiques, la matrice de coefficients de régression ne sera pas retrouvée. Il ne faut pas être surpris donc, si les résultats changent dans cette situation. Nonobstant, changer ou retirer une variable peut être pertinent dans certains contextes, surtout pour l’étude de la misspécification (en anglais) des modèles, c’est-à-dire lorsqu’un modèle erroné est utilisé plutôt que le vrai modèle, ce qui entraîne notamment des biais. Les études à ce sujet emprunteront une méthode statistique similaire.","code":"# Matrice B\n# Rappel : beta21 <- .2; beta31 <- .4; beta32 <- -.5\nB <- matrix(c(  0,     0,    0,\n             beta21,   0,    0,\n             beta31, beta32, 0), \n           ncol  = 3, nrow = 3, byrow = TRUE)\n\n# Vecteur de variances\nV = c(1, 1, 1)\n\n# Montrer la matrice B\nB\n>      [,1] [,2] [,3]\n> [1,]  0.0  0.0    0\n> [2,]  0.2  0.0    0\n> [3,]  0.4 -0.5    0# Créer une matrice de covariance préliminaire\nS <- diag(V)\n\n# Aucune covariance n'est inscrite dans S\nS\n>      [,1] [,2] [,3]\n> [1,]    1    0    0\n> [2,]    0    1    0\n> [3,]    0    0    1\n\n# Première étape\ni <- 2\n\n# Calcul de la covariance entre x1 et x2\nB[i, 1:(i-1)] %*% S[1:(i-1),1:(i-1)]\n>      [,1]\n> [1,]  0.2\n\n# Calcul de la covariance entre x1 et x2 assignée à COV\nCOV <- B[i, 1:(i-1)] %*% S[1:(i-1),1:(i-1)]\nS[i, 1:(i-1)] <- COV\n\n# Il faut remplacer ce résultat de chaque côté de la diagonale\nS[1:(i-1),i] <- COV\n\n# La matrice est mise à jour pour i = 2\nS\n>      [,1] [,2] [,3]\n> [1,]  1.0  0.2    0\n> [2,]  0.2  1.0    0\n> [3,]  0.0  0.0    1\n\n# Pour la seconde étape, l'équation est reprise pour i = 3\ni <- 3\n\n# Calcul de la covariance entre x1 et x2 assignée à COV\nCOV <- B[i, 1:(i-1)] %*% S[1:(i-1), 1:(i-1)]\nS[i, 1:(i-1)] = COV\nS[1:(i-1), i] = COV\n\n# La matrice est mise à jour pour i = 3\nS\n>      [,1]  [,2]  [,3]\n> [1,]  1.0  0.20  0.30\n> [2,]  0.2  1.00 -0.42\n> [3,]  0.3 -0.42  1.00\n\n# Elle est approximativement identique \n# aux données de l'exemple précédent\ncov(X)\n>       x1     x2     x3\n> x1 1.002  0.202  0.303\n> x2 0.202  1.004 -0.418\n> x3 0.303 -0.418  0.997\n# Une note pour indiquer que les valeurs à remplacer pourrait\n# être fait en une seule ligne de syntaxe en bénéficiant du \n# recyclage vectoriel de R, mais la solution n'est ni simple, \n# ni élégante.\nremplacer = cbind(c(rep(i, i-1), 1:(i-1)), c(1:(i-1), rep(i, i-1)))\nS[remplacer]# La même que l'exemple précédent\nset.seed(1448) \n\n# Création de données\nX <- MASS::mvrnorm(n = n, mu = c(0, 0, 0), Sigma = S)\n\n# Configurer en tableau de données (data.frame)\nX <- as.data.frame(X)\ncolnames(X) = c(\"x1\",\"x2\",\"x3\")\n\n# Retrouver beta21\nlm(x2 ~ x1, data = X)\n> \n> Call:\n> lm(formula = x2 ~ x1, data = X)\n> \n> Coefficients:\n> (Intercept)           x1  \n>    -0.00219      0.20191\n\n# Retrouver beta31 et beta32\nlm(x3 ~ x1 + x2, data = X)\n> \n> Call:\n> lm(formula = x3 ~ x1 + x2, data = X)\n> \n> Coefficients:\n> (Intercept)           x1           x2  \n>    -0.00579      0.40295     -0.49899\n# De Beta vers covariance (beta 2 covariance)\nbeta2cov <- function(B, V = NULL){\n  \n  p <- dim(B)[1]    # Nombre de variables\n  \n  if(is.null(V)){   # Si V est nulle, alors V est une\n    S <- diag(p)    # matrice diagonale d'identité,\n  }else{            # autrement il s'agit d'une matrice\n    S <-  diag(V)   # avec les variances en diagonale\n  }  \n  \n  # Boucle de calcul pour la covariance\n  # de la variable i (i = 2:p)\n  for(i in 2:p){\n    \n    COV = B[(i), (1:(i-1))] %*% S[1:(i-1), 1:(i-1)]\n    S[i, 1:(i-1)] = COV\n    S[1:(i-1), i] = COV\n    \n  }\n  \n  return(S)\n  \n}\n# De la covariance à Beta (cov2beta)\ncov2beta <-  function(S){\n  \n  p <- ncol(S)            # Nombre de variable\n  BETA <- matrix(0, p, p) # Matrice vide\n  \n  # Boucle de calcul pour la covariance \n  # de la variable i (i = 1:(p-1))\n  for(i in 1:(p-1)){\n    \n    BETA[i+1, 1:i] <- solve(S[1:i, 1:i], S[1+i, 1:i])\n    \n  }\n  \n  return(BETA)\n  \n}cov2beta(S)\n>      [,1] [,2] [,3]\n> [1,]  0.0  0.0    0\n> [2,]  0.2  0.0    0\n> [3,]  0.4 -0.5    0"},{"path":"médier.html","id":"médier","chapter":" 13 Médier","heading":" 13 Médier","text":"L’analyse de médiation est une analyse statistique de plus en plus populaire auprès des expérimentateurs, peu importe leur discipline. Cette analyse cherche à expliquer les mécanismes biologiques, psychologiques, cognitifs, etc., qui sous-tendent la relation entre une variable indépendante et une variable dépendante par l’inclusion d’une troisième variable, c’est-à-dire la variable médiatrice. L’intérêt pour cette technique est patent, puisqu’elle quantifie le degré selon lequel une variable participe à la transmission du changement d’une cause vers son effet. L’analyse de médiation peut contribuer à mieux comprendre la relation entre une variable indépendante et une variable dépendante lorsque ces variables n’ont pas de lien direct évident.L’analyse de médiation est un sous-ensemble de l’analyse de trajectoire dans lequel le statisticien s’intéresse à la relation entre la variable indépendante \\(x\\) sur la variable dépendante \\(y\\) par l’intermédiaire de la variable médiatrice \\(m\\). Elle s’inscrit dans un système d’équations. L’analyse de médiation se base sur les liens indirects qui existent dans ce système d’équations. Ces liens indirects sont ces relations intermédiaires qui intéressent le statisticien.Concevoir ce qu’est un lien indirect est parfois plus aisé en le comparant aux liens directs. Un lien direct, c’est la relation entre une variable indépendante et une dépendante, comme le coefficient de régression, par exemple. Le lien indirect, c’est la relation qui existe entre une variable indépendante et une dépendante à travers une ou plusieurs autres variables.Pour les fins de ce chapitre, l’analyse à un seul médiateur sera présentée (aussi nommé l’analyse de médiation simple).","code":""},{"path":"médier.html","id":"analyse-de-médiation-simple","chapter":" 13 Médier","heading":"13.1 Analyse de médiation simple","text":"Le diagramme de trajectoire correspondant au modèle de médiation simple14 (un seul médiateur) est présenté dans le panneau supérieur de la Figure 13.1.\nFigure 13.1: Modèle de médiation simple\nLe cadran supérieur devrait être familier aux lecteurs, car il été abordé dans le chapitre Créer, dans une orientation légèrement différente. La Figure 13.1 peut également être représentée avec une matrice de coefficients de régression \\(\\mathbf{B}\\) dans laquelle se retrouvent les coefficients de régression qui relient les variables.\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{ccc}\n0 & 0 & 0 \\\\\n\\beta_{2,1} & 0 & 0 \\\\\n\\beta_{3,1} & \\beta_{3,2} & 0 \\\\\n\\end{array}\n\\right)\n\\tag{12.22}\n\\end{equation}\\]Dans ce cas-ci, \\(\\beta_{2,1}\\) signifie que la variable \\(x\\) prédit le médiateur \\(m\\), \\(\\beta_{3,2}\\) signifie que le médiateur prédit la variable \\(y\\) et \\(\\beta_{3,1}\\) signifie que la variable \\(x\\) prédit la variable \\(y\\). En analyse de médiation, ces effets sont nommés des effets directs, comme l’effet direct de \\(x\\) sur \\(y\\), ou l’effet direct de \\(m\\) sur \\(y\\). Pour identifier un effet médiateur, le statisticien cherche l’effet indirect de \\(x\\) sur \\(y\\), c’est-à-dire l’effet de \\(x\\) passant par \\(m\\). L’effet indirect est \\(\\beta_{2,1}\\times\\beta_{3,2}\\). Ce résultat est dérivé notamment des travaux de Wright (1934) sur la méthode de coefficients de trajectoires (path coefficients) qui est un moyen flexible de relier les coefficients de régression entre les variables d’un système d’équations.Lorsqu’il n’y pas de \\(m\\), la relation existante entre \\(x\\) et \\(y\\) est nommé l’effet total représenté par \\(\\sigma_{x,y}\\) et est illustré dans le cadran inférieur de la Figure 13.1. Ce lien correspond au coefficient de régression entre \\(x\\) et \\(y\\). Dans ce cas spécifique à deux variables, il s’agit également de la covariance entre \\(x\\) et \\(y\\). L’effet total peut être séparé en deux autres effets dont il fait la somme : l’effet direct de \\(x\\) sur \\(y\\) (\\(\\beta_{3,1}\\)) et l’effet indirect\\(\\beta_{2,1}\\beta_{3,2}\\).\\[\\begin{equation}\n\\sigma_{3,1} = \\beta_{3,1} + \\beta_{2,1} \\beta_{3,2}\n\\tag{13.1}\n\\end{equation}\\]Un avantage de l’équation (13.1) est qu’elle met en relation l’interdépendance des quatre composantes pour dériver un effet total, direct ou indirect. Si trois des mesures sont connues, la quatrième est dérivable avec un peu de réaménagement algébrique. Un autre avantage sur la plan statistique est également de montrer le lien intime qui existe entre la matrice de covariance et les coefficients de régression.","code":""},{"path":"médier.html","id":"tester-leffet-indirect","chapter":" 13 Médier","heading":"13.2 Tester l’effet indirect","text":"Qu’en est-il du test d’hypothèse de l’effet indirect? Il existe trois méthodes principales pour tester si l’effet indirect est significatif. Il s’agit de la méthode d’étape causale (causal step method), la plus populaire étant celle de Baron et Kenny (Baron & Kenny, 1986), la méthode delta multivarié (multivariate delta method) (Rao, 2002) duquel le test de Sobel (Sobel, 1982) est un cas particulier et (c) les méthodes de bootstrap (Efron & Tibshriani, 1979). Chacune de ces méthodes sera détaillée ci-dessous.","code":""},{"path":"médier.html","id":"la-méthode-détape-causale","chapter":" 13 Médier","heading":"13.2.1 La méthode d’étape causale","text":"La méthode d’étape causale aussi connue sous le nom de test de Baron-Kenny est un test séquentiel d’hypothèse afin de vérifier l’existence du lien indirect. Ce test est présenté à des fins historiques uniquement et parce que certains chercheurs l’exigent et l’utilisent encore. Par contre, dans la littérature méthodologique, il n’est plus recommandé, étant rejeté en faveur d’autres méthodes plus adéquates tant sur le plan statistique que conceptuel. La méthode provenant des années 80 lorsque les ordinateurs personnels n’étaient pas encore dans toutes les chaumières certainement du mérite pour l’époque, mais n’est plus nécessaire aujourd’hui. En plus, c’est un bon exercice d’extraction de résultats avec R.Pour réaliser le test en bonne et due forme, trois tests d’hypothèse sont réalisés en séries.Existe-t-il une relation entre \\(x\\) et \\(y\\) pour l’effet total? Autrement dit, avant de procéder à l’analyse du lien indirect, y -t-il un lien entre les variables indépendantes et dépendantes?Existe-t-il une relation entre \\(x\\) et \\(y\\) pour l’effet total? Autrement dit, avant de procéder à l’analyse du lien indirect, y -t-il un lien entre les variables indépendantes et dépendantes?Existe-t-il une relation directe entre \\(x\\) et \\(m\\)? Y -t-il un lien entre le variable indépendante et le médiateur?Existe-t-il une relation directe entre \\(x\\) et \\(m\\)? Y -t-il un lien entre le variable indépendante et le médiateur?Existe-t-il une relation directe entre \\(m\\) et \\(y\\)? Y -t-il un lien entre le médiateur et la variable dépendante?Existe-t-il une relation directe entre \\(m\\) et \\(y\\)? Y -t-il un lien entre le médiateur et la variable dépendante?Les étapes 2 et 3 visent à vérifier si le médiateur bel et bien un rôle à jouer entre la variable indépendante et dépendante. Le rejet de l’une ou l’autre de ces trois hypothèses mènerait certainement à un statisticien à douter d’une relation entre les variables. Comment pourrait-il y avoir un lien indirect, si l’un de ces liens n’était pas soutenu par les données.En guise de quatrième test, souvent les chercheurs suivant cette tendance d’analyse testent si la médiation est complète ou partielle. La médiation complète signifie que l’entièreté du lien total entre \\(x\\) et \\(y\\) est maintenant attribuable à l’ajout de \\(m\\). Ce résultat s’observe lorsque le lien direct entre \\(x\\) et \\(y\\) (lorsque \\(m\\) est inclus pour prédire \\(y\\)) n’est pas significatif contrairement à la première étape où le lien total, lui, était significatif. Si le lien direct entre \\(x\\) et \\(y\\) était toujours significatif même après avoir ajouté le \\(m\\) dans la prédiction de \\(y\\), alors la médiation est dite partielle.La syntaxe montre comment la méthode d’étape causale pourrait être programmée dans R. Il y trois régressions (lm()), une pour chaque test d’hypothèse et quatre étapes sous forme de conditionnel (les trois hypothèses plus le type de médiation).Si une des conditions () n’est pas respectée, le test retourne l’hypothèse nulle. À chacune des étapes, la valeur-\\(p\\) du coefficient testé est extraite et comparée à l’erreur de type fixée à l’avance (\\(\\alpha\\)). Le test doit être significatif pour procéder à l’étape suivante. À la toute fin, l’hypothèse nulle est rejetée et le type de médiation (complète ou partielle) est rapporté.Pour chaque régression, il faut extraire la valeur-\\(p\\) de l’estimateur concerné. La valeur=\\(p\\) se trouve dans le sommaire (summary()) du résultat de la régression (etape) dans la liste coefficients. Dans cette liste, il faut identifier la ligne (\"estimateur\") à la colonne \"Pr(>|t|)\" qui correspond aux valeurs-\\(p\\). Au final, l’extraction se commande summary(etape)$coefficients[\"estimateur\", \"Pr(>|t|)\"]. Pour bien fonctionner, les variables du jeu de données doivent se nommer x, m et y.Plusieurs raisons suggèrent de ne pas utiliser la méthode d’étape causale. D’abord, comme une série de tests d’hypothèse est réalisée, l’erreur de type est différente de celle fixée. Il y trois hypothèses nulles à rejeter, chacune ayant un seuil \\(\\alpha\\). La vraie erreur de type est égale la probabilité de rejeter toutes ces hypothèses nulles accidentellement. Cela correspond à \\(\\alpha^3\\). Avec \\(\\alpha=.05\\), cela signifie que le taux est de \\(\\alpha^3=.05^3= 1.25\\times 10^{-4}\\), ce qui est bien plus stricte que l’erreur de type fixée. Cela entraîne une perte de puissance, c’est-à-dire de trouver des effets indirects lorsqu’ils sont vrais.Une seconde raison est que l’absence d’effet total entre la variable indépendante et dépendante n’est pas une hypothèse obligatoire. Autrement dit, la première étape, tester si \\(x\\) est lié à \\(y\\) sans tenir de compte de \\(m\\), n’est pas recommander. Il peut exister des effets indirects théoriquement valides sans effets totaux. De plus, les deux autres hypothèses (étapes) ne sont pas obligatoire non plus (quoi qu’elles sont un peu plus dures à justifier) d’ailleurs. Il est tout à fait possible d’avoir des effets indirects significatifs dont les effets directs qui le compose sont non-significatifs15.","code":"\nBK <- function(donnees, alpha = .05){\n  # alpha est l'erreur de type I\n  \n  # Créer l'hypothèse nulle\n  decision = FALSE\n  interpretation = \"L'effet indirect est non significatif\"\n  \n  # Première régression\n  # Est-ce que l'effet total est significatif?\n  etape1 <- lm(formula = y ~ x, data = donnees)\n  pC <- summary(etape1)$coefficients[\"x\", \"Pr(>|t|)\"]\n  \n  if (pC <= alpha){\n    # Si le coefficient rho_31 est significatif, alors\n    # Deuxième régression\n    # Est-ce que l'effet direct de x vers m est significatif?\n    etape2 <- lm(formula = m ~ x, data = donnees)\n    pA <- summary(etape2)$coefficients[\"x\", \"Pr(>|t|)\"]\n    \n    if (pA <= alpha){\n      # Si le coefficient beta_21 est significatif, alors\n      # Troisième régression\n      # Est-ce que l'effet direct de m vers y est significatif?\n      etape3 <- lm(formula = y ~ x + m, data = donnees)\n      pB <- summary(etape3)$coefficients[\"m\", \"Pr(>|t|)\"]\n      \n      # Enregistrer le résultats, si l'effet de médiation a lieu\n      decision <- (pB <= alpha)\n      \n      if (decision){\n        # S'il y médiation, est-elle partielle ou complète?\n        # Est-ce que Beta_31 est significatif?\n        if (summary(etape3)$coefficients[\"x\", \"Pr(>|t|)\"] <= alpha){\n          # Si oui,\n          interpretation <- \"L'effet indirect est significatif \n          et la médiation est partielle\"\n        } else {\n          # Si non,\n          interpretation <- \"L'effet indirect est significatif \n          et la médiation est complète\"\n        }\n      }\n    }\n  }\n  cat(interpretation)\n  return(decision)\n  \n}"},{"path":"médier.html","id":"la-méthode-delta-multivarié","chapter":" 13 Médier","heading":"13.2.2 La méthode delta multivarié","text":"La méthode delta multivarié souvent appelée le test de Sobel (1982), auteur qui l’popularisé pour l’analyse des effets indirects est une méthode ayant recours à l’erreur type approximative de l’effet indirect.Le test de Sobel se base sur le calcul selon lequel le ratio de l’effet indirect, \\(\\beta_{2,1}\\beta{3,2}\\) par son erreur standard asymptotique \\(\\sqrt{\\beta_{3,2}^2\\sigma^2_{\\beta_{2,1}} +\\beta^2_{2,1}\\sigma^2_{\\beta_{3,2}}}\\) se distribue selon une distribution gaussienne. L’équation (13.2) représente ce calcule\\[\\begin{equation}\nz=\\frac{\\beta_{2,1}\\beta{3,2}}{\\sqrt{\\beta_{3,2}^2\\sigma^2_{\\beta_{2,1}} +\\beta^2_{2,1}\\sigma^2_{\\beta_{3,2}}}}\n\\tag{13.2}\n\\end{equation}\\]où \\(z\\) signifie qu’il s’agit d’un score-\\(z\\), les \\(\\beta\\) sont les coefficients de régression et les \\(\\sigma^2_\\beta\\) sont les erreurs standard des coefficients de régression.La syntaxe suivante illustre une fonction qui calcule l’équation (13.2). La syntaxe calcule deux régressions m ~ x et m ~ x. Du sommaire des résultats, elle extrait, les deux coefficients de régressions et leur erreur standard respective. Elle calcule enfin le score \\(z\\) de l’équation (13.2) avec la valeur-\\(p\\) associée. Pour bien fonctionner avec cette fonction, les variables du jeu de données doivent se nommer x, m et y.La raison selon laquelle il ne faut pas recourir au test de Sobel est que le calcul est asymptotique. En fait, la distribution de l’effet indirect tend vers la normalité lorsque la taille d’échantillon est grande. Cela poussé les chercheurs à développer des corrections pour ce test afin de l’amélioré. La distribution des effets indirects peut facilement se programmer avec R. En se basant sur le chapitre Simuler, il est possible de construire une petite illustration. D’abord, il faut créer une fonction qui crée des jeux de données, ici, gen.ind.effect() qui prend une matrice de covariance Sigma et une taille d’échantillon n. La fonction extrait ensuite les deux coefficients de régression et les multiplie. Noter la fonction coef() qui permet d’extraire plus simplement les coefficients de régression d’une sortie de lm() et unname() qui dénomme le résultat (facultatif, mais plus élégant pour la sortie)16.Par la suite, la fonction replicate() permet de répéter n fois la fonction expr. Il faut bien distinguer le n (nombre de participants) de gen.ind.effect() de celui de replicate() (nombre de réplications). Enfin, un histogramme est produit pour présenter les résultats.La Figure 13.2 montre la distribution des effets indirects obtenus avec la simulation. La ligne correspond à la distribution gaussienne sous-jacente au test de Sobel. Dans le cas n = 50 testé, il appert évident que les effets ne suivent pas exactement la distribution attendue. La distribution est centrée sur la bonne valeur \\(0.2 \\times 0.3 = 0.06\\). La distribution est toutefois asymétrique.\nFigure 13.2: Distribution de l’effet indirect\nLe chapitre Simuler présente une technique statistique toute désignée lorsque la disbribution statistique n’est pas connue17, il s’agit du bootstrap.","code":"\nmdm <- function(donnees, alpha = 0.05){\n  # alpha est l'erreur de type I\n  \n  # Réaliser les deux régressions\n  etape1 <- lm(formula = m ~ x, data = donnees)\n  etape2 <- lm(formula = y ~ x + m, data = donnees)\n  \n  # Extraire les statistiques\n  # Les coefficients de régression\n  beta_21 <- etape1$coefficients[\"x\"]\n  beta_32 <- etape2$coefficients[\"m\"]\n  # Les erreurs standards\n  SEa <- summary(etape1)$coefficients[\"x\", \"Std. Error\"]\n  SEb <- summary(etape2)$coefficients[\"m\", \"Std. Error\"]\n  \n  # Calcul du score z de l'effet indirect\n  SE <- sqrt(beta_21^2 * SEb^2 + beta_32^2 * SEa^2)\n  z <- beta_21 * beta_32 / SE\n  \n  # Décision\n  if(abs(z) >= qnorm(1 - alpha/2)){\n    cat(\"L'effet indirect est significatif\")\n    decision = TRUE\n  } else {\n    cat(\"L'effet indirect n'est pas significatif\")  \n    decision = FALSE\n  }\n  return(decision)\n}\n# Pour la reproductibilité\nset.seed(1442)\n\n# Matrice de covariance\nSigma <- matrix(c( 1, .2,  0,\n                  .2,  1, .3,\n                   0, .3,  1),\n                ncol = 3, nrow = 3,\n                dimnames = list(nom <- c(\"x\", \"m\", \"y\"), \n                                nom))\n\n# Créer une fonction qui génère des données\n# selon une matrice de covariance `Sigma`\n# et un nombre de participants `n`\ngen.ind.effect <- function(Sigma, n){\n  # Créer jeu de données\n  jd <- as.data.frame(MASS::mvrnorm(n =  n, \n                                    Sigma = Sigma, \n                                    mu = rep(0, ncol(Sigma))))\n  # Réaliser deux régressions\n  etape1 <- lm(formula = m ~ x, data = jd)\n  etape2 <- lm(formula = y ~ x + m, data = jd)\n  \n  # Extraire et multiplier les coefficients de régression\n  unname(coef(etape1)[\"x\"] * coef(etape2)[\"m\"])\n}\n# Répliquer 5000 la fonction `gen.ind.effect`\ntest.ind <- replicate(n = 5000, expr = gen.ind.effect(Sigma, n = 50))\n\n# Afficher l'histogramme des résultats\nhist(test.ind, prob = TRUE)"},{"path":"médier.html","id":"la-technique-du-bootstrap","chapter":" 13 Médier","heading":"13.2.3 La technique du bootstrap","text":"La technique la plus recommandée dans la littérature méthodologique est la méthode du bootstrap. Elle effectivement plusieurs avantages comparativement à ses adversaires. Elle ne présuppose par une distribution normale de l’effet indirect, ce que sous-entend la méthode delta multivarié. Elle une erreur de type et une puissance appropriée contrairement aux deux autres. Elle ne viole aucun postulat. Dans les faits, il est assez rare de voir des différences notables avec le test de Sobel. Elle demeure la technique recommandée.Le bootstrap se base sur trois étapes :Sélectionner aléatoirement et avec remplacement les unités d’un jeu de données;Sélectionner aléatoirement et avec remplacement les unités d’un jeu de données;Calculer l’indice statistique désirée;Calculer l’indice statistique désirée;Réitérer ces étapes un nombre très élevé de fois.Réitérer ces étapes un nombre très élevé de fois.Le test d’hypothèse de l’effet indirect n’échappe pas à cette logique.","code":"\nboot <- function(donnees, alpha = .05, nreps = 5000){\n  # alpha est l'erreur de type I\n  # nreps  est le nombre de répétitions\n  \n  # Informations nécessaire au bootstrap\n  # Nombre d'unités\n  n = nrow(jd)  \n  \n  # Variable vide pour enregistrer\n  effet.indirect = as.numeric()\n  \n  # La boucle\n  for (i in 1:nreps){\n    # Sélectionner aléatoirement et avec remplacement\n    # les unités d'un jeu de données\n    index <- sample(n, replace = TRUE)\n    D = jd[index,]\n    \n    # Calculer l'indice statistique désirée\n    b21 <- coef(lm(y ~ x, data = D))[\"x\"]\n    b32 <- coef(lm(y ~ x + m, data = D))[\"m\"]\n    effet.indirect[i] = b21 * b32\n  }\n  \n  # Créer l'intervalle de confiance avec alpha \n  CI <- quantile(effet.indirect, \n                 probs = c(alpha/2, 1 - alpha/2))\n  \n  # Si l'intervalle ne contient pas 0, \n  # l'hypothèse nulle est rejetée.\n  if(prod(CI > 0)){\n    \n    cat(\"L'effet indirect est significatif\")\n    decision = TRUE\n    \n  }else{\n    \n    cat(\"L'effet indirect est non significatif\") \n    decision = FALSE\n    \n  }\n  \n  return(decision)\n  \n}"},{"path":"médier.html","id":"la-création-de-données-2","chapter":" 13 Médier","heading":"13.3 La création de données","text":"La création de données d’un modèle de médiation est assez rudimentaire, particulièrement après la lecture du chapitre Créer. En fait, dans ce chapitre, un modèle de médiation simple (à trois variables) est utilisé. Par contre, le chapitre n’insiste pas sur les effets indirects, ce qui sera fait ici.Pour l’exemple, \\(\\mathbf{B}\\) spécifie les coefficients de régression.\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{ccc}\n0 &  0 & 0 \\\\\n.5 &  0 & 0 \\\\\n-.3 & .4 & 0 \\\\\n\\end{array}\n\\right)\n\\tag{13.3}\n\\end{equation}\\]Le jeu de données est créé en suivant les étapes de chapitre Créer. Maintenant, il reste à déterminer l’effet indirect dans le jeu de données. Le jeu de données issu du système d’équations sera utilisé pour la suite.Comme les fonctions maisons pour la méthode d’étape causale (BK()), la méthode delta multivarié (mdm()) et la méthode bootstrap (boot()), il est possible de les utiliser pour vérifier l’effet indirect.Toutes les analyses confirment la présence d’un effet indirect.Le défaut des fonctions maison (BK(), mdm() et boot()) est certainement qu’elles ne font que tester les effets indirects. Une fonction plus intéressante serait d’afficher toutes les sorties, soit les coefficients de régressions, leur erreur type, leur intervalle de confiance ou toutes autres informations jugées pertinentes.","code":"\n# Pour la reproductibilité\nset.seed(1102)\n\n# Paramètres du modèle\n# Taille d'échantillon\nn <- 100\n\n# Matrice de coefficients de régression\nB <-  matrix(c( 0,  0, 0,\n               .5,  0, 0,\n               .3, .6, 0), \n             ncol = 3, nrow = 3,\n             byrow = TRUE,\n             dimnames = list(nom <- c(\"x\", \"m\", \"y\"), \n                             nom))\n# Variance des variables\nV = c(1, 1, 1) \n\n# Première option : \n# Créer de la matrice de covariance à partir de B et V\n# Vérifier que la fonction `beta2cov()` est \n# bien dans l'environnement sinon la ligne suivante\n# ne fonctionne pas.\nS <- beta2cov(B = B, V = V)   \n\n# Créer le jeu de données avec la matrice de covariance\njd <- MASS::mvrnorm(n = n, mu = c(0, 0, 0), Sigma = S)\njd <- as.data.frame(jd)\n\n# Deuxième option : \n# Création des données en système d'équations\n# Première étape\nx <- rnorm(n, sd = V[1])\n\n# Deuxième étape\nem <- rnorm(n = n, sd = sqrt(V[2] - B[2,1]^2 * V[1]))\nm <- B[2,1] * x + em\n\n# Troisième étape\nsd_ey = sqrt(V[3] - (B[3,1]^2 * V[1] + \n                       B[3,2]^2 * V[2] + \n                       2 * B[3,1] * B[3,2] * \n                       V[1]^.5 * V[2]^.5 * B[2,1]))\ney <- rnorm(n = n, sd = sd_ey)\ny <- B[3,1] * x + B[3,2] * m + ey\n\n# Créer le de données avec les trois variables\njd <- data.frame(x = x,\n                 m = m,\n                 y = y)BK(donnees = jd)\n> L'effet indirect est significatif \n>           et la médiation est partielle\n> [1] TRUE\nmdm(donnees = jd)\n> L'effet indirect est significatif\n> [1] TRUE\nboot(donnees = jd)\n> L'effet indirect est significatif\n> [1] TRUE"},{"path":"médier.html","id":"analyse-complète","chapter":" 13 Médier","heading":"13.4 Analyse complète","text":"La fonction suivante extrait tous les coefficients de régression d’un modèle récursif. L’ordre des variables est ici d’une énorme importance, puisque c’est l’ordre des variables dans le jeu de données qui déterminer l’ordre causal des variables: la première étant la cause de toutes, et la dernière l’effet de toutes.Pour tester ce code, il vaut la peine de tester chacune des étapes de la syntaxe précédente avec une matrice de covariance.Transformer la matrice de covariance en matrice de coefficient de régression.Vectoriser BETA.Libeller les effets directs.Lister tous les effets indirects possiblesDans cet exemple, il y deux niveaux d’effets indirects : un niveau à trois variables dont il y quatre combinaisons possibles et un second niveau à quatre variables dont il n’y qu’une combinaison.Extraire tous les effets indirects et les libeller adéquatement.Calculer les effets totaux de la première variable et mettre le tout en commun.Magnifique! Toutefois, le statisticien ne s’intéresse rarement qu’aux coefficients de régression. Il aime aussi connaître l’erreur type (erreur standard), la valeur-\\(p\\), ou peut-être même souhaite-t-il calculer un intervalle de confiance. Une solution bien simple qui ne nécessitera que peu de syntaxe, en plus de respecter les postulats sous-jacents à l’analyse de médiation est le bootstrap. Les éléments fondamentaux du bootstrap sont toujours les mêmes : prendre un jeu de données avec des unités rééchantillonnées aléatoirement avec remplacement, calculer les indices désirés, et réitérer un nombre élevé de fois.Avantageusement la fonction maison indirect() calcule tous les indices statistiques pertinents. Il ne reste que le rééchantillonnage et les réplications à programmer.La variable Resultats contient tous les résultats pertinents. La colonne Resultats$Estimates retourne tous les coefficients de régression avec leur erreur type (erreur standard ou standard error) en deuxième colonne. Les dernières colonnes donnent les intervalles de confiance inférieurs et supérieurs. Comme aucune ne contient la valeur 0 au sein de son intervalle, alors elles sont toutes significatives.\nFigure 13.3: Résultats de l’analyse de médiation\nLes résultats sont illustrés dans la Figure 13.3. Les coefficients sont ajoutés à leurs trajectoires respectivement. Par bonnes mesures, des étoiles de significativité, *, l’ultime symbole de découvertes scientifiques, sont ajoutés aux trajectoires dont les intervalles de confiance n’incluent pas 0. Il ne reste qu’à rapporter l’effet indirect dans le texte ou un tableau d’un article scientifique, comme le tableau 13.1.\nTable 13.1: Résultats de l’analyse de médiation\nCalculer les valeurs-\\(t\\) et valeurs-\\(p\\) est envisageable en utilisant les résultats déjà recueillis. La valeur-\\(t\\) est le ratio entre l’estimateur et son erreur type, la valeur-\\(p\\) est la rareté d’observer cette valeur-\\(t\\) ou une valeur plus rare par rapport à l’hypothèse nulle avec un degré de liberté de dl = n - p, soit plus exactement le nombre d’unités moins le nombre de variables indépendantes.Si les coefficients de régression standardisés étaient préférés, ceux-ci sont obtenables simplement en standardisant le jeu de données, puis rouler l’analyse de médiation de nouveau. Pour standardiser rapidement, z.donnees = apply(donnees, MARGIN = 2, FUN = scale) applique (apply()) la fonction FUN = scale qui standardise les données (donnees) par colonne MARGIN = 2.","code":"\nindirect <-  function(donnees){\n  COV <-  cov(donnees)   # Matrice de covariance\n  p <-  ncol(COV)        # Nombre de variables\n  \n  # Calculer la matrice des coefficients de\n  # régression. Le lecteur assidu aura reconnu \n  # la fonction `cov2beta()`\n  BETA <-  matrix(0, p, p)  \n  for(i in 1:(p-1)){\n    R <-  solve(COV[1:i,1:i], COV[1+i,1:i])\n    BETA[i+1, 1:i] <-  R\n  }\n  \n  # Extraire les coefficients de régression en vecteur\n  est <-  as.matrix(BETA[lower.tri(BETA)])\n  \n  # Libellés des effets directs\n  name <-  colnames(COV)\n  label <-  matrix(name[combn(p, 2)], (p * (p-1) / 2), 2, byrow = TRUE)\n  rname <-  apply(FUN = paste, as.matrix(label[,1]), MARGIN = 2, \"->\")\n  rname <-  apply(FUN = paste, rname, label[,2], MARGIN = 2, \"\")\n  row.names(est) <-  rname\n  \n  # Lister tous les effets indirects possibles\n  if(p != 3){\n    # S'il y a plus de 3 variables, \n    # identifier les niveaux supérieurs\n    # d'effets indirects (à 4 variables et plus)\n    listeffects <-  mapply(combn, p, 3:p)\n  } else {\n    # S'il y a 3 variables, il n'y a qu'un niveau\n    listeffects <-  list((matrix(1:3, 3, 1)))\n  }\n  \n  \n  # Extraire tous les effets indirects\n  for(i in 1:length(listeffects)){    # Nombre de niveaux d'effet indirect\n    J <-  ncol(listeffects[[i]])      # Nombre d'effets du niveau i\n    for(j in 1:J){                \n      ide <-  listeffects[[i]][,j]    # Identifier l'effet en cours\n      B <-  BETA[ide, ide]            # Leur coefficients de régression \n      B <-  B[-1, -ncol(B)]           # Retirer les coefficients superflus\n      e <-  as.matrix(prod(diag(B)))  # Calculer l'effet indirect\n      rownames(e) <- paste(name[ide], # Le libellé\n                           collapse = \" -> \")\n      est <- rbind(est, e)            # Ajouter l'effet aux autres\n    }\n  }\n  \n  # Ajout les effets totaux\n  # Calculs\n  totald <-  as.matrix(solve(COV[1,1], COV[p, 1]))\n  totali <-  as.matrix(totald - BETA[p, 1])\n  \n  # Libeller\n  rownames(totali) <- paste(\"total indirect\",\n                            colnames(COV)[1],\n                            \"->\",\n                            colnames(COV)[p])\n  rownames(totald) <-  paste(\"total effect\",\n                             colnames(COV)[1],\n                             \"->\",\n                             colnames(COV)[p])\n  \n  # Mettre le tout en commun\n  estimates <-  rbind(est, totali, totald)\n  return(estimates)\n}# Nombre de variables\np <- 4\n\n# Voici la matrice de covariance utilisée\nCOV <- matrix(c(3, 2, 1, 4,\n                2, 6, 2, 5,\n                1, 2, 5, 1,\n                4, 5, 1, 4), ncol = p, nrow = p)\ncolnames(COV) = letters[1:p]\nrownames(COV) = letters[1:p]\n\n# Normalement, celle-ci serait obtenue d'un échantillon\nCOV\n>   a b c d\n> a 3 2 1 4\n> b 2 6 2 5\n> c 1 2 5 1\n> d 4 5 1 4# Les prochaines calcules la matrice de coefficients\n# régression. Le lecteur assidu aura reconnu \n# la fonction `cov2beta()`\nBETA <-  matrix(0, p, p)  \nfor(i in 1:(p-1)){\n  R <-  solve(COV[1:i,1:i], COV[1+i,1:i])\n  BETA[i+1, 1:i] <-  R\n}\nBETA\n>       [,1]  [,2]   [,3] [,4]\n> [1,] 0.000 0.000  0.000    0\n> [2,] 0.667 0.000  0.000    0\n> [3,] 0.143 0.286  0.000    0\n> [4,] 1.033 0.567 -0.233    0# Extraire les coefficients de régression en vecteur\nest <-  as.matrix(BETA[lower.tri(BETA)])\nest\n>        [,1]\n> [1,]  0.667\n> [2,]  0.143\n> [3,]  1.033\n> [4,]  0.286\n> [5,]  0.567\n> [6,] -0.233# Libellés des effets directs\nname <-  colnames(COV)\nlabel <-  matrix(name[combn(p, 2)], (p * (p-1) / 2), 2, byrow = TRUE)\nrname <-  apply(FUN = paste, as.matrix(label[,1]), MARGIN = 2, \"->\")\nrname <-  apply(FUN = paste, rname, label[,2], MARGIN = 2, \"\")\nrow.names(est) <-  rname\n# Beaucoup de syntaxe pour au final bien peu, mais\n# le résultat est élégant\nest\n>           [,1]\n> a -> b   0.667\n> a -> c   0.143\n> a -> d   1.033\n> b -> c   0.286\n> b -> d   0.567\n> c -> d  -0.233# Lister tous les effets indirects possibles\nif(p != 3){\n  # S'il y a plus de 3 variables, identifier les niveaux supérieurs\n  # d'effets indirects (à 4 variables et plus)\n  listeffects <-  mapply(combn, p, 3:p)\n} else {\n  # S'il y a 3 variables, il n'y a qu'un niveau\n  listeffects <-  list((matrix(1:3, 3, 1)))\n}\nlisteffects\n> [[1]]\n>      [,1] [,2] [,3] [,4]\n> [1,]    1    1    1    2\n> [2,]    2    2    3    3\n> [3,]    3    4    4    4\n> \n> [[2]]\n>      [,1]\n> [1,]    1\n> [2,]    2\n> [3,]    3\n> [4,]    4# Extraire tous les effets indirects\nfor(i in 1:length(listeffects)){  # Nombre de niveaux d'effet indirect\n  J <-  ncol(listeffects[[i]])      # Nombre d'effet du niveau i\n  for(j in 1:J){                \n    ide <-  listeffects[[i]][,j]    # Identifier l'effet en cours\n    B <-  BETA[ide, ide]            # Leur coefficients de régression \n    B <-  B[-1, -ncol(B)]           # Retirer les coefficients superflus\n    e <-  as.matrix(prod(diag(B)))  # Calculer l'effet indirect\n    rownames(e) <- paste(name[ide], # Le libellé\n                         collapse = \" -> \") \n    est <- rbind(est, e)            # Ajouter l'effet aux autres\n  }\n}\nest\n>                     [,1]\n> a -> b            0.6667\n> a -> c            0.1429\n> a -> d            1.0333\n> b -> c            0.2857\n> b -> d            0.5667\n> c -> d           -0.2333\n> a -> b -> c       0.1905\n> a -> b -> d       0.3778\n> a -> c -> d      -0.0333\n> b -> c -> d      -0.0667\n> a -> b -> c -> d -0.0444# Ajout les effets totaux\n# Calculs\ntotald <-  as.matrix(solve(COV[1,1], COV[p, 1]))\ntotali <-  as.matrix(totald - BETA[p, 1])\n\n# Libeller\nrownames(totali) <- paste(\"total indirect\",\n                          colnames(COV)[1],\n                          \"->\",\n                          colnames(COV)[p])\nrownames(totald) <-  paste(\"total effect\",\n                           colnames(COV)[1],\n                           \"->\",\n                           colnames(COV)[p])\n\n# Mettre le tout en commun\nestimates <-  rbind(est, totali, totald)\nestimates\n>                          [,1]\n> a -> b                 0.6667\n> a -> c                 0.1429\n> a -> d                 1.0333\n> b -> c                 0.2857\n> b -> d                 0.5667\n> c -> d                -0.2333\n> a -> b -> c            0.1905\n> a -> b -> d            0.3778\n> a -> c -> d           -0.0333\n> b -> c -> d           -0.0667\n> a -> b -> c -> d      -0.0444\n> total indirect a -> d  0.3000\n> total effect a -> d    1.3333# Le bootstrap de `indirect()` pour le jeu de données en exemple (trois variables)\n# Informations préliminaires\nalpha <- .05         # Erreur de type I\nn <-  nrow(jd)       # Nombre d'unités\nreps <-  5000        # Nombre de réplications\n\n# Vérifier que la fonction `indirect()`  est bien \n# dans l'environnement\nEst <-  indirect(jd)\n\n# Variable vide (Est) pour enregistrer les résultats\n# avec comme 1ere colonne, les résultats originaux\nEst <-  data.frame(Est = Est, \n                   X = matrix(0, ncol = reps)) \n\n# La boucle\n# Elle commence à 2 à cause de la première colonne\nfor(i in 2:(reps+1)){\n  index <- sample(n, replace = TRUE)\n  D <- jd[index,]\n  Est[,i] <-  indirect(D)\n}\n\n# Mettre le tout en commun\nResultats <- data.frame(\n  Estimates = Est$Est,\n  S.E. = apply(Est, MARGIN = 1, FUN = sd), \n  CIinf = apply(Est, MARGIN = 1, FUN = quantile, probs = alpha/2),\n  CIsup = apply(Est, MARGIN = 1, FUN = quantile, probs = 1-alpha/2)\n)\nResultats\n>                       Estimates   S.E.  CIinf CIsup\n> x -> m                    0.336 0.0930 0.1602 0.521\n> x -> y                    0.312 0.0616 0.1974 0.442\n> m -> y                    0.625 0.0731 0.4786 0.765\n> x -> m -> y               0.210 0.0608 0.0976 0.333\n> total indirect x -> y     0.210 0.0608 0.0976 0.333\n> total effect x -> y       0.521 0.0843 0.3641 0.691Resultats$t.value <- Resultats$Estimates / Resultats$S.E.\nResultats$p.value <- (1 - pt(abs(Resultats$t.value), df = n - p)) * 2\nround(Resultats, 3) # Résultats arrondis à 3 décimales\n>                       Estimates  S.E. CIinf CIsup t.value\n> x -> m                    0.336 0.093 0.160 0.521    3.61\n> x -> y                    0.312 0.062 0.197 0.442    5.06\n> m -> y                    0.625 0.073 0.479 0.765    8.55\n> x -> m -> y               0.210 0.061 0.098 0.333    3.45\n> total indirect x -> y     0.210 0.061 0.098 0.333    3.45\n> total effect x -> y       0.521 0.084 0.364 0.691    6.18\n>                       p.value\n> x -> m                  0.000\n> x -> y                  0.000\n> m -> y                  0.000\n> x -> m -> y             0.001\n> total indirect x -> y   0.001\n> total effect x -> y     0.000"},{"path":"médier.html","id":"les-packages-2","chapter":" 13 Médier","heading":"13.5 Les packages","text":"Le présent chapitre ne fait que gratter la surface de ce qu’il est possible de faire avec l’analyse de médiation. Des articles comme Caron & Valois (2018) et Lemardelet & Caron (2022) donnent des exemples de syntaxe R en plus d’approfondir l’analyse. Il existe plusieurs packages R pour réaliser l’analyse de médiation, comme mediation (Tingley et al., 2014) et Rmediation (Tofighi & MacKinnon, 2011), tous les deux ayant leur propre documentation. Pour des analyses plus compliquées, les packages comme lavaan (Rosseel, 2012) permettent de faire des analyses de médiation avec la modélisation par équations structurelles, Toutefois, ce chapitre espère avoir convaincu le lecteur que l’analyse peut être relativement aisément fait maison.","code":""},{"path":"références.html","id":"références","chapter":"Références","heading":"Références","text":"","code":""}]
