[{"path":"index.html","id":"bienvenue","chapter":"Bienvenue!","heading":"Bienvenue!","text":"Bienvenue dans la version préliminaire et en ligne du livre Méthodes quantitatives avec R de P.-O. Caron. Le livre porte sur la programmation statistique en R et vise à démystifier les mécanismes ésotériques derrière les logiciels statistiques. Son objectif est de faire le pont entre la statistique et la programmation afin de les rendre plus accessibles et d’autonomiser la pratique. L’approche permet à la fois une meilleure compréhension des statistiques à l’aide du logiciel, mais constitue également un motif d’apprentissage pour débuter la programmation. Il vise un public autant intéressé à s’initier à R qu’à en connaître davantage sur la statistique, mais surtout un lecteur qui souhaite maîtriser les deux à la fois.Si vous avez des commentaires ou des suggestions, n’hésitez pas à les partager. Cet ouvrage est en construction. Il est imparfait et contient malheureusement des erreurs. Vous pouvez m’écrire à l’adresse suivante : pier-olivier.caron[]teluq.ca pour toutes suggestions et améliorations.","code":""},{"path":"index.html","id":"remerciements","chapter":"Bienvenue!","heading":"Remerciements","text":"Les étudiants et étudiantes qui ont suivi des cours avec ce manuel et plus spécialement Catherine Meek-Bouchard pour ses commentaires.","code":""},{"path":"préface.html","id":"préface","chapter":"Préface","heading":"Préface","text":"La méthode scientifique impose aux expérimentateurs de tirer des conclusions avec minutie sur leur objet d’étude. Toutefois, ils font confiance à d’autres pour des tâches qui dépassent leur propre champs de compétence, et ce, avec raisons, car celles-ci peuvent être fort compliquées et nécessitées des formations académiques complémentaires. Les statistiques font parties de ces expertises extérieures en sciences humaines et sociales. Pour ne pas faillir de son esprit critique, ne serait-ce qu’un instant, il est nécessaire de comprendre les fondements et l’implantation logiciel. Trop souvent, les expérimentateurs n’ont qu’une brève introduction aux statistiques et font confiance à des logiciels commerciaux pour les calculs sophistiqués. Le tout se déroule sans tracas apparent. Toutefois, une expérience plus soutenue avec ces logiciels révéle qu’ils ont eux-aussi leurs lots de défauts, de bogues et d’erreurs de calcul. Ces erreurs se produisent parfois à l’insu de l’utilisateur. Sans connaître le résultat attendu, comment distinguer le vrai du faux des sorties statistiques?Il faut être naïf pour croire que le code fonctionne tel que prévu.Par exemple, que vaut l’expression \\(-9^{(.5)}\\)? Reconnaître qu’il s’agit de \\(\\sqrt{-9}\\) indique immédiatement que le résultat n’est pas réel (c’est un nombre complexe). Si ce calcul est demandé à R,celui-ci retourne la réponse \\(-3\\) à cause de la priorité des opérations, \\(-(9)^{.5}\\). Était-ce la réponse désirée?Des idiosyncrasies computationnelles se retrouvent dans tous les logiciels. En pratique, le programmeur doit en avoir conscience et ce type de problème devrait rester trivial…Bennett et al. (2010) montrent qu’un saumon de l’Atlantique décédé des cognitions sociales. Ils placent sous imagerie par résonance magnétique fonctionnelle (IRMf) le saumon pendant une tâche de reconnaissance des émotions chez les humains. La Figure 0.1 montre le résultat de leur analyse. Soit ils sont tombés sur une découverte étonnante en termes de cognition ichtyologique post-mortem, soit quelque chose cloche en ce qui concerne l’approche statistique utilisée.\nFigure 0.1: Cognitions sociales d’un saumon atlantique décédé. Tirés de Bennett et al. 2010, p.4\nLes points rouges de la Figure 0.1 indiquent les cognitions sociales du saumon. Plus sérieusement, ces chercheurs critiquent l’absence de contrôle de l’erreur de type (faux positif) lors de comparaisons multiples. Ce problème est d’autant plus important avec des humains sous IRMf, car ces situations comportent beaucoup de bruits statistiques, contrairement au saumon décédé qui n’en produit pas du tout. Les auteurs montrent bien qu’en absence de ce contrôle, l’analyse peut fournir des résultats farfelus. Il revient à l’expérimentateur de bien utiliser les analyses statistiques et de savoir ce que le logiciel fait et ne fait pas.L’étude de Bennett et al. (2010) n’est qu’un canular. Les auteurs ont remporté le prix Ig Nobel en 2012, un prix scientifique pour de vrais chercheurs ayant réalisé une vraie étude faisant d’abord rire, puis réfléchir. En pratique, ce type de problème devrait rester trivial…Eklund et al. (2016) tentent de valider les méthodes statistiques derrière l’IRMf à l’aide de données réelles. Ils utilisent des données d’IRMf provenant de 499 personnes saines et en état de repos pour effectuer 3 millions d’analyses de groupes de tâches. En utilisant ces données nulles avec différents plans expérimentaux, ils estiment l’incidence des résultats significatifs (en théorie fixé à 5% de faux positifs pour un seuil de signification de 5%). Toutefois, ils constatent que les progiciels les plus courants pour l’analyse de l’IRMf entraînent des taux de faux positifs allant jusqu’à 70%. Ces résultats remettent en question la validité d’un certain nombre d’études utilisant l’IRMf et ont des conséquences importantes sur l’interprétation des résultats de neuro-imagerie.Cette étude fait un tollé dans la littérature scientifique. Bien que certains aspects de l’étude soient attaquables, il n’en demeure pas moins qu’elle met en lumière le danger de faire confiance aveuglément aux analyses statistiques.L’objectif de ce court texte n’est pas de discrédité les analyses statistiques ou les chercheurs qui les emploient. Ce n’est pas non plus de concrétiser l’adage : faire dire n’importe quoi à une statistique (c’est plus facile de faire dire n’importe quoi à des mots, par ailleurs). Les statistiques sont très utiles et fournissent de l’information qu’il est impossible d’obtenir autrement. Il s’agit d’une mise en garde justifiant en partie l’approche pédagogique de ce livre.Il est dit en partie, car le seconde, en filigrane de ce livre, repose sur la double approche d’apprentissage statistique et programmation. Il s’agit de la symbiose entre les deux. D’une part, rendre accessible et efficace des calculs parfois laborieux et ennuyants et, d’autre part, de fournir un objectif de programmation afin de susciter l’intérêt et l’engagement. En programmant des modèles statistiques, le lecteur une plus grande emprise sur ceux-ci. Il peut mieux les utiliser, les étudier et les modifier. Il peut s’en servir pour connaître et apprendre davantage. Certaines boîtes noires resteront irrémédiablement des boîtes noires pour le lecteur, mais plus elles seront démystifiées, mieux il comprendra ce qui se cache derrière les algorithmes statistiques.La première section de ce livre explique et justifie le logiciel R, montre comment l’installer ainsi que quelques rudiments statistiques qui seront fort utiles pour le reste de la lecteur.La seconde section couvre ce qui sera probablement le plus important pour l’utilisateur : , l’entrer, l’importation (Importer), la gestion (Manipuler) et la visualisation (Visualiser) des données.La troisième section commence par les analyses statistiques descriptives (Décrire), mais porte principalement sur l’inférence statistique avec les tests d’hypothèses (Inférer) pour par la suite développer les analyses de base (Analyser), comme le test \\(t\\), l’ANOVA, la corrélation, et le test du \\(\\chi^2\\) carré. En parallèle, les méthodes pour créer des jeux de données propres aux analyses sont illustrées. La section se termine sur des exemples d’utilisation de R (Simuler) à des fins plus empiriques, soit développer une simulation Monte-Carlo, mais surtout, les techniques de rééchantillonnage (le bootstrap).La quatrième section présente le modèle linéaire général, comme la régression (Prédire), la médiation (Médier) et la modération (Modérer). Comme auparavant, l’accent est mis sur la création de données correspondant à ces modèles, particulièrement au chapitre Créer qui montre en détails la création de modèle linéaire récursif.La cinquième section montre les analyses factorielles, soit l’analyse en composantes principales (Décomposer), l’analyse factorielle exploratoire (Explorer). Un chapitre est dédier au sujet d’identifier le nombre de facteurs à retenir dans les analyses exploratoires (Réduire).","code":"-9^(.5)\n> [1] -3"},{"path":"commencer.html","id":"commencer","chapter":" 1 Commencer","heading":" 1 Commencer","text":"Dans cette section, le logiciel R est brièvement décrit ainsi qu’une justification, plus spécifiquement ces avantages, en plus de pointer quelques options complémentaires (RStudio) pour son utilisation. Par la suite, les étapes pour installer et commencer avec les logiciels R et RStudio sont présentées.","code":""},{"path":"commencer.html","id":"r","chapter":" 1 Commencer","heading":"1.1 R?","text":"R (R Core Team, 2023) est un logiciel de programmation statistique libre-accès et un environnement pour la computation statistique et l’affichage graphique. Il s’agit d’un projet GNU qui est similaire au langage et à l’environnement S, développés aux Laboratoires Bell (anciennement &T, aujourd’hui Lucent Technologies) par John Chambers. Créé par Ross Ihaka et Robert Gentleman, R fournit une grande variété de techniques statistiques (modélisation linéaire et non linéaire, analyses statistiques classiques, analyse de séries chronologiques, classification) et graphiques, et est hautement extensible.R est un logiciel basé sur la syntaxe plutôt qu’une approche pointer-et-cliquer (point--click) comme les logiciels traditionnels. Il peut être plus effrayant ou apparaître trop complexe pour un nouvel utilisateur, mais une fois apprivoisée, cette bête démontre un fort potentiel que ce soit en automatisation, en personnalisation, en production de figure de haute qualité, etc. R l’avantage de mettre en plein contrôle ses utilisateurs. Bref, c’est une créature qu’il vaut la peine de maîtriser.","code":""},{"path":"commencer.html","id":"pourquoi-r","chapter":" 1 Commencer","heading":"1.2 Pourquoi R?","text":"Les logiciels traditionnelles suspendent trop souvent la réflexion critique. Ils sont dociles. L’usager clique sur les bonnes options et obtient les résultats désirés. En échange d’une expérience “simple et intuitive”, ils compromettent l’épanouissement de l’utilisateur et cloîtrent l’analyse statistique dans une boîte noire, un programme dont le fonctionnement ne peut être connu. Sont évacuées toutes connaissances des analyses, seules les entrées et les sorties sont pertinentes.En se limitant à ces logiciels, les utilisateurs sont également à la merci des compagnies qui les distribuent. Elles maintiennent des prix exorbitants pour des licences annuelles, malgré le faible soutien technique, la désuétude ou le manque de mises à jour, la présence de bogues informatiques. Ces problèmes sont monnaie courante bien que la licence ne soit pas de la petite monnaie.Contrairement aux logiciels traditionnels, R permet de réaliser les analyses, mais aussi de les programmer soi-même, de générer des données propres à un modèle, de rester à jour sur les nouvelles tendances et les découvertes en méthodologie de la recherche, de partager aisément les connaissances et la reproduction d’analyses sophistiquées, et tout cela, gratuitement. Évidemment, cela n’est possible que par l’immense communauté derrière le logiciel.R est complètement gratuit. Il est le logiciel le plus utilisé parmi les scientifiques de sciences de données, statisticiens, etc. Il est l’exemple ultime d’une plateforme communautaire qui fait mieux que les compétiteurs commerciaux. De plus en plus de personnes migrent vers R, mais peu de ses utilisateurs quittent le logiciel vers un autre. Et plus de personnes se joignent à la communauté, plus il y de documentation, d’aide, de soutien, que ce soit sous forme de livres, de vidéos, d’article, d’ateliers, de formations, de forum. L’expérience R devient de plus en plus accessible aux nouveaux immigrants. Comme le code source est ouvert, ses utilisateurs collaborent à la création de modules augmentant ses capacités qui permettent de résoudre des problèmes de plus en plus sophistiqués (et pas juste en statistiques!). Ces modules sont également gratuits et il y en littéralement des milliers.L’avantage de maîtriser R, plus spécifiquement de l’utilisation de syntaxe, pour l’expérimentateur est de rendre l’analyse statistique facilement transmissible entre expérimentateur (facilite grandement la collaboration), reproductible (vérification et collaboration), répétable (pour de nouvelles données ou expériences), et ajustable (pour de nouveaux scénarios). Quelqu’un d’autre peut jeter un coup d’oeil à l’analyse réalisée et voir exactement ce qui s’est produit, tant dans la gestion du jeu de données que l’analyse et la génération de graphiques. L’analyse peut être reproduite par les pairs, voire répéter si des données supplémentaires ont été recueillies ou si une nouvelle expérience été réalisée.Même si R est un langage de programmation et que cela peut en intimider plus d’un, il est relativement intuitif à apprendre et assez simple d’utilisation dans la mesure où les ressources appropriées pour apprendre sont accessibles à l’utilisateur. L’apprentissage de la programmation, même si ce n’est pas en R, permet de mieux comprendre le fonctionnement des ordinateurs en plus de reconsidérer le dogmatisme de tout-puissant “algorithmes”. Il permettra aussi à l’utilisateur lorsqu’il saura suffisamment maîtriser la bête à créer lui-même la syntaxe qu’il lui permettra de résoudre les problèmes sur lesquels il s’intéresse.Enfin, R possède un incroyable moteur pour la visualisation de données et de capacités graphiques. Aucun autre logiciel ne lui arrive à la cheville.","code":""},{"path":"commencer.html","id":"installer-r","chapter":" 1 Commencer","heading":"1.3 Installer R","text":"R est compatible pour Windows, Mac et Linux. Pour télécharger le logiciel, il faut se rendre sur le site http://www.r-project.org et sélectionner les hyperliens “download R” ou “CRAN mirror”. Il faut ensuite choisir un mirror de son pays d’origine. Par la suite, la page permet de choisir la version appropriée à son système d’exploitation. Il faut alors suivre les indications.Il est principalement utilisé en anglais bien qu’peut le définir en français. La plupart de l’aide est en anglais surtout celle pour les analyses spécialisées. Toutefois, il y beaucoup de ressources accessibles en ligne en français pour une introduction débutante et intermédiaire.","code":""},{"path":"commencer.html","id":"rstudio","chapter":" 1 Commencer","heading":"1.3.1 RStudio","text":"Bien que R puisse être utilisé seul, son aspect rudimentaire peut en inquiéter plus d’un. Le logiciel RStudio permet une utilisation plus fluide et intuitive pour les usagers ayant peu ou pas d’expériences en programmation. RStudio est un environnement de développement intégré (IDE pour integrated development environment) pour R par Posit (initialement RStudio avant l’été 2022). Il comprend une console, un éditeur de mise en évidence de la syntaxe qui prend en charge l’exécution directe du code, ainsi que des outils de traçage, d’historique, de débogage et de gestion de l’espace de travail. Le logiciel est gratuit et libre-accès pour une utilisation personnelle. Il comporte aussi une version commerciale qui nécessite un certain déboursement de fonds.Pour télécharger le logiciel, il faut se rendre sur le site https://posit.co/ et naviguer jusqu’au téléchargement du logiciel. La version gratuite de RStudio est téléchargeable spécifiquement sur cette page https://posit.co/download/rstudio-desktop/#download. RStudio est uniquement disponible en anglais.","code":""},{"path":"commencer.html","id":"les-avantages-de-rstudio","chapter":" 1 Commencer","heading":"1.3.1.1 Les avantages de RStudio","text":"Les avantages de RStudio, comparativement à l’utilisation de R, sont d’offrir une gestion de la console, du script, de l’environnement des variables et des documents externes en une seule interface (et bien d’autres!). En plus de la console, RStudio permet l’édition de syntaxe qui peut être commandée ligne par ligne en utilisant CTRL + Enter (Windows) ou CMD + Enter (MacOS), ce qui s’avére fort utile lors de la programmation de fonction ou d’analyses de données. RStudio affiche également les variables en mémoire dans le menu Global Environment, ce qui permet de suivre l’état de la programmation (quelle variable existe ou n’existe pas). Enfin, RStudio affiche dans un quatrième menu les fichiers dans le directoire des fichiers R ce qui permet de voir notamment les jeux de données, mais aussi d’autres fonctions ou scripts. C’est également à cet endroit où l’aide (help) est fournie et les figures (plot) affichées.Tout le contenu du présent ouvrage est réalisable avec R ou avec RStudio. L’usage de ce dernier sera toutefois plus agréable aux lecteurs.Oh! Un autre avantage de Rstudio est qu’il peut être utiliser comme un éditeur de texte. D’ailleurs, cet ouvrage est complètement rédigé avec RStudio (avec les packages Rmarkdown et bookdown).","code":""},{"path":"commencer.html","id":"autres-options","chapter":" 1 Commencer","heading":"1.3.2 Autres options","text":"Il existe plusieurs interfaces utilisateurs graphiques (GUI pour Graphical User Interface) pour R comme R Commander (certainement l’option la plus connue en sciences humaines et sociales) ou JASP, mais aussi plusieurs autres. Ces deux options sont gratuites et libre-accès. Il existe aussi d’autres options payantes. Ces logiciels visent l’utilisation de R par une approche pointer-et-cliquer (point--click) au travers les analyses plutôt que de recourir à la syntaxe. Ces options plus intuitives pour le nouvel utilisateur ont parfois des effets limitatifs pour des analyses plus avancées et ont comme effet indésirable de promouvoir la boîte noire statistique.","code":""},{"path":"commencer.html","id":"démarrer-r-ou-rstudio","chapter":" 1 Commencer","heading":"1.4 Démarrer R ou RStudio","text":"À l’ouverture de R, illustrée à la Figure 1.1, le logiciel présente la console, une interface très rudimentaire (et assez déstabilisante pour un logiciel pourtant si promu). Une fois le logiciel ouvert, l’application offre une invitation discrète à écrire des commandes. Le symbole > au bas de la console est une invite (prompt) indiquant où taper les commandes. C’est à cette ligne de commande que les expressions sont immédiatement évaluées.\nFigure 1.1: Ouverture (effrayante!) de la console R\nÀ titre d’exemple, la console peut être utilisée comme une calculatrice glorifiée.Plusieurs nouveaux utilisateurs se limitent à ouvrir R y faire quelques calculs, puis le referme et ne l’ouvre plus jamais. Ce livre tente d’emmener le lecteur un peu plus loin.Dans la Figure 1.1, R est défini en français. Cela n’pour effet que de modifier le menu déroulant (“Fichier, Edition, etc.”) au sommet du logiciel et les différentes options de ce menu. Le fonctionnement reste le même (les fonctions ne sont pas traduites, par exemple).La console R n’étant pas un éditeur de texte, il faudra enregistrer la syntaxe utilisée lors d’une séance pour la conserver. Le logiciel offre une option d’écriture de script intégré, mais n’est pas lié directement à la console. Il faudra donc se résoudre à abuser du copier-coller ou à sourcer le script (tâche plus ardue pour les nouveaux utilisateurs). Plusieurs éditeurs de texte sont utiles ou même construits pour directement travailler avec R, le plus connu étant certainement RStudio, déjà mentionné. L’environnement intégré est beaucoup plus fonctionnel.La Figure 1.2, montre l’interface de RStudio, déjà un peu moins intimidante que celle de R. À l’ouverture de RStudio, quatre types de fenêtres sont disponibles : la console (cadran inférieur gauche), les scripts (cadran supérieur gauche), l’environnement (cadran supérieur droit) et l’affichage (cadran inférieur droit). L’emplacement de ces cadrans peut être modifié selon les désirs de l’utilisateur.\nFigure 1.2: Ouverture (moins effrayante) de RStudio\nLa console RStudio est identique à la console usuelle retrouvée avec R. Elle sert les mêmes fonctions.Le script est un éditeur de texte dans lequel de la syntaxe est rédigée, sauvegardée, manipulée, et testée. S’il n’est pas ouvert ou s’il faut ouvrir un script supplémentaire, il faut procéder par le menu déroulant File > New File > R Script ou bien CTRL + Shift + N. Il peut y avoir plusieurs scripts ouverts simultanément.L’environnement global permet de connaître les variables et fonctions maison en mémoire vive. L’onglet History montre les dernières lignes de code commandées (non affichées dans la figure - il suffit de cliquer sur l’onglet à côté de Environment).Le cadran inférieur droit montre le fichier de travail R qui contiendra ordinairement les fichiers de travail actifs (scripts, jeu de données, fonctions maison, etc.). Si aucun directoire n’est demandé explicitement par R (par exemple, si un jeu de donnée doit être téléchargé), le logiciel cherche par défaut dans le fichier actif pour télécharger les fichiers demandés. Il faut s’assurer d’être dans le bon fichier, car cela cause quelques soucis à l’occasion. RStudio permet de travailler par projet, ce qui est très pratique tant sur le plan de l’organisation du programmeur que pour la gestion des directoires de travail. Pour ouvrir un projet, il faut procéder par le menu déroulant File > New Project… et suivre les indications et ajuster selon les besoins du projet.","code":"2 + 2\n> [1] 4"},{"path":"commencer.html","id":"les-scripts","chapter":" 1 Commencer","heading":"1.5 Les scripts","text":"Ce qu’il importe le plus avec R, et ce qui fait resplendir RStudio, est de conserver la syntaxe d’une session à l’autre. Le logiciel ne le fait pas très bien. Il faudra sauvegarder dans un script les expressions et le code utilisés. Ces fichiers ont souvent comme extension “.R” et permettent de conserver, voire partager la syntaxe. Il est possible d’y ajouter des commentaires pour de futures utilisations. Tout éditeur de texte permet la sauvegarde de syntaxe, certains sont mieux que d’autres pour une utilisation avec R.RStudio contient déjà un panneau contenant un script qu’il est possible de sauvegarder et de rouler directement dans la console. Ce dernier est directement lié et il est possible de rouler la syntaxe ligne par ligne avec CTRL + Enter (Windows) ou CMD + Enter (MacOS).","code":""},{"path":"programmer.html","id":"programmer","chapter":" 2 Programmer","heading":" 2 Programmer","text":"Une fois R (ou RStudio) ouvert, qu’est-il possible de réaliser? Dans les prochaines sections, les différents éléments de programmation permettant la création et la manipulation de données sont présentés afin de dépasser le cadre de l’utilisation calculatrice.","code":""},{"path":"programmer.html","id":"les-variables","chapter":" 2 Programmer","heading":"2.1 Les variables","text":"Pour manipuler des données, il faut recourir à des variables. Pour attribuer une valeur à une variable, il faut assigner cette valeur avec <- (ALT + -) ou =, par exemple,où est maintenant égale à 2. La première ligne assigne (enregistre) la valeur 2 à . La deuxième ligne, indique à la console R d’imprimer le résultat pour le voir. Par la suite, peut être utilisée dans des fonctions, des calculs ou analyses plus complexes. De surcroît, peut devenir une fonction, une chaîne de caractère (string) ou un jeu de données.Conventionnellement, les puristes de R recommandent l’usage de <- plutôt que = pour l’assignation. Il y quelques nuances computationnelles entre les deux, mais qui échappent irrémédiablement aux néophytes et même aux usagers intermédiaires. Par tradition, <- prévaudra dans ce livre.Pour nommer des variables, seuls les caractères alphanumériques peuvent être utilisés ainsi que le tiret bas _ et le .. Les variables ne peuvent commencer par un nombre.Réassigner une valeur à une variable déjà existante écrase la valeur précédente.La sortie produit 3 et non plus 2.Cette remarque est importante, car elle signifie que des fonctions sont écrasables en nommant des variables. Il faut ainsi éviter de nommer des variables avec des fonctions utilisées par R, notamment l’utilisation des noms suivants.Certains mots sont tout simplement interdits d’utilisation.","code":"a <- 2\na\n> [1] 2a <- 2\na <- 3\na\n> [1] 3\nc; q; t; C; D; I; T; F; pi; mean; var; sd; length; diff; repTRUE; FALSE; break; for; in; if; else; \nwhile; function; Inf; NA; NaN; NULL"},{"path":"programmer.html","id":"les-opérateurs-arithmétiques","chapter":" 2 Programmer","heading":"2.2 Les opérateurs arithmétiques","text":"La première utilisation d’un nouvel usager de R est généralement d’y recourir comme calculatrice. Les opérateurs arithmétiques de base comme l’addition +, la soustraction -, la multiplication *, la division / , et l’exposant ^ sont intuitivement disponibles.Évidemment, ces opérateurs fonctionnent sur des variables numériques.Les deux premières lignes assignent des valeurs à et b, puis la troisième imprime la division. L’absence de marqueur <- ou = indique à R d’imprimer la réponse dans la console. Si le résultat / b est assigné à une variable, alors aucun résultat n’est affiché, bien que la variable contienne la réponse.Il n’y aucune réponse d’affichée. Maintenant, si la variable resultat est demandée, R affiche son contenu.D’autres fonctions sont aussi très utiles. Par exemple, la racine carrée sqrt() (qui n’est rien d’autre que ^(.5)) et le logarithme naturel log(). Il suffit d’insérer une variable ou une valeur à l’intérieur d’une de ces fonctions pour en obtenir le résultat.","code":"2 + 2\n> [1] 4\n1 / 3\n> [1] 0.333\n2 * 3 + 2 ^ 2\n> [1] 10a <- 1\nb <- 10\na / b\n> [1] 0.1\nresultat <- a / bresultat\n> [1] 0.1sqrt(4)\n> [1] 2\n4^(1/2)\n> [1] 2\nlog(4)\n> [1] 1.39"},{"path":"programmer.html","id":"les-commentaires","chapter":" 2 Programmer","heading":"2.3 Les commentaires","text":"Les scripts R peuvent contenir des commentaires. Ceux-ci sont désignés par le désormais célèbre #. Une ligne de script commençant par ce symbole est ignorée par la console. Ces commentaires permettent aussi bien de préciser différentes étapes d’un script, que d’expliquer la nomenclature des variables ou encore d’expliquer une fonction, ses entrées, ses sorties. Les commentaires sont extrêmement utiles, car les annotations peuvent souvent sauver énormément de temps et d’effort lors d’utilisations ultérieures.Dans cet exemple, la première ligne est ignorée. Autrement, la console R produit une erreur, car cette ligne est pour le logiciel pur charabia!","code":"# La variable resultat est le quotient des variables a et b\nresultat <- a / b\nresultat\n> [1] 0.1"},{"path":"programmer.html","id":"les-chaînes-de-caractère","chapter":" 2 Programmer","heading":"2.4 Les chaînes de caractère","text":"La plupart du temps, les variables utilisées sont numériques, c’est-à-dire qu’elles contiennent des nombres. Parfois, les données sont des mots, c’est-à-dire, des chaînes de caractères (string). Les chaînes de caractères sont définis par le double apostrophe \"...\", où remplace les trois points par les mots désirés.1","code":"titre <- \"Bonjour tout le monde!\"\ntitre\n> [1] \"Bonjour tout le monde!\""},{"path":"programmer.html","id":"concaténer","chapter":" 2 Programmer","heading":"2.5 Concaténer","text":"Par défaut, R ne peut assigner qu’une valeur à une variable. Pour grouper des éléments ensemble, c’est-à-dire, pour créer des jeux de données, des vecteurs, des matrices, des listes, il faudra utiliser des fonction de concaténation, dont voici une liste des plus utiles avec quelques exemples, de la plus stricte (vecteur) à la plus flexible (liste).","code":""},{"path":"programmer.html","id":"créer-un-vecteur","chapter":" 2 Programmer","heading":"2.5.1 Créer un vecteur","text":"Une fonction fort utile permet de joindre des valeurs dans une seule variable. Précédemment, l’assignation d’une valeur à des variables se limitait à une chaîne de caractères ou à une valeur numérique. La fonction concaténer c() (ou combiner, créer) met plusieurs éléments (deux ou plus) dans une seule variable. Son est de vectoriser les arguments. Chaque élément est délimité par une virgule ,.Elle fonctionne également avec les chaînes de caractères.Et les deux.La fonction c() est strict sur les arguments, car elle leur accorde le même attribut. Par exemple, phrase ne contient que des chaînes de caractères. Les valeurs 1 et 2 ont perdu leur classe numérique (elles ne sont plus utilisables comme nombre2). Cela se remarque par les guillemets anglophones autour des valeurs \"1\" et \"2\" imprimées.Il faudra également faire attention aux arguments passés à la fonction c(), car celle-ci vectorise les arguments. Autrement dit, elle crée des vecteurs (une ligne en quelque sorte) avec les entrées fournies, peu importe leur structure de départ. Par exemple, un jeu de données passant par c() devient une seule ligne (un seul vecteur) de valeurs. Les fonctions cbind() et rbind() permettent de joindre des colonnes et des lignes, respectivement.","code":"valeurs <- c(-5, 5)\nvaleurs\n> [1] -5  5texte <- c(\"Bonjour\", \"tout\", \"le\", \"monde\")\ntexte\n> [1] \"Bonjour\" \"tout\"    \"le\"      \"monde\"phrase <- c(1, \"Chat\", 2, \"Souris\")\nphrase\n> [1] \"1\"      \"Chat\"   \"2\"      \"Souris\""},{"path":"programmer.html","id":"créer-une-matrice","chapter":" 2 Programmer","heading":"2.5.2 Créer une matrice","text":"La fonction matrix() crée des matrices, comme des matrices de covariances, par exemple. La fonction utilise trois arguments, une matrice de nombre à entrer dans la matrice, un nombre de colonnes et un nombre de lignes. La fonction utilise le recyclage, ce qui est utile à certaines occasions.Les matrices sont une formes de jeu de données dans lequel tous les éléments partagent le même attribut (tous numériques, caractères, logiques, etc.).Une note devancée sur l’utilisation de 1:3 et 1:16 du code précédent qui permettent de générer des séquences de nombre simplement.","code":"# Une matrice de 0 de taille 3 x 3\nmatrix(0, ncol = 3, nrow = 3)\n>      [,1] [,2] [,3]\n> [1,]    0    0    0\n> [2,]    0    0    0\n> [3,]    0    0    0\n\n# Une matrice contenant les nombres 1:3 pour une matrice 3 x 3\nmatrix(1:3, ncol = 3, nrow = 3)\n>      [,1] [,2] [,3]\n> [1,]    1    1    1\n> [2,]    2    2    2\n> [3,]    3    3    3\n\n# Si la séquence préférée est de gauche à droite plutôt\n# que de bas en haut\nmatrix(1:3, ncol = 3, nrow = 3, byrow = TRUE)\n>      [,1] [,2] [,3]\n> [1,]    1    2    3\n> [2,]    1    2    3\n> [3,]    1    2    3\n\n# Une matrice avec un nombre d'entrées égale au nombre de cellules\nmatrix(1:16, ncol = 4, nrow = 4)\n>      [,1] [,2] [,3] [,4]\n> [1,]    1    5    9   13\n> [2,]    2    6   10   14\n> [3,]    3    7   11   15\n> [4,]    4    8   12   16"},{"path":"programmer.html","id":"créer-un-jeu-de-données","chapter":" 2 Programmer","heading":"2.5.3 Créer un jeu de données","text":"Un jeu de données (data.frame) est un peu comme l’extension de la matrice. La différence étant que les éléments entre les colonnes peuvent partager des attributs différents. Ainsi chaque ligne représente une unité (un participant, un objet) et chaque colonne représente une dimension (informations ou variable) différente de cette objectif. La fonction data.frame() permet de créer de tel objet. La fonction prend comme un argument une série de vecteurs. Des noms peuvent être attribués au colonnes qui correspondent à des variables.En utilisant nom.de.variable = vecteur à l’intérieur de data.frame(), les noms des colonnes deviennent nom.de.variable. Cela permettra une plus grande flexibilité lorsqu’il faudra [gérer] et manipuler les données.Comme les matrices, les jeux de données ont aussi une restriction. Alors que les jeux de données libèrent la contrainte d’avoir des objets de même attributs entre les colonnes (variables), ils doivent être créés avec des vecteurs de même longueur. Autrement dit, chaque colonne doit avoir exactement le même nombre de lignes. Parfois, R *recycle/ pour combler les éléments. Il faut donc porter une attention particulière afin de vérifier si c’est bien l’intention de l’utilisateur ou non.","code":"# Quelques variables\nvar1 <- c(\"Éloi\", \"Laurence\")\nvar2 <- c(6, 3)\nvar3 <- c(TRUE, TRUE)\n\n# Entrer de trois vecteurs non identifiés\njd1 <- data.frame(var1, var2, var3)\n\n# Entrer de trois vecteurs identifiés\njd2 <- data.frame(nom = var1, age = var2, enfant = var3)\n\n# Comparer\njd1 ; jd2\n>       var1 var2 var3\n> 1     Éloi    6 TRUE\n> 2 Laurence    3 TRUE\n>        nom age enfant\n> 1     Éloi   6   TRUE\n> 2 Laurence   3   TRUE"},{"path":"programmer.html","id":"créer-une-liste","chapter":" 2 Programmer","heading":"2.5.4 Créer une liste","text":"Une troisième option pour stocker de informations dans une seule variable est d’avoir recourt aux listes. La liste libère à la fois l’utilisateur des objets de mêmes attributs et de même longueur. Ainsi, une liste, peut contenir des vecteurs, des matrices, des jeux de données et même d’autres listes.Pour créer une liste, il faut utiliser la fonction list(). Comme data.frame(), des noms de colonnes peuvent être donnés pour chaque liste pour faciliter la manipulation ultérieure de la liste.L’utilisation de listes est une caractéristique prédominante avec R. Par exemple, R ne peut sortir qu’une variable par fonction. Si la fonction doit retourner plusieurs éléments, ceux-ci doivent se retrouver dans une liste. Ce qui restera plus nébuleux pour le lecteur, c’est que l’optimisation de R se fait par listes. Cela sera noté aux moments appropriés.","code":"#Quelques variables\nvar1 <- c(\"chat\", \"chien\")\nvar2 <- 1:10\n\n# Entrer de deux vecteurs non identifiés\njd1 <- list(var1, var2)\n\n# Entrer de deux vecteurs identifiés\njd2 <- list(animal = var1, nombre = var2)\n\n# Comparer\njd1 ; jd2\n> [[1]]\n> [1] \"chat\"  \"chien\"\n> \n> [[2]]\n>  [1]  1  2  3  4  5  6  7  8  9 10\n> $animal\n> [1] \"chat\"  \"chien\"\n> \n> $nombre\n>  [1]  1  2  3  4  5  6  7  8  9 10"},{"path":"programmer.html","id":"référer-à-des-sous-éléments","chapter":" 2 Programmer","heading":"2.6 Référer à des sous-éléments","text":"Avec des variables contenant plusieurs valeurs, il peut être utile de référer à une seule valeur ou un ensemble de valeurs de la variable. Les crochets [] à la suite du nom d’une variable permettent d’en extraire les valeurs désirées sans tout sortir l’ensemble.Dans le premier exemple, seul un élément est demandé. Dans le deuxième exemple, la commande 1:3 produit la série de \\(1,2,3\\) et en extrait ces nombres. Dans le dernier exemple, la fonction c() est astucieusement utilisée pour extraire les éléments \\(2\\) et \\(4\\). Le quatrième exemple montre comment retirer un élément en utilisant des valeurs négatives et le cinquième exemple montre comment retirer des éléments.La section Manipulation de données montre comment référer à des sous-éléments de jeux de données, de matrices et de listes de façon plus avancées.","code":"# Un exemple de vecteur\nphrase <- c(1, \"Chat\", 2, \"Souris\")\n\n# Extraire le premier élément de la variable phrase\nphrase[1]\n> [1] \"1\"\n\n# Extraire les éléments 1, 2 et 3\nphrase[1:3]\n> [1] \"1\"    \"Chat\" \"2\"\n\n# Extraire les éléments 2 et 4\nphrase[c(2,4)]\n> [1] \"Chat\"   \"Souris\"\n\n# Ne pas extraire l'élément 1\nphrase[-1]\n> [1] \"Chat\"   \"2\"      \"Souris\"\n\n# Ne pas extraire les éléments 1 et 3\nphrase[-c(1, 3)]\n> [1] \"Chat\"   \"Souris\""},{"path":"programmer.html","id":"les-packages","chapter":" 2 Programmer","heading":"2.7 Les packages","text":"L’utilisation de packages (souvent nommées bibliothèques, modules, paquets ou paquetage en français - ici, l’usage de package est maintenu) est l’attrait principal de R. Pour éviter l’anglicisme, Antidote (Druide informatique INC., 2022) suggère forfait, achat groupé ou progiciel (ce dernier étant certainement le terme approprié).Les packages sont de regroupement de fonctions. C’est certainement l’aspect qui le plus contribué au succès et à sa dissémination de R. Il s’agit de la mise en commun d’un effort collaboratif afin de créer des fonctions et de les partager librement entre les usagers. Le téléchargement de base de R offre déjà quelques packages rudimentaires (comme base qui offre des fonctions comme sum() ou stat qui offre des fonctions comme mean() et var()), mais qui suffisent rarement lorsque des analyses plus avancées ou plus spécialisées sont nécessaires.L’une des forces des packages est qu’ils sont fournis généralement avec un bon manuel d’utilisation. Plusieurs contributeurs leur sont associés (avec un responsable). Ils sont maintenus régulièrement. Le soutien des responsables est parfois aisé à obtenir et les auteurs de ces packages sont motivés à maintenir les packages opérationnels et aux bénéfices de tous. La faiblesse des packages est qu’il s’agit malheureusement de généralement. Il arrive que certains packages produisent des erreurs de calcul, qu’ils soient laissés en désuétude par leurs auteurs, qu’ils aient migrés sous une autre forme, ou que de meilleures options soient disponibles sans aucune notice. Cela va sans dire, ce problème concerne les logiciels traditionnels également. Il s’agit toutefois d’un enjeu moindre, car les packages sont souvent recommandés par des collègues, des autorités dans leur domaine respectif ou des ouvrages de référence, ce qui aura comme tendance de promouvoir les meilleurs packages. Pas toujours. Il faut rester critique et ne pas de laisser tromper par une boîte noire.Une dernière faiblesse : les packages agissent parfois en boîte noire, c’est-à-dire qu’ils court-circuitent la réflexion de l’utilisateur qui leur fait confiance. Il peut être parfois difficile de savoir ce que les fonctions produisent exactement. Au contraire des logiciels traditionnels, ces boîtes noires peuvent dans la plupart des cas être accessibles directement, elles sont liés en plus à des articles scientifiques ou de la documentation qui permet dans comprendre les tenants et aboutissants.","code":""},{"path":"programmer.html","id":"installer-des-packages","chapter":" 2 Programmer","heading":"2.7.1 Installer des packages","text":"Pour installer un package, il faut utiliser la fonctionoù les \"...\" doivent être remplacé par le nom du package. Il est important de bien inscrire le nom du package entre guillemet anglophone. Il est aussi possible de sélectionnerTools;\nInstall Packages…puis de nommer le package sous l’onglet package. Avec R il faudra auparavant choisir un miroir (sélectionner un pays), ce qui n’est pas nécessaire avec RStudio. Une fois téléchargé, il n’est plus nécessaire de refaire cette étape à nouveau, à l’exception de potentielles et ultérieures mises à jour lorsqu’elles devront être effectuées.","code":"\ninstall.packages(\"...\")"},{"path":"programmer.html","id":"appeler-un-package","chapter":" 2 Programmer","heading":"2.7.2 Appeler un package","text":"Ce qui n’est pas des plus intuitif avec R, c’est qu’une fois le package téléchargé, il n’est pas directement utilisable. Il faut d’abord l’appeler avec la fonction library().Cette étape doit être faite à chaque ouverture de R. Cela permet de ne pas mettre en mémoire trop de package simultanément. Il importe d’indiquer tous les packages utilisés en début de script sans quoi des erreurs, comme l’absence de fonctions, sont produites.Une technique à laquelle l’utilisateur peut avoir recourt lorsqu’il souhaite n’utiliser qu’une fonction spécifique d’un package est l’utilisation des :: débutant par le nom du package suivi par le nom de la fonction, comme MASS::mvrnorm(). La fonction s’utilise de façon usuelle. En utilisant ::, il n’est pas nécessaire d’appeler le package avec la fonction library(). Il faut toute fois que le package soit bel et bien installer.","code":"\nlibrary(\"...\")"},{"path":"programmer.html","id":"les-fonctions","chapter":" 2 Programmer","heading":"2.8 Les fonctions","text":"R offre une multitude de fonctions et permet également à l’usager de bâtir ses propres fonctions (fonctions maison). Elles permettent d’automatiser des calculs (généralement, mais peut faire beaucoup plus!). Tout au long de cet ouvrage, les fonctions sont identifiées par l’ajout de parenthèse à leur fin, comme ceci : function(). Ces fonctions ont généralement la forme suivante.Ici, nom est le nom auquel la fonction sera référée par la suite, function est la fonction R qui permet de créer la fonction maison, argument1 et argument2 sont les arguments (les entrées) fournis à la fonction et à partir desquels les calculs sont réalisés, et les accolades {} définissent le début et la fin de la fonction dans le script.Il est bien utile de créer ses propres fonctions bien que R possède une pléthore de fonctions et de packages en contenant encore plus. Toutes les fonctions, qu’elles soient maisons ou déjà intégrées, respectent le même fonctionnement, ce pour quoi il est utile de s’y pencher. Les fonctions maison permettent d’automatiser certains calculs qui sont propres à résoudre les problèmes spécifiques de l’utilisateur et d’être réutilisé ultérieurement.Voici un exemple trivial de fonction, soit la somme de deux nombres.Par défaut, une fonction retourne la dernière ligne calculée si elle n’est pas assignée à une variable. Si le résultat d’une fonction est assigné, la fonction ne retourne pas le résultat dans la console, mais assigne bel et bien la variable.L’utilisation de return() à la fin de la fonction est une bonne pratique, car elle permet d’éviter des problèmes ou des ambiguïtés.","code":"\nnom <- function(argument1, argument2, ...) {\n  \n  # Calcul à réaliser\n  \n}addition <- function(a, b) {\n  \n  a + b\n  \n}\n\naddition(2,3)\n> [1] 5addition2 <- function(a, b) {\n  # Le résultat est assigné à une variable\n  somme <- a + b\n}\n\n# Ne produit pas de sortie\naddition2(100, 241)\n\n# Comme il y a assignation, total n'est pas affichée\ntotal <- addition2(100, 241)\n\n# En roulant total, la sortie affiche bien la sortie de addition2()\ntotal\n> [1] 341addition3 <- function(a, b) {\n  \n  # Le résultat est assigné à une variable\n  somme <- a + b\n  \n  return(somme)\n}\n\n# Les deux fonctions produisent une sortie\naddition3(4, 6)\n> [1] 10\ntotal <- addition3(4, 6)\ntotal\n> [1] 10"},{"path":"programmer.html","id":"définir-une-boucle","chapter":" 2 Programmer","heading":"2.9 Définir une boucle","text":"Pour automatiser certains calculs, il peut être utile de recourir à une boucle (loop) qui permet de répéter plusieurs fois une même opération. Voici l’anatomie d’une boucle.L’élément est la fonction déclarant la boucle. Les renseignements sur les itérations se retrouvent entre les parenthèses. La variable prendra successivement tous les éléments dans () le vecteur à gauche (vec). Tout le contenu de la boucle (ce qui est répété) se retrouve entre les accolades {}, c’est ce qui est calculé à chaque boucle. Dans cet exemple, la boucle se répète \\(k\\) fois, soit de \\(1,2,3,...,k\\), à cause de l’expression 1:k qui correspond à générer un vecteur de \\(1\\) à \\(k\\) (voir la section La séquence). La variable quant à elle change de valeur à chaque itération. Elle prend tour à tour les valeurs \\(1,2,3,...,k\\). La variable peut judicieusement être utilisée dans la boucle afin de profiter ce comportement, notamment pour le classement des résultats. Lorsque la boucle atteint \\(k\\), elle se termine.Il est aussi possible de rédiger la boucle en utilisant uniquement k. Alors, prendra toutes les valeurs contenues dans k. La longueur du vecteur k définit le nombre d’itérations.","code":"\nfor(i in vec){\n  # Calcul désiré\n}\nfor(i in k){\n  # Calcul désiré\n}"},{"path":"programmer.html","id":"les-clauses-conditionnelles","chapter":" 2 Programmer","heading":"2.10 Les clauses conditionnelles","text":"Pour réaliser des opérations sous certaines conditions ou opérer des décisions automatiques, il est possible d’utiliser des arguments conditionnels avec des opérateurs logiques. Par exemple, sélectionner des unités ayant certaines caractéristiques, comme les participants ayant 18 ans et moins, les personnes ayant un trouble du spectre de l’autiste, ou encore par sexe. Il est aussi possible d’utiliser les opérateurs pour définir à quelle condition telle ou telle autre fonction doit être utilisée. Il faudra alors utiliser les arguments logiques.\nTable 2.1: Symboles logiques et leur signification\nR teste si les valeurs de la variable correspondent à l’opérateur logique en les déclarant comme vraies (TRUE) ou fausses (FALSE).Cela peut être utilisé pour référer à des sous-éléments comme abordés précédemment.Ici, toutes les valeurs vraies de l’opérateur logique sont rapportées.Les opérateurs logiques servent également à définir des opérations conditionnelles. La fonction est alors utilisée. Il y trois principales formes : (Si ceci, alors cela), le  else (Si ceci, alors cela, sinon autre chose) et les échelles else else.L’anatomie d’une fonction comporte d’abord la fonction . L’argument entre parenthèses à sa plus simple expression doit être vérifié par vrai (TRUE) ou faux (FALSE). Si l’argument est vrai, alors le calcul désiré est réalisé, autrement le logiciel ignore le calcul de la fonction entre accolades {}.Il est possible d’élaborer cette logique avec la fonction else qui permet de spécifier une suite à la fonction si l’argument est faux (FALSE).Enfin, il est possible d’élaborer un arbre de décision avec toute une échelle de conditionnels.L’arbre de décision peut devenir aussi compliqué que l’utilisateur le désire : chacune des branches peut contenir autant de ramifications que nécessaire.Il peut arriver pour certaines fonctions de devoir spécifier si certains paramètres sont vrais (TRUE) ou faux (FALSE) ou de définir des variables ayant ces valeurs. Lorsque c’est le cas, il est toujours recommandé d’écrire les valeurs logiques tout au long comme TRUE et FALSE, même si R reconnaît T et F, car ces dernières peuvent être réassignées, contrairement aux premières.","code":"valeurs <- 1:6\n# Toutes les valeurs plus grandes que 3.\nvaleurs > 3\n> [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE# Toutes les valeurs plus grandes que 3.\nvaleurs[valeurs > 3]\n> [1] 4 5 6\nif(x){\n  # Opération désirée\n}x <- -2\nif(x < 0){\n  print(\"la valeur est négative\")\n}\n> [1] \"la valeur est négative\"x <- 2\nif(x < 0){\n  print(\"la valeur est négative\")\n}else{\n  print(\"la valeur est positive\")\n}\n> [1] \"la valeur est positive\"x <- 0\nif(x < 0){\n  print(\"la valeur est négative\")\n}else if(x > 0){\n  print(\"la valeur est positive\")\n}else{\n  print(\"la valeur est égale à 0\")\n}\n> [1] \"la valeur est égale à 0\""},{"path":"programmer.html","id":"obtenir-de-laide","chapter":" 2 Programmer","heading":"2.11 Obtenir de l’aide","text":"En utilisant help(nom) ou ?nom, où il faut remplacer nom par le nom d’une fonction ou d’un package, R offre de la documentation. Les fonctions d’aide retournent une page de documentation contenant généralement de l’information sur les entrées et les sorties des fonctions. Certaines sont mieux détaillées que d’autres, tout dépendant de leurs créateurs et des personnes qui maintiennent ces fonctions.Il existe également la fonction ??nom qui produit une liste de toutes fonctions R ayant partiellement l’inscription introduite à la place de nom. Aussi, example(nom) produit un exemple d’une fonction.","code":"\n# Obtenir de l'aide pour la fonction help()\n?help"},{"path":"programmer.html","id":"en-cas-de-pépins","chapter":" 2 Programmer","heading":"2.12 En cas de pépins","text":"Il arrive parfois que le code utilisé ne fonctionne pas, que des erreurs se produisent ou que des fonctions fort utiles demeurent inconnues. Même après plusieurs années d’utilisation, les utilisateurs font encore quotidiennement des erreurs (au moins une!). Un excellent outil est d’utiliser un moteur de recherche, de poser une question à l’aide de quelques mots clés bien choisis, préférablement en anglais, et en y inscrivant “R” ou “R” ou “R”. La plupart du temps, les programmeurs de packages ont une solution sur leur site ou leurs instructions de packages. Il y aussi des plateformes publiques et en ligne, comme StackOverflow, qui collectent questions et réponses sur le codage. D’autres utilisateurs peuvent avoir posé la même question et des auteurs de programmes R et d’autres usagers y ont répondu aux bénéfices de tous. Dans le cas d’une solution introuvable, ces mêmes plateformes permettent de poser de nouvelles questions. Il faudra toutefois attendre qu’un usager plus expérimenté prenne le temps d’y répondre.","code":""},{"path":"calculer.html","id":"calculer","chapter":" 3 Calculer","heading":" 3 Calculer","text":"Dans cette section, les fonctions essentielles couramment utilisées sont présentées en rafale. L’accent est mis sur la définition de la fonction (qu’est-ce qu’elle fait?) et son utilité (à quoi sert-elle?). Pour les fonctions essentielles de nature statistiques (moyennes, médianes, etc.), cette section développe une fonction maison (rédigée par l’utilisateur pour la mettre en pratique) et identifie la fonction déjà implantée en R.","code":""},{"path":"calculer.html","id":"la-longueur","chapter":" 3 Calculer","heading":"3.1 La longueur","text":"La longueur d’une variable correspond au nombre d’éléments qu’elle contient. La fonction length() permettra d’obtenir ce résultat. Ce sera particulièrement utile lorsqu’il faudra calculer, par exemple, le nombre de boucle à réaliser à partir des éléments d’un vecteur ou la taille d’échantillon (le nombre d’unités d’observation d’une variable).La somme d’une chaîne de caractères est toujours de \\(1\\), peu importe le nombre de caractères. La fonction nchar() produira le nombre de caractères.Une variable qui existe, mais qui ne contient pas de valeur aura une longueur égale à \\(0\\). Ce type de variable est utile lorsqu’il faut créer une variable dont la taille sera altérée.Pour les matrices et les jeux de données, ncol() (nombre de colonnes) et nrow() (nombre de lignes) sont plus efficaces et intuitives.","code":"x <- c(1, 2, 3)\nlength(x)\n> [1] 3\n\ny <- \"Bonjour tout le monde!\"\nlength(y)\n> [1] 1\n\nnchar(y)\n> [1] 22"},{"path":"calculer.html","id":"la-répétion","chapter":" 3 Calculer","heading":"3.2 La répétion","text":"La fonction rep() est utile pour répéter volontairement des valeurs. Il y trois possibilités de répétitions: l’argument times définit le nombre de fois que le vecteur est répété; l’argument définit le nombre de fois que chaque élément est répété; l’argument length.précise le nombre d’éléments de la sortie. Plusieurs combinaisons de ces arguments sont possibles.","code":"vec <- c(2, 4, \"chat\")\n\n# Répéter `vec` trois fois\nrep(vec, times = 3)\n> [1] \"2\"    \"4\"    \"chat\" \"2\"    \"4\"    \"chat\" \"2\"    \"4\"   \n> [9] \"chat\"\n\n# Répéter chaque élément de `vec` trois fois\nrep(vec, each = 3)\n> [1] \"2\"    \"2\"    \"2\"    \"4\"    \"4\"    \"4\"    \"chat\" \"chat\"\n> [9] \"chat\"\n\n# Répéter chaque élément de `vec` d'une longueur de 8\nrep(vec, length.out = 8)\n> [1] \"2\"    \"4\"    \"chat\" \"2\"    \"4\"    \"chat\" \"2\"    \"4\"\n\n#  Répéter chaque élément 3 fois à 2 reprises\nrep(vec, times = 2, each = 3)\n>  [1] \"2\"    \"2\"    \"2\"    \"4\"    \"4\"    \"4\"    \"chat\" \"chat\"\n>  [9] \"chat\" \"2\"    \"2\"    \"2\"    \"4\"    \"4\"    \"4\"    \"chat\"\n> [17] \"chat\" \"chat\""},{"path":"calculer.html","id":"la-séquence","chapter":" 3 Calculer","heading":"3.3 La séquence","text":"Une première fonction pour créer des séquences de nombres est l’utilisation de : avec un nombre avant et après la ponctuation.Pour plus de malléabilité, la fonction seq() génère une séquence régulière de valeurs. Les arguments sont seq(= , = , = ) traduisibles par de , à, par. Les arguments par défaut seront très utiles pour simplifier l’écriture; La fonction commence ou termine la séquence par 1 et fera des bonds de 1 entre les valeurs. Un autre argument est la longueur de la sortie length.qui spécifie le nombre d’éléments que devra comporter le vecteur de sortie.","code":"# 1:3 équivaut à c(1, 2, 3)\n1:3\n> [1] 1 2 3# Une séquence de 1 (défaut, from = 1) à 10\nseq(10)\n>  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Une séquence de 1 (défaut, from = 1) à -10\nseq(-10)\n>  [1]   1   0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10\n\n# Une séquence de -10 (défaut, from = 1) à 1\nseq(from = -10, to = 1)\n>  [1] -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1\n\n# Une séquence de nombres paires (from = 2, to = 10, by = 2)\nseq(2, 10, 2)\n> [1]  2  4  6  8 10\n\n# Une séquence de nombres paires \nseq(from = 2, by = 2, length.out = 5)\n> [1]  2  4  6  8 10"},{"path":"calculer.html","id":"la-somme","chapter":" 3 Calculer","heading":"3.4 La somme","text":"Il est possible de calculer des sommes de variables pour en obtenir le total. En tant qu’humain, le calcul d’une série de nombre correspond à prendre chaque nombre et de les additionner un à un. La fonction suivante reproduit assez bien ce qu’un humain ferait (avec ses quelques caprices de programmation tel que devoir déclarer l’existence de la variable de total et spécifier le nombre d’éléments à calculer).À noter que l’utilisation de la boucle est à des fins illustratives seulement. En termes de rendement computationnel, elle est bien peu efficace. Il faudra privilégier la fonction sum() pour calculer le total de son entrée.Il faut prendre garde : R calcule le total de tous les éléments de l’entrée sans égard aux lignes et aux colonnes. Autrement dit, il vectorise les entrées. Si deux variables étaient entrées par inadvertance, alors R calculerait la somme de ces deux variables plutôt que de retourner deux totaux. À cette fin, les fonctions rowSums() et colSums() seront utiles lorsqu’il faudra calculer des sommes sur des lignes (row) ou des colonnes (col).","code":"somme <- function(x){\n  # La taille du vecteur `x`\n  n <- length(x)\n  \n  # Définir une variable nulle\n  total <- 0\n  \n  # Boucle pour additionner chaque élément\n  for(i in 1:n){\n    \n    # Prendre le ie élément et l'additionner\n    # au total des (i-1)e éléments précédents\n    total <- total + x[i]\n  }\n  # Retourner le total après la boucle\n  return(total)\n}\n\n# Pour tester\nx <- c(1,2,3,4,5,-6)\nsomme(x)\n> [1] 9\nsum(x)\n> [1] 9"},{"path":"calculer.html","id":"la-moyenne","chapter":" 3 Calculer","heading":"3.5 La moyenne","text":"La moyenne est une mesure de tendance centrale qui représente le centre d’équilibre d’une distribution (un centre de gravité en quelque sorte). Si le poids d’un des côtés d’une distribution de probabilité était altéré (plus lourde ou plus légère), alors la moyenne se déplacerait relativement vers cette masse.La moyenne d’un échantillon correspond à la somme de toutes les unités d’une variable divisée par le nombre de données de cette variable ou, mathématiquement, \\[\\bar{x}=\\frac{\\Sigma_{=1}^n x}{n}\\] où \\(x\\) est la variable, \\(n\\) est le nombre d’unité et \\(\\Sigma_i^n\\) représente la somme de toutes les unités de \\(x\\). R possède déjà une fonction permettant de calculer la moyenne sans effort, mean() où l’argument est la variable. Il est possible de développer une fonction maison pour calculer la moyenne commeoù sum(x) calculer la somme de toutes les unités de x, / permet la division et length(x) calcule le nombre d’unités du vecteur x. Par exemple, à partir d’une variable x, les fonctions suivantes donnent le même résultat. Par contre la fonction mean() est beaucoup plus robuste que cette dernière équation.Comme pour sum(), les fonctions rowMeans() et colMeans() seront utiles lorsqu’il faudra calculer des moyennes sur des lignes (row) ou des colonnes (col).3","code":"\nx_bar <- sum(x)/length(x)# Un vecteur\nx <- c(0, 1, 2, 3, 4, 5)\n\n# Comparaison\nmean(x)\n> [1] 2.5\nsum(x)/length(x)\n> [1] 2.5"},{"path":"calculer.html","id":"la-médiane","chapter":" 3 Calculer","heading":"3.6 La médiane","text":"La médiane d’un échantillon correspond à la valeur où \\(50\\%\\) des données se situe au-dessous et au-dessus de cette valeur. C’est la valeur au centre des autres (lorsqu’elles sont ordonnées). Quand le nombre de données est impair, le \\(\\frac{(n+1)}{2}\\)e élément est la médiane. Quand le nombre est pair, la moyenne des deux valeurs au centre correspond à la médiane. Cette statistique est intéressante comme mesure de tendance centrale, car elle est plus robuste aux valeurs aberrantes (moins sensibles) que la moyenne.Évidemment, R offre déjà une fonction median() pour réaliser le calcul. Il est toutefois possible de programmer une fonction maison. Il faut utiliser la fonction sort() pour ordonner les données (croissant par défaut).L’expression n%%2, lue \\(n \\bmod 2\\), joue astucieusement le rôle de vérifier si n est impaire. La formule générale \\(x \\bmod y\\) représente une opération binaire associant à deux entiers naturels le reste de la division du premier par le second. Par exemple, \\(60 \\bmod 7\\), noter 60%%7 dans R, donne \\(4\\) soit le reste de \\(7*8 + 4 = 60\\). Le logiciel le confirme.Il s’agit d’une technique de programmation très pratique. Dans le cas de n%%2, la formule donne \\(1\\) dans le cas d’un nombre impair ou \\(0\\) dans le cas d’un nombre pair, puis teste ce résultat pour déterminer s’il réalise s[(n+1)/2] lorsque n%%2==1(TRUE) , ce qui correspond à choisir l’élément au centre d’un vecteur de taille impair, ou bien mean(s[n/2+0:1] lorsque n%%2==0(FALSE) , ce qui correspond à choisir les deux éléments au centre d’un vecteur pair et d’en faire la moyenne. Il s’agit de l’une des nombreuses façons selon lesquelles il est possible de programmer la médiane.","code":"mediane <- function(x) {\n  # Longueur du vecteur\n  n <- length(x)\n  \n  # Ordonner le vecteur\n  s <- sort(x)\n  # Vérifier si la longueur est paire ou impaire et\n  # alors calculer la valeur médiane correspondante\n  ifelse(n%%2 == 1, s[(n + 1) / 2], mean(s[n / 2 + 0:1]))\n}\n\n# Un vecteur\nx <- c(42, 23, 53, 77, 93, 20, 37, 24, 60, 62)\n\n# Comparaison\nmedian(x)\n> [1] 47.5\nmediane(x)\n> [1] 47.560%%7\n> [1] 4"},{"path":"calculer.html","id":"la-variance","chapter":" 3 Calculer","heading":"3.7 La variance","text":"La variance d’un échantillon est une mesure de dispersion. Elle représente la somme des écarts (distances) par rapport à la moyenne au carré divisée par la taille d’échantillon moins \\(1\\). Mathématiquement, il s’agit de l’équation (3.1).\\[\ns^2 = \\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2\n\\tag{3.1}\n\\]\nIl est assez aisé d’élaborer une fonction pour réaliser se calculer avec les fonctions déjà abordées.La variance peut aussi être calculée plus efficacement avec la fonction R var().","code":"\nvariance <- function(x){\n  # Longueur du vecteur\n  n <- length(x)\n  \n  # Moyenne du vecteur\n  xbar <- mean(x)\n  \n  # La variance\n  variance <- sum((x - xbar) ^ 2)/(n - 1)\n  return(variance)\n}# Un vecteur\nx <- c(26, 6, 40, 36, 14, 3, 21, 48, 43, 2)\n\n# Comparaison\nvariance(x)\n> [1] 300\nvar(x)\n> [1] 300"},{"path":"calculer.html","id":"lécart-type","chapter":" 3 Calculer","heading":"3.8 L’écart type","text":"L’écart type d’un échantillon représente la racine carrée de la variance. Elle une interprétation plus intuitive en tant que mesure de la moyenne des écarts par rapport à la moyenne. Si le calcul avait été entrepris avec les distances par rapport à la moyenne (au lieu des écarts au carré), alors la somme serait toujours de 0, un résultat tout à fait bancal. En prenant la racine carrée des écarts au carré, ce qui constitue une mesure de distance euclidienne, l’écart type devient une mesure de l’étalement de la dispersion autour du centre d’équilibre.\\[\ns =\\sqrt{s^2}= \\sqrt{\\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2}\n\\]\nAvec R, la fonction de base est sd(). Il est possible de récupérer la fonction maison précédemment rédigée.","code":"ecart.type <- function(x){\n  # La racine carrée de la variance\n  et <- sqrt(variance(x))\n  return(et)\n}\n\n# Comparaison\necart.type(x)\n> [1] 17.3\nsd(x)\n> [1] 17.3"},{"path":"calculer.html","id":"lasymétrie","chapter":" 3 Calculer","heading":"3.9 L’asymétrie","text":"L’asymétrie (skewness) est une mesure couramment utilisée de symétrie d’une distribution. Une distribution symétrique, comme la distribution normale, implique que la densité des observations est approximativement égale des deux côtés de la moyenne (valeurs plus hautes et plus basses).\nUne asymétrie négative indique que la distribution est asymétrique à gauche et que la moyenne des données (moyenne) est inférieure à la valeur médiane. Une asymétrie positive indique l’inverse, c’est-à-dire qu’une distribution est asymétrique à droite. Une distribution asymétrique droite est biaisée vers les valeurs les plus élevées, de sorte que la moyenne de la distribution est supérieure à la médiane de la distribution.Le calcul d’asymétrie correspond au ratio entre le troisième moment (écarts au cube) de la distribution par rapport à la racine carré du deuxième moment au cube. Plus simplement, le calcul peut aussi se voir comme le ratio entre la moyenne des écarts centrés au cube par rapport à l’écart type au cube.\\[\n\\gamma_1 = \\frac{\\sum_{=1}^n(x_i-\\bar{x})^3}{\\Big(\\sqrt{\\sum_{=1}^n(x_i-\\bar{x})^2}\\Big)^3}\n\\]Et en code R.R de base ne contient pas de fonction permettant de calculer l’asymétrie. Il faut utiliser le package moments pour obtenir la fonction requise, moments::skewness() ou psych avec psych::skew(). Plusieurs autres packages offrent des fonctions.","code":"\nasymetrie <- function(x){\n  n <- length(x)           # Nombre de données\n  x <- x - mean(x)         # Centrer les données\n  skew <- sum(x^3) /       # Écarts au cube divisés par\n          sqrt(sum(x^2))^3 # l'écart type au cube\n  return(skew)\n}\nmoments::skewness(x)\npsych::skew(x)"},{"path":"calculer.html","id":"laplatissement","chapter":" 3 Calculer","heading":"3.10 L’aplatissement","text":"L’aplatissement (kurtosis) est une mesure de la propension d’une distribution à produire des valeurs éloignées de la moyenne. Souvent erronément référée comme une mesure de la pointe (peakedness; planéité, pointu ou modalité), l’aplatissement réfère à l’épaisseur des queues de la distribution (Westfall, 2014). Plus elles sont épaisses, et plus les valeurs ont tendances à être éloignées (les valeurs extrêmes sont fréquentes). Plus elles sont minces, plus les valeurs ont tendances à être près de la moyenne (les valeurs extrêmes sont rares).Comme l’asymétrie, le calcul se base sur le ratio du quatrième moment (écarts bicarrés4) sur la racine du deuxième moment des écarts bicarrés. Plus simplement, il s’agit du ratio de la moyenne des écarts centrés de degré 4 divisée par la racine carré des écarts\\[\n\\beta_2 = \\frac{\\sum_{=1}^n(x_i-\\bar{x})^4}{\\Big(\\sqrt{\\sum_{=1}^n(x_i-\\bar{x})^2}\\Big)^4}\n\\]Comme la valeur de l’aplatissement tend vers 3, les statisticiens soustraient cette valeur à l’aplatissement \\(\\beta_2\\) pour la centrer sur \\(0\\).\\[\n\\gamma_2 = \\beta_2-3\n\\]\nL’aplatissement \\(\\beta_2\\) représente l’aplatissement régulier et l’aplatissement \\(\\gamma_2\\) correspond à l’aplatissement excessif.En code R.R de base ne contient pas de fonction permettant de calculer l’aplatissement. Il faut utiliser le package moments pour obtenir la fonction requise moments::kurtosis() ou psych avec psych::kurtosi(). Il faut noter que moments calcule l’aplatissement régulier et que, dans le cas de psych, il y plusieurs types d’aplatissement, le plus commun étant le type 1 (aplatissement excessif). Le type 3 est toutefois plus désirables comme il n’est pas biaisé. Plusieurs autres packages offrent des fonctions pour calculer l’aplatissement.","code":"\naplatissement <- function(x){\n  n <- length(x)            # Nombre de données\n  x <- x - mean(x)          # Centrer les données\n  kurt <- mean(x^4) /       # Écarts bicarrés divisés par\n          sqrt(mean(x^2))^4 # l'écart type au bicarré\n  return(kurt)\n}\nmoments::kurtosis(x) - 3\npsych::kurtosi(x, type = 1)"},{"path":"calculer.html","id":"les-graines","chapter":" 3 Calculer","heading":"3.11 Les graines","text":"Par souci de reproductibilité, il est possible de déclarer une valeur de départ aux variables pseudoaléatoires, ce que l’nomme une graine ou seed en anglais. Cela permet de toujours d’obtenir les mêmes valeurs à plusieurs reprises, ce qui est très utile lors d’élaboration de simulations complexes ou lorsque des étudiants essaient de répliquer les résultats tirés d’un ouvrage pédagogique.Il suffit de spécifier cette commande (en remplaçant nombre par un nombre) en début de syntaxe pour définir la séquence de nombre. Cette fonction sera utilisée à plusieurs reprises dans le de reproduire les mêmes sorties.Cette fonction est présentée, car elle reviendra régulièrement dans ce livre pour qu’il soit possible de reproduire et obtenir exactement les mêmes résultats.","code":"\nset.seed(\"nombre\")"},{"path":"calculer.html","id":"les-distributions","chapter":" 3 Calculer","heading":"3.12 Les distributions","text":"Il existe plusieurs distributions statistiques déjà programmées avec R. Voici les principales utilisées dans cet ouvrage.\nTable 3.1: Noms des distributions, fonctions et leurs arguments\nLes libellés ci-dessus ne commanderont pas de fonction. Il faut joindre en préfixe à ces distributions l’une des quatre lettres suivantes : d, p,q, ou r. La plus simple est certainement r (random) qui génère n valeurs aléatoires de la distribution demandée selon les paramètres spécifiés. Les fonctions q (quantile) prennent un argument de 0 à 1 (100%), soit un percentile et retourne la valeur de la distribution. La fonction p (probabilité) retourne la probabilité cumulative (du minimum jusqu’à la valeur) d’une valeur de cette distribution. Enfin, la lettre d (densité) permet, notamment, d’obtenir les valeurs de densité de la distribution.Voici un exemple avec la distribution normale.Ces quatre lettres peuvent être associées à toutes les distributions énumérées et bien d’autres. Elles respectent toutes ce cadre.Afin d’illustrer ce que font ces variables, la Figure 3.1 montre dnorm(), pnorm() et qnorm(). La fonction rnorm() n’est pas illustrée. Cette dernière retourne des valeurs de l’axe des \\(x\\) en respectant les probabilités d’une courbe normale. La fonction dnorm() prend en argument une valeur de l’axe des \\(x\\) et retourne la valeur de la courbe normale (la densité) correspondante, soit la courbe illustrée. En d’autres termes, elle retourne la hauteur de la courbe (ligne pointillée). Les fonctions pnorm() et qnorm() sont interreliées. La fonction pnorm() prend une valeur de l’axe des \\(x\\) et retourne sa probabilité (de \\(-\\infty\\) à \\(x\\)), soit la zone grise de la Figure 3.1. La fonction qnorm(), quant à elle, prend une probabilité et retourne la valeur sur l’axe des \\(x\\) correspondant.\nFigure 3.1: Illustration des fonctions liées à la distribution normale\nCes fonctions entreront en jeu dans le chapitre Inférer.","code":"set.seed(9876)\n\n# Génère 5 valeurs aléatoires en fonction des paramètres\nrnorm(n = 5, mean = 10, sd = .5)\n> [1] 10.51  9.42  9.90  9.95 10.01\n\n# Retourne les valeurs associés à ces probabilités\nqnorm(p = c(.025,.975))\n> [1] -1.96  1.96\n\n# Retourne la probabilité d'obtenir un score de 1.645 et moins\npnorm(q = c(.5, 1.645, 1.96))\n> [1] 0.691 0.950 0.975\n\n# La valeur de la densité de la distribution\ndnorm(x = c(0, 1))\n> [1] 0.399 0.242"},{"path":"rédiger.html","id":"rédiger","chapter":" 4 Rédiger","heading":" 4 Rédiger","text":"R et RStudio offrent un environnement pour rédiger des rapports de recherche. Le package Rmarkdown (Allaire et al., 2021) intègre les fonctions pour ce faire. Plus, R et RStudio permettent de faire des articles, des livres (comme celui-ci), des sites, des applications, etc. Pour ce faire, ils intègrent différents langages de programmation. Cette section insiste sur la production de rapports de recherche simple. La philosophie étant, si Rmarkdown ne le fait pas, c’est superflu. Il demeure possible de personnaliser plusieurs éléments de ces textes, mais souvent cela sera inutile pour l’utilisateur débutant.L’avantage principal de rédiger dans R est certainement le fait de tenir dans un projet, toute la syntaxe, le jeu de données, les graphiques ainsi que le texte rapportant le tout dans un seul fichier.","code":""},{"path":"rédiger.html","id":"préliminaires","chapter":" 4 Rédiger","heading":"4.1 Préliminaires","text":"Pour la toute première utilisation, il faut s’assurer est que le package Rmarkdown soit bien installé (voir Les packages). Il est préférable d’utiliser RStudio: l’environnement y est plus agréable et facilite l’expérience de l’utilisateur.Une seconde étape, si l’utilisateur veut produire des pdf, est d’installer un éditeur \\(\\LaTeX\\). Cela peut se faire facilement avec la commande suivante.Il suffit de le faire une fois à la première utilisation ou pour d’éventuelles mises à jour. Maintenant, tout est prêt!","code":"\ntinytex::install_tinytex()"},{"path":"rédiger.html","id":"ouvrir-un-document","chapter":" 4 Rédiger","heading":"4.2 Ouvrir un document","text":"Une fois tout installé, il faut faire File -> New File -> Rmarkdown pour ouvrir un nouveau document. La boîte de dialogue affichée à la Figure 4.1 demande quatre éléments : le titre, le nom de l’auteur, la date et le format du rapport (html, pdf ou word). Les deux premiers sont les plus utilisés. Une fois rempli, il faut cliquer sur OK.\nFigure 4.1: Boîte de dialogue de Rmarkdown\nCette procédure ouvre un script d’extension .Rmd. Il s’agit d’un exemple typique qui rappelle des éléments de base pour produire un document. L’exemple est présenté à la Figure 4.2. C’est une bonne source pour se rappeler les lignes de la syntaxe Rmarkdown. Chaque élément est vu en détail dans la prochaines section.\nFigure 4.2: Exemple de Rmarkdown\nIl est possible de voir immédiatement le résultat final de ce script en cliquant sur Knit (tricoter) au milieu en haut (le symbole de pelote de laine avec une aiguille). À chaque fois que l’utilisateur désire produire le rapport, il faudra le tricoter en cliquant sur cet icône.","code":""},{"path":"rédiger.html","id":"les-éléments-importants","chapter":" 4 Rédiger","heading":"4.3 Les éléments importants","text":"","code":""},{"path":"rédiger.html","id":"lentête","chapter":" 4 Rédiger","heading":"4.3.1 L’entête","text":"L’entête est la zone qui conserve les éléments du texte, permet la personnalisation et l’ajout de composante, le cas échéant. Cet espace se nomme YAML pour Yet Another Markdown Language. Pour l’instant, les renseignements initiaux s’y retrouvent (titre, nom, date et format).\nFigure 4.3: Entête ou YAML\n","code":""},{"path":"rédiger.html","id":"les-niveaux-de-titre","chapter":" 4 Rédiger","heading":"4.3.2 Les niveaux de titre","text":"Pour les titres de section, les # permette d’indiquer qu’il s’agit effectivement d’un titre et leur nombre permette d’indiquer leur niveau, comme # est de niveau 1, ## est de niveau 2, ### est le niveau 3, etc. Dans l’exemple, il y deux titres de sections de niveau 2.","code":""},{"path":"rédiger.html","id":"les-chunks","chapter":" 4 Rédiger","heading":"4.3.3 Les chunks","text":"Comme le texte est rédiger dans R, il sera utile d’employer du code de ce langage dans le rapport. Ainsi, les rapports servent à jumeler, textes, analyses et graphiques. Pour déterminer le texte du code à lire, la syntaxe à rouler est inscrite dans un chunk (morceau en français). Les chunks sont délimités par trois accents graves (backticks en anglais), puis un accolade indiquant le langage utilisé dans le chunk (r en général), et se termine par trois autre accents grave. La Figure 4.4 montre un chunk.\nFigure 4.4: Un chunk\nPour accélérer l’écriture au clavier, il est possible de faire Ctrl + Alt + (pour Windows) ou Option + Cmd + (pour Mac) pour ouvrir un chunk complet automatiquement. C’est beaucoup plus efficace que de le taper caractère par caractère.Le code peut être présenté avec echo = TRUE (par défaut) ou caché echo = FALSE au besoin.Le code est automatiquement exécuté. S’il y un chunk, le code est roulé. Cela se modifie avec eval = TRUE (par défaut) pour exécuter le code ou ne pas exécuter le code echo = FALSE. Il va sans dire que s’il y une erreur de code dans un chunk, R ne produira pas le rapport.Ces arguments echo et eval doivent être spécifier dans les accolades du chunk. Il est possible de combiner les deux selon les besoins.","code":""},{"path":"rédiger.html","id":"les-figures","chapter":" 4 Rédiger","heading":"4.3.4 Les figures","text":"Pour inclure une figure, il faut la programmer dans un chunk. Dans l’exemple, le graphique du jeu de pressure est présenté avec plot(pressure) dans un chunk, voir la Figure 4.5. Noter dans les accolades la présence de echo=FALSE qui, au moment de produire l’image, n’affichera pas le code source dans le document final (le chunk n’est pas rapporté). Seule la figure est affichée.\nFigure 4.5: Ajouter une figure\nUne autre option est utile pour les figure est de leur ajouter une légende (caption en anglais). Pour ce faire, il faut ajouter dans les accolades du chunk l’argument fig.cap = 'Légende de la figure'.Un souci qui arrive à l’occasion est la taille des figures dans le document final. Il est possible de contrôler la taille avec les arguments .height=\"50%\" et .width=\"50%\" (en changeant le 50% par le pourcentage désiré) dans les accolades du chunk.","code":""},{"path":"rédiger.html","id":"les-images-externes","chapter":" 4 Rédiger","heading":"4.3.5 Les images externes","text":"Pour ajouter une image, il est possible de l’ajouter simplement avec la syntaxe suivante dans un chunk. Comme les figures, il possible de leur donner une légende et de gérer leur taille.","code":"\nknitr::include_graphics(\"chemin_vers_l_image/image.extension\")"},{"path":"rédiger.html","id":"les-tableaux","chapter":" 4 Rédiger","heading":"4.3.6 Les tableaux","text":"Pour ajouter un tableau, il est possible de passer par la fonction kable() de knitr. Le premier argument est le tableau à présenter. Dans cet exemple, il s’agit des six premières lignes (avec la fonction head()) du jeu de données cars. Les autres options sont pour la présentation. L’argument caption = spécifie le titre, l’argument align = permet de centrer (avec \"c\") les éléments dans les cellules et booktabs = TRUE retire la majorité du grillage du tableau.Il importe pour chaque tableau de s’assurer que la variable contient le data.frame ou la matrix disposé tel que l’utilisateur le souhaite (voir Concaténer pour créer ces objets).\nTable 4.1: Titre du tableau\n","code":"\nknitr::kable(head(cars), \n             caption = \"Titre du tableau\", \n             align = \"c\", \n             booktabs = TRUE)"},{"path":"rédiger.html","id":"les-équations","chapter":" 4 Rédiger","heading":"4.3.7 Les équations","text":"Il arrive qu’il soit nécessaire d’ajouter des équations dans un rapport. Pour ce faire, R utilise latex, un très puissant langage de programmation pour la rédaction de texte. Toute équation commence et termine avec le signe de dollars $. Par exemple, l’équation \\(2+2=4\\) s’écrit $2+2=4$. Il est possible d’ajouter des exposants avec ^ ou des indices avec _. Les lettres grecques s’écrivent avec une barre oblique, par exemple, \\(\\beta\\), s’écrit $\\beta$ ou la variance \\(\\sigma^2\\), s’écrit $\\sigma^2$.S’il faut utiliser les vrais symboles, par exemple $ et %, alors il faut ajouter un \\ devant, comme \\$ ou \\%. Également, l’utilisation de la barre oblique devant un espace permet de créer un espace insécable.","code":""},{"path":"rédiger.html","id":"référer-à-des-variables-dans-le-texte","chapter":" 4 Rédiger","heading":"4.3.8 Référer à des variables dans le texte","text":"Pour référer à des variables de l’environnement du rapport, il est possible d’appeler ces valeurs des accents graves. Voici un exemple.Pour rapporter la nouvelle variable moyenne dans le texte, il faut écrire `r moyenne`, c’est-à-dire un accent grave, l’appel à r pour indiquer que la suite est du code R, le nom de la variable et un autre accent grave pour indiquer la fin. Ainsi, `r moyenne` retranscrit son contenu, soit 10.","code":"\nmoyenne <- 10"},{"path":"rédiger.html","id":"formater-du-texte","chapter":" 4 Rédiger","heading":"4.3.9 Formater du texte","text":"Plutôt que d’utiliser des onglets et des options pour modifier le texte, il faut utiliser la syntaxe Markdown (un langage de programmation de texte) pour altérer le texte. Ainsi, entourer un ou des mots avec une étoile de chaque côté met le texte en italique, comme ceci : *italique* donne italique; deux étoiles mettent en gras, **gras** donne gras, pour ajouter des éléments de code, il faut entourer le mot d’accent grave, `code` donne code. Pour souligner il faut passer par du latex pour les pdf, avec \\underline{souligné}, ou par <u>souligné<\/u> en html, ce qui donne souligné.","code":""},{"path":"rédiger.html","id":"ajouter-un-hyperlien","chapter":" 4 Rédiger","heading":"4.3.10 Ajouter un hyperlien","text":"Pour ajouter un hyperlien, il faut l’écrire entre crochet avec le préfixe http://, comme : <http://mqr.teluq.ca>, ce qui donne http://mqr.teluq.ca. Si l’utilisateur préfère cacher le lien sous un mot ou une phrase comme ceci : ce livre, il doit écrire : [ce livre](http://mqr.teluq.ca).","code":""},{"path":"rédiger.html","id":"les-commentaires-dans-le-texte","chapter":" 4 Rédiger","heading":"4.3.11 Les commentaires dans le texte","text":"Un commentaire dans le texte permet de délimiter une section de texte qui ne sera par exécuté lorsque le texte sera rendu. Cela permet de retirer du texte ou du code qui n’est pas présentement utile tout en le conservant dans le fichier.Pour commenter dans un fichier .Rmd, il faut entourer le texte de <!--- et --->, comme ceci <!-- Commentaires -->. Pour être plus efficace, plutôt que d’écrire tout au long, il est possible de sélectionner le texte puis Ctrl + Shift + C, ce qui commentera tout le texte sélectionné.","code":""},{"path":"rédiger.html","id":"pour-aller-plus-loin","chapter":" 4 Rédiger","heading":"4.4 Pour aller plus loin","text":"Cette section n’est qu’un survol des possibilités. Pour aller plus loin, il y de la documentation sur le web pouvant aider à maîtriser la création d’équation, des spécifier sur la gestion des figures, ou les façons de formater les figures. Et comme c’est la communauté de R qui crée le contenu, il y plus d’une bonne manière de faire la même chose.","code":""},{"path":"exercice-rudiments.html","id":"exercice-rudiments","chapter":"Exercices","heading":"Exercices","text":"Quel est le résultat de mean <- c(1, 2, 3)? Pourquoi?Quel est le résultat de mean <- c(1, 2, 3)? Pourquoi?Quelle est la différence entre # Caractère et \"Caractère\"?Quelle est la différence entre # Caractère et \"Caractère\"?Créer un vecteur contenant les valeurs \\(4, 10, 32\\). Calculer la moyenne et l’écart type de ce vecteur.Créer un vecteur contenant les valeurs \\(4, 10, 32\\). Calculer la moyenne et l’écart type de ce vecteur.Créer un vecteur contenant les valeurs de \\(4\\) à \\(11\\). Sélectionner la deuxième valeur de ce vecteur, puis additionner 100 à cette valeur et remplacer la dans le vecteur.Créer un vecteur contenant les valeurs de \\(4\\) à \\(11\\). Sélectionner la deuxième valeur de ce vecteur, puis additionner 100 à cette valeur et remplacer la dans le vecteur.Générer 10 valeurs aléatoires distribuées normalement avec une moyenne de 50 et un écart type de 4. Calculer la moyenne, la médiane et la variance.Générer 10 valeurs aléatoires distribuées normalement avec une moyenne de 50 et un écart type de 4. Calculer la moyenne, la médiane et la variance.Créer un jeu de données contenant quatre sujets avec, pour chacun, leur nom de famille, leur âge et un score d’appréciation tiré d’une distribution uniforme allant de 0 à 100.Créer un jeu de données contenant quatre sujets avec, pour chacun, leur nom de famille, leur âge et un score d’appréciation tiré d’une distribution uniforme allant de 0 à 100.Rédiger une fonction calculant l’hypoténuse d’un triangle rectangle. Rappel: le théorème de Pythagore est \\(c^2=^2+b^2\\).Rédiger une fonction calculant l’hypoténuse d’un triangle rectangle. Rappel: le théorème de Pythagore est \\(c^2=^2+b^2\\).Rédiger une fonction calculant un score-\\(z\\) pour une variable. Rappel: un score-\\(z\\), correspond à \\(z=\\frac{x-\\mu}{\\sigma}\\).Rédiger une fonction calculant un score-\\(z\\) pour une variable. Rappel: un score-\\(z\\), correspond à \\(z=\\frac{x-\\mu}{\\sigma}\\).Rédiger une fonction calculant la médiane d’une variable (ne pas recopier celle de ce livre).Rédiger une fonction calculant la médiane d’une variable (ne pas recopier celle de ce livre).Rédiger une fonction qui pivote une liste de \\(k\\) éléments par \\(n\\). Par exemple, une liste de six (\\(k=6\\) comme \\([1,2,3,4,5,6]\\)) pivoté de deux (\\(n=2\\)) devient (\\([3,4,5,6,1,2]\\)).Rédiger une fonction qui pivote une liste de \\(k\\) éléments par \\(n\\). Par exemple, une liste de six (\\(k=6\\) comme \\([1,2,3,4,5,6]\\)) pivoté de deux (\\(n=2\\)) devient (\\([3,4,5,6,1,2]\\)).Rédiger une fonction pour générer une séquence de Fibonacci (chaque nombre est la somme des deux précédents) jusqu’à une certaine valeur, soit \\(1, 1, 2, 3, 5, 8,...\\). (Question difficile)Rédiger une fonction pour générer une séquence de Fibonacci (chaque nombre est la somme des deux précédents) jusqu’à une certaine valeur, soit \\(1, 1, 2, 3, 5, 8,...\\). (Question difficile)","code":""},{"path":"entrer.html","id":"entrer","chapter":" 5 Entrer","heading":" 5 Entrer","text":"S’il y bien une caractéristique de R qui rebute les nouveaux utilisateurs, c’est certainement la saisie de données qui est n’est pas mise au premier plan. Lorsque le logiciel s’ouvre, que ce soit R ou RStudio, l’aspect table de données n’existe pas. Un tout nouvel utilisateur habitué aux logiciels traditionnels reste pantois : où sont les données?Conceptuellement, R importe des données pour les manipuler t produire des graphiques. Elles ne sont pas entrées dans le logiciel. Il existe toutefois quelques méthodes pour ce faire, dont en voici une courte liste. Ces méthodes sont présentées; elles ne sont pas recommandées.","code":""},{"path":"entrer.html","id":"data.entry-r-base","chapter":" 5 Entrer","heading":"5.1 data.entry() (R-base)","text":"De base, R offre la possibilité d’entrer des données dans un tableur avec la commande jd <- data.entry(). Si une base de données est demandée comme argument (p. ex., data.entry(data <- jd)), alors le jeu de données s’ouvre. Il est aussi possible d’ouvrir le fichier avec des variables déjà créées avec R.Si l’utilisateur désire un tableur vierge, alors il doit taper data.entry(1) dans la console pour ouvrir le tableur avec une seule valeur (1)5. L’utilisateur peut alors modifier les noms de colonnes et entrer les données comme dans un logiciel traditionnel. La Figure 5.1 montre l’interface bien moins attrayante que celles des compétiteurs.\nFigure 5.1: Ouverture du tableur R\nLorsque l’entrée de données est terminée, l’utilisateur doit sauvegarder le jeu de données ou l’environnement de travail qui pourront être importés pour de futures utilisations ou entrées (voir la section Sauvegarder un jeu de données). En général, l’utilisateur qui entre manuellement ses données préférera certainement un autre tableur, puis importer ses données, mais R est en mesure de faire ce travail.","code":""},{"path":"entrer.html","id":"data_edit-dataeditr","chapter":" 5 Entrer","heading":"5.2 data_edit() (DataEditR)","text":"Comme il arrive régulièrement, un problème avec R se résout avec un package. Il existe un package qui permet de faire l’entrée de données en tableur avec R. Le package DataEditR (Hammill, 2021) est une interface utilisateur graphique pour la saisi de données. Il résout l’un de plus grands défis lorsqu’un utilisateur migre des tableurs traditionnels vers R : une feuille de calcul interactive où il est possible de pointer et cliquer pour modifier, ajouter, supprimer des donnes vers un mode strict de syntaxe.Pour démarrer, il faut d’abord installer le package, puis l’appeler. Pour commencer à entrer des données, la syntaxe data_edit() est suffisante. Pour ouvrir un jeu de données, il suffit de l’ajouter en argument data_edit(jd).Une fois l’interface ouvert, il est possible de manipuler le jeu de données avec les options affichés et avec le clic droit qui permettra notamment d’ajouter des lignes et des colonnes.Il est recommandé de ne laisser que les données brutes, toutes les modifications et manipulations doivent rester en syntaxe R dans un script associé au jeu de données. Lorsque les entrées sont terminées, sauvegarder la base de données, préférablement en extension .csv. Il est aussi possible de sortir le tableur en tableau de données en assignant la fonction à une variable comme jd <- data_edit().\nFigure 5.2: Ouverture du tableur de DataEditR\nEn général, l’utilisateur doit importer ces données dans l’environnement R. Il faut même le faire avec data_edit() à chaque ouverture d’un nouvelle séance, pour poursuivre l’entrée ou réaliser des manipulations manuelles.","code":"\n# Pour installer le package\ninstall.packages(\"DataEditR\")\n\n# Pour rendre la package accessible\nlibrary(\"DataEditR\")\n\n# La fonction\ndata_edit()"},{"path":"importer.html","id":"importer","chapter":" 6 Importer","heading":" 6 Importer","text":"Un jeu de données est essentiel pour réaliser des analyses statistiques et des représentations graphiques. Après l’ouverture de R, l’usager remarque, stupéfait, que le logiciel n’est pas un tableur (tableau de données), contrairement aux logiciels traditionnels. Cette caractéristique l’peut-être même frappé à la première ouverture!Les jeux de données sont traités de la même façon que les scripts. Il est d’usage de les conserver dans un fichier externe et d’y recourir pendant la séance de travail.Un jeu de données porte généralement les extensions “.Rdata”, lorsqu’elles proviennent de R, ou d’extensions “.dat” et “.txt”. Évidemment, R permet une grande flexibilité, il est ainsi possible d’exporter et d’importer dans d’autres extensions. Les extensions “.Rdata” sont des environnements R, elles contiennent potentiellement plusieurs variables, comme une séance de travail complète. Elles ont aussi l’avantage que, si l’utilisateur double-clique sur un fichier d’extension “.Rdata”, celui-ci s’ouvre automatiquement dans l’environnement R.Il faut importer le jeu de données à chaque nouvelle séance d’utilisation. Contrairement aux logiciels traditionnels, il n’est pas nécessaire, ni même recommander!, de modifier, d’altérer ou de manipuler un fichier contenant des données. Cela n’est pas nécessaire dans la mesure où les syntaxes décrivant ces manipulations sont conservées. Il devient alors impossible d’endommager, de corrompre, d’altérer ou d’archiver erronément le fichier de données. Les données originales restent intactes. Il suffit de les importer à chaque nouvelle séance, de commander la syntaxe qui lui est associée pour obtenir de nouveau la version propre du jeu de données.Il est recommandé de ne jamais manipuler les fichiers de données une fois toutes les vérifications réalisées (absence d’erreur dans les données). Il n’est jamais nécessaire de modifier ces fichiers avec R. Également, un même jeu de données peut être associé à plusieurs syntaxes de nettoyage, de manipulation et d’analyse, mais tous les collaborateurs partagent le même jeu de données, ce qui facilite les échanges entre eux.","code":""},{"path":"importer.html","id":"jeux-de-données-provenant-de-r-et-de-packages","chapter":" 6 Importer","heading":"6.1 Jeux de données provenant de R et de packages","text":"Plusieurs packages offrent, en plus des fonctions, des jeux de données. Mieux encore! R offre des jeux de données inclus avec le logiciel. La fonction data() permet de voir la liste des jeux de données disponibles. Taper simplement le nom du jeu de données permet de l’utiliser comme s’il avait été déclaré auparavant.La fonction head() introduite ici donne simplement un aperçu des six premières lignes du jeu de données pour ne pas afficher le jeu de données complet (ce qui prend beaucoup d’espace inutilement dans la console).Pour consulter tous les jeux de données des packages importés, il est possible d’utiliser cette ligne de code.Pour utiliser ces jeux, il faut rendre actif le package associé avec la fonction library().","code":"head(cars)\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6     9   10\ndata(package = .packages((all.available = TRUE)))"},{"path":"importer.html","id":"créer-un-jeu-de-données-artificielles","chapter":" 6 Importer","heading":"6.2 Créer un jeu de données artificielles","text":"Une façon rudimentaire et efficace d’obtenir des données avec R est de les créer à l’aide des fonctions génératrices de données pseudoaléatoires vue à la section Les distributions et les joindre ensemble (voir section Créer un jeu de données).Et voilà un jeu de données simple et sauvegardé dans le dossier de travail auquel il est possible de se référer.Ici, deux nouvelles fonctions sont employées : round() arrondie les valeurs à l’unité et save() permet de sauvegarder un jeu de données crée un le jeu de données.","code":"\n# Pour la reproductibilité\nset.seed(142)\n\n# Nombre d'unité\nn <- 30\n\n# Identifiant (séquence de 1 à n)\nid <- 1:n\n\n# Variables\nsexe <- rbinom(n, size = 1, prob = .5)\nQI <-  round(rnorm(30, mean = 100, sd = 15) - 5 * sexe)\n# Être \"1\" soustrait 5 points au QI en moyenne\n# Arrondi avec round()\n\n# Création du jeu de données\njd <- data.frame(id = id, \n                 sexe = sexe,\n                 QI = QI)\n\n# Enregistrement\nsave(jd, file = \"donnees.Rdata\")"},{"path":"importer.html","id":"sauvegarder-un-jeu-de-données","chapter":" 6 Importer","heading":"6.3 Sauvegarder un jeu de données","text":"Si un jeu de données est créé directement avec R, par exemple, les jeux de données artificiels, il est possible de les sauvegarder avec la fonction save() qui enregistre une variable dans un fichier.La fonction save() deux arguments principaux : un nom de variable à enregistrer et un nom de fichier d’extension .Rdata, les deux entre guillemets anglophones.Il est possible à la fin d’une session de travail de sauvegarder l’environnement complet dans un fichier save.image(). Ainsi, toutes les variables et fonctions maison sont conservées pour une future utilisation.Il y aussi la famille de fonction write() pour enregistrer le jeu de données en différentes extension, comme, write.csv(), write.csv2(), write.table(). Celles-ci fonctionne comme la fonction save().","code":"\nsave(\"variable\", file = \"fichier.Rdata\")\nsave.image(file = \"SessionTravail.Rdata\")"},{"path":"importer.html","id":"voir-la-base-de-données","chapter":" 6 Importer","heading":"6.4 Voir la base de données","text":"Parfois des valeurs s’ajoutent lors de l’exportation ou l’importation des données. Des logiciels traditionnels font parfois ce mauvais tour. Une vérification de la base de données est par conséquent impérative, surtout lors de la première utilisation du jeu de données. Deux méthodes de vérification sont suggérées. D’abord, ouvrir le fichier avec un éditeur de texte de base, comme bloc-notes, pour s’assurer qu’aucun caractère indésirable ne s’est ajouté à l’insu de l’utilisateur. Ensuite, voir avec la fonction View() dans R si la base de données s’affiche correctement et que les colonnes et les lignes correspondent à ce qui est attendu.Il est possible de voir les données en utilisant la fonction View() et en y insérant le nom de la variable associée au jeu de données. Le logiciel affiche un tableur avec les données dont il est impossible de modifier les valeurs. Cette fonction est utile pour s’assurer que le jeu de données est en ordre, bien importé, ou le consulter.","code":"\nView(jd)"},{"path":"importer.html","id":"emplacement-du-jeu-de-données","chapter":" 6 Importer","heading":"6.5 Emplacement du jeu de données","text":"Idéalement, le fichier contenant le jeu de données se retrouve déjà dans le directoire de travail (ou dans le projet R en cours). Dans ce contexte, il suffit de référer seulement au nom du fichier.Si le jeu de données est sur le web, il peut être importé en précisant l’URL.S’il est dans un fichier sur l’ordinateur, mais pas dans le directoire de travail, il s’agit essentiellement la même méthode.Pour ces deux derniers exemples d’importations, noter bien l’utilisation du / (barre oblique ou slash) ou du plus robuste // plutôt que le \\ (barre oblique inverse) généralement utilisée par les ordinateurs.Si l’utilisateur ne connaît pas exactement la trajectoire du fichier, il peut se résoudre à passer par l’explorateur de fichiers (Windows ou Apple) pour déterminer l’emplacement du fichier de jeu de données. Il faut alors utiliser la fonction file.choose() sans aucun argument à l’intérieur de la fonction d’importation.L’utilisateur doit identifier manuellement (pointer et cliquer) où se trouve le fichier. Il se promène de fichier en fichier jusqu’à ce qu’il arrive au bon jeu de données, un peu comme le font les logiciels traditionnels lorsque l’utilisateur souhaite sauvegarder un fichier à un certain endroit. Une fois la trajectoire du fichier identifiée, la variable chemin contient cette trajectoire qui peut alors être utiliser dans les fonctions d’importation ou bien manipuler comme n’importe quelle chaîne de caractères.","code":"\njd <- read.table(\"fichier.txt\")\njd <- read.table(\"https://site/ou/trouver/le/fichier.txt\")\njd <- read.table(\"C://site//ou//trouver//le//fichier.txt\")\nchemin <- file.choose()"},{"path":"importer.html","id":"importer-avec-r","chapter":" 6 Importer","heading":"6.6 Importer avec R","text":"Dans la plupart des situations, les analyses et les graphiques sont réalisés à partir d’un jeu de données se trouvant dans un fichier. Celui-ci est importé en R pour être manipulé. Les jeux de données peuvent se trouver dans un fichier dans l’ordinateur, mais aussi sur le web. Ils peuvent être en différents types de format.Pour la description de l’importation, le présent ouvrage prend pour acquis que le fichier de données se retrouve dans le directoire de travail (ce qui est l’idéal en général). La création de trajectoires pour différents emplacements est présentée dans la section Emplacement du jeu de données.Les fonctions de base permettent d’importer la plupart des jeux de données, particulièrement s’ils ont été exportés dans un format compatible. Pour le cas où ces fichiers ne sont pas importables, des packages pallient ce besoin. Dans cet ouvrage, seule une présentation sommaire de ces options est discutée, l’utilisateur est recommandé à la documentation de ces packages pour plus d’informations.La prochaine section décrit les fonctions pour importer les bases de données “manuellement”. En plus de ces méthodes, RStudio possède une interface permettant l’importation des données.","code":""},{"path":"importer.html","id":"la-fonction-de-base","chapter":" 6 Importer","heading":"6.6.1 La fonction de base","text":"La fonction de base read.table() permet d’importer la plupart des jeux de données. Ceux-ci ont certaines caractéristiques qu’il faudra préciser comme argument à la fonction read.table() pour assurer une importation adéquate. Ces caractéristiques sont header = FALSE, sep = \"\", et fill = !blank.lines.skip (les éléments à droite sont les options par défaut).Parfois, certains fichiers sauvegardent le nom des variables en tête de colonne (première ligne). Par défaut, R assume qu’il s’agit de valeurs. L’argument header = TRUE ajouté à read.table() précise à R lors de l’importation que ces libellés sont des noms de colonnes.Si un autre symbole est utilisé pour délimiter (séparer) des valeurs dans le fichier, comme ;ou ,, l’argument sep = \";\" ou sep = \",\" précise le séparateur.Si les lignes du fichier sont de tailles inégales, R assume qu’il s’agit de valeur, et ces blancs de texte sont ajoutés comme valeurs (\"\"). Pour gérer cette situation, l’argument fill = FALSE règle la situation.","code":""},{"path":"importer.html","id":"fichiers-dextension-.txt","chapter":" 6 Importer","heading":"6.6.2 Fichiers d’extension .txt","text":"Un fichier d’extension .txt est un fichier texte délimité par des tabulations (tab-delimited text files) et est importé à l’aide de la fonction read.table().","code":"\njd <- read.table(\"fichier.txt\")"},{"path":"importer.html","id":"fichiers-dextension-.dat","chapter":" 6 Importer","heading":"6.6.3 Fichiers d’extension .dat","text":"Un fichier d’extension .dat est un fichier générique de données et est importé à l’aide de la fonction read.table().","code":"\njd <- read.table(\"fichier.dat\")"},{"path":"importer.html","id":"fichiers-dextension-.csv","chapter":" 6 Importer","heading":"6.6.4 Fichiers d’extension .csv","text":"Un fichier d’extension .csv use généralement de séparateur comme \";\" (lorsque le système numérique de la langue d’origine utilise la virgule - comme le français par exemple) ou \",\" (pour les autres langues qui n’utilise pas la virgule) et ont généralement les noms de variables en première ligne. Ainsi, la fonction read.table() est utilisable pourvu que le séparateur soit précisé et la présence d’en-tête également.Il existe aussi la fonction read.csv() (nombres décimaux délimités par un point) et read.csv2() (nombres décimaux délimités par une virgule) pour importer des fichiers d’extension .csv. Il s’agit exactement de read.table() à l’exception des arguments par défaut, mais précisant par défaut header = TRUE et fill = TRUE et détecte s’il s’agit de \";\" ou \",\".","code":"\njd <- read.table(\"fichier.csv\", sep = \";\", header = TRUE)"},{"path":"importer.html","id":"fichiers-délimités","chapter":" 6 Importer","heading":"6.6.5 Fichiers délimités","text":"Pour les fichiers recourant à un autre caractère qu’une tabulation, qu’une \",\", ou un \";\" pour délimiter les valeurs, il faut spécifier le caractère dans read.table() importe le fichier.Comme pour read.csv() et read.csv2(), les fonctions read.delim() et read.delim() pourraient être utilisées.","code":"\njd <- read.table(\"fichier.txt\", sep = \"$\")"},{"path":"importer.html","id":"fichiers-dextension-.sav-.dta-.syd-et-.mtp","chapter":" 6 Importer","heading":"6.6.6 Fichiers d’extension .sav, .dta, .syd et .mtp","text":"Comme le lecteur s’en doute peut-être, R de base ne permet pas d’importer des fichiers spécifiques d’autres logiciels. Par contre, avec les années se sont développés des packages permettant de pallier la situation. Le package foreign (R Core Team, 2020) permet d’importer des fichiers issus de IBM SPSS (.sav), Stata (.dta) et Systat (.syd) et Minitab (mtp) avec, respectivement les fonctions read.spss(), read.data(), read.systat() et read.mtp(). La logique d’importation est la même pour ces quatre fonctions.Pour read.spss(), il y deux arguments qui sont importants à souligner. Par défaut, la fonction ne retourne pas un data frame et utilise les libellés de valeurs (value labels). Dans la plupart des cas, l’utilisateur désire probablement obtenir un jeu de données de type data.frame et les valeurs sous-jacentes aux libellés de valeurs. L’utilisateur peut alors changer ces arguments .data.frame = TRUE (par défaut, FALSE) et use.value.labels = FALSE(par défaut, TRUE).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\nlibrary(foreign)\n\n# SPSS\njd <- read.spss(\"fichier.sav\", \n                to.data.frame = TRUE,\n                use.value.labels = FALSE)\n\n# Stata\njd <- read.dta(\"fichier.dta\")\n\n# Systat\njd <- read.systat(\"fichier.syd\") \n\n# Minitab\njd <- read.mtp(\"fichier.mtp\")"},{"path":"importer.html","id":"fichiers-dextension-.xls-et-xlsx","chapter":" 6 Importer","heading":"6.6.7 Fichiers d’extension .xls et xlsx","text":"Il n’existe pas de fonction de base pour importer des fichiers Microsoft Excel (extensions .xls et .xlsx). Par contre, il existe plusieurs packages qui permettent de la faire, comme readxl (Wickham & Bryan, 2022). Le package readlxl permet d’utiliser la fonction read_excel() pour importer le fichier.La fonction read_excel() possède un argument sheet = qui permet de préciser la feuille qu’il faut importer ou range = (p. ex. range = A1:B20 qui permet d’importer un rectangle de plage de données (du coin supérieur gauche A1 au coin inférieur droit B20).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n# Excel\nlibrary(readxl)\njd <- read_excel(\"fichier.xls\")"},{"path":"importer.html","id":"fichiers-dextension-.html","chapter":" 6 Importer","heading":"6.6.8 Fichiers d’extension .html","text":"Il n’existe pas de fonction de base pour importer des fichiers d’extension .html, (HTML, HyperText Markup Language). Le package XML fournit une solution possible avec la fonction readHMTLTable() (Temple Lang, 2022).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n# HTML\nlibrary(XML)\njd <- readHMTLTable(\"fichier.html\")"},{"path":"importer.html","id":"fichiers-dextension-.json","chapter":" 6 Importer","heading":"6.6.9 Fichiers d’extension .json","text":"Il n’existe pas de fonction de base pour importer des fichiers d’extension .json, (JavaScript Object Notation). Comme le lecteur pourra s’y attendre, il existe un package pour rectifier la situation : le package rjson et sa fonction fromJSON() (Couture-Beil, 2022).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n#JSON\nlibrary(rjson)\njd <- fromJSON(\"fichier.json\")"},{"path":"importer.html","id":"fichiers-dextension-.sas7bdat","chapter":" 6 Importer","heading":"6.6.10 Fichiers d’extension .sas7bdat","text":"Il n’existe pas de fonction de base pour importer des fichiers d’extension .sas7bdat, (Statistical Analysis System). Il existe le package sas7bdat pour importer des données de SAS vers R avec la fonction read.sas7bdat() (Shotwell, 2022).Consulter la documentation du package pour plus d’informations sur les options possibles.","code":"\n# SAS\nlibrary(sas7bdat)\njd <- read.sas7bdat(\"fichier.sas7dbat\")"},{"path":"importer.html","id":"importation-avec-rstudio","chapter":" 6 Importer","heading":"6.7 Importation avec RStudio","text":"RStudio offre une interface simple pour télécharger directement un jeu de de données IBM SPSS, Microsoft Excel, SAS, STATA, et des extensions “.txt” et “.readr”. Il y même un outil de visualisation pour s’assurer que le tout est en ordre. Pour procéder, il faut faire *File > Import dataset > “format de fichier* où”format de fichier” remplace Text, SPSS, Excel et les autres. Il faut suivre les instructions. En indiquant le chemin du fichier, R importe le fichier et fournie une syntaxe afin de reproduire l’importation pour de futurs usages.","code":""},{"path":"importer.html","id":"exporter-dautres-logiciels","chapter":" 6 Importer","heading":"6.8 Exporter d’autres logiciels","text":"","code":""},{"path":"importer.html","id":"exporter-de-ibm-spss","chapter":" 6 Importer","heading":"6.8.1 Exporter de IBM SPSS","text":"Il est possible d’exporter des données de IBM SPSS pour une utilisation avec R. Il faut quelques manipulations préalables. En ayant le fichier de données IBM SPSS ouvert, il faut cliquer sur “Enregistrer sous” sous le menu déroulant “Fichier”. Par défaut, IBM SPSS choisie toutes les variables, mais il est possible de sélectionner seulement les variables d’intérêt en décochant les variables qu’il n’est pas nécessaire de conserver. Ensuite, il faut sélectionner le type de fichier de sauvegarde, préférablement “Tabulé (*.dat)“. IBM SPSS offre également la possibilité d’enregistrer les noms de variables (première option à cocher) et les libellés de valeur. Il suffit maintenant de nommer le fichier et de cliquer sur l’onglet”Enregister”.En s’assurant que nouveau fichier se trouve dans le directoire actif de R, il suffit de télécharger le fichier.L’option header est FALSE si les noms de variables n’ont pas été conservés (première ligne du fichier). La variable jd contient la base de données et est prête à être manipulée.","code":"\njd <- read.table(file = \"donnees.tab\", header = TRUE)"},{"path":"importer.html","id":"exporter-de-microsoft-excel","chapter":" 6 Importer","heading":"6.8.2 Exporter de Microsoft Excel","text":"Il est possible d’exporter des données de Microsoft Excel pour une utilisation avec R. Il faut sélectionner l’onglet “Fichier”, puis Enregistrer sous”. Dans le menu déroulant, sélectionner comme type de fichier “Texte Unicode (*.txt)“. Intituler le fichier, puis cliquer sur”Enregister”.Microsoft Excel sauvegarde l’entièreté de la page active. Il est donc pertinent de créer une feuille Microsoft Excel ne contenant que les informations à conserver.Par la suite, en s’assurant que nouveau fichier se trouve dans le directoire actif de R, il suffit de télécharger le fichier avec read.table() (voir la prochaine section) et les arguments convenant au jeu de données.","code":""},{"path":"importer.html","id":"quelques-conseils","chapter":" 6 Importer","heading":"6.9 Quelques conseils","text":"Voici quelques conseils pour la gestion le jeu de données.Éviter les noms trop longs ou trop courts et dépourvus de signification. Cela augmente le risque d’erreur. L’utilisation de huit caractères ou moins est une bonne recommandation (quoique ce n’est pas une règle!). Pour conserver plus de renseignements, utiliser les commentaires de la syntaxe.Éviter les noms trop longs ou trop courts et dépourvus de signification. Cela augmente le risque d’erreur. L’utilisation de huit caractères ou moins est une bonne recommandation (quoique ce n’est pas une règle!). Pour conserver plus de renseignements, utiliser les commentaires de la syntaxe.Éviter les espaces entre les mots. Cela peut être interprété erronément comme deux éléments. À la place, collez les mots et distinguer-les avec des majuscules (MaFonction), utiliser le tiret bas (ma_fonction) ou un point (ma.fonction).Éviter les espaces entre les mots. Cela peut être interprété erronément comme deux éléments. À la place, collez les mots et distinguer-les avec des majuscules (MaFonction), utiliser le tiret bas (ma_fonction) ou un point (ma.fonction).Éviter les espaces ou les vides dans les données. Cela peut être interprété comme des données (absentes) ou non.Éviter les espaces ou les vides dans les données. Cela peut être interprété comme des données (absentes) ou non.Éviter les symboles suivants ?, $, %, ^, &, *, (, ), -, #, ?, , , <, >, /, |, \\, [, ], { et } qui peuvent erronément être interprétés comme de la syntaxe autant dans les noms de variables que dans les données.Éviter les symboles suivants ?, $, %, ^, &, *, (, ), -, #, ?, , , <, >, /, |, \\, [, ], { et } qui peuvent erronément être interprétés comme de la syntaxe autant dans les noms de variables que dans les données.Vérifier que les valeurs manquantes sont identifiées NA.Vérifier que les valeurs manquantes sont identifiées NA.Si les données proviennent d’un autre logiciel, vérifier la présence de commentaires qui pourraient occasionner des lignes ou colonnes supplémentaires et ainsi corrompre le jeu de données.Si les données proviennent d’un autre logiciel, vérifier la présence de commentaires qui pourraient occasionner des lignes ou colonnes supplémentaires et ainsi corrompre le jeu de données.Vérifier que l’exportation et l’importation se sont bien déroulées.Vérifier que l’exportation et l’importation se sont bien déroulées.","code":""},{"path":"manipuler.html","id":"manipuler","chapter":" 7 Manipuler","heading":" 7 Manipuler","text":"Avec R, il ne faut jamais manipuler directement le fichier contenant les données. Cette pratique est déconseillée. Il vaut mieux préserver le fichier original intact, ce qui évite de nombreuses complications liées à la compatibilité entre les versions, la reproductibilité des analyses et la maintenance des fichiers. Toutes les manipulations doivent être conservées dans un fichier script. Cela favorise le suivi des modifications apportées en comparant tout simplement les traces dans les syntaxes, mais aussi le partage entre collègues.En pratique, l’expérimentateur aura le jeu de données officiel (final) avec lequel travailler. Il doit l’importer à chaque début de séance. Par la suite, il ne lui reste qu’à mettre en place le nettoyage et la préparation du jeu de données ou bien, si cela est déjà fait, de rouler les scripts des séances précédentes, ce qui se fait facilement en quelques cliques.Il existe plusieurs méthodes pour gérer des données; il ne faut pas s’étonner de voir d’autres ouvrages aborder la gestion de données d’une autre façon. Au final, la meilleure méthode est celle qui m’est l’utilisateur à son aise.Dans la première section, les manipulations rudimentaires d’un jeu de données sont expliquées. Dans la seconde section, la philosophie tidyverse est présentée, car elle permet une manipulation assez intuitive des jeux de données6.","code":""},{"path":"manipuler.html","id":"manipulation-de-données","chapter":" 7 Manipuler","heading":"7.1 Manipulation de données","text":"Les tableaux ont généralement deux dimensions (lignes par colonnes). Différents éléments ou groupes d’éléments peuvent être extraits des jeux de données. Plusieurs méthodes peuvent être utilisées en fonction des besoins. Le jeu de données cars (disponible avec R) sera utilisé à des fins illustratives.Le jeu de données contient 50 unités d’observation (lignes) et deux variables (colonnes), soit la vitesse (speed) et la distance (dist).","code":"head(cars)\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6     9   10"},{"path":"manipuler.html","id":"référer-à-une-variable-dans-un-jeu-de-données","chapter":" 7 Manipuler","heading":"7.1.1 Référer à une variable dans un jeu de données","text":"Il est possible de référer à une variable soit en utilisant l’emplacement de la variable par rapport aux autres en utilisant les crochets ou en utilisant le signe $ puis le nom de la variable après le libellé de le jeu de données. L’opération est fort simple avec le symbole $.Précédemment utilisés pour extraire des valeurs dans une variable unidimensionnelle (voir Référer à des sous-éléments), les [] peuvent extraire des données sur un tableau en deux dimensions (ligne par colonne). Entre crochets, il faut spécifier la ou les lignes désirées, puis la ou les colonnes désirées. Laissez une des dimensions en blanc (vide) indique au logiciel de rapporter toutes les valeurs. Par exemple, pour obtenir le même résultat que la syntaxe précédente, soit obtenir dist de la deuxième colonne, il faut référer entre crochets à la colonne \\(2\\). Comme toutes les lignes sont désirées, la dimension des lignes reste vide.Il est possible de faire la même chose avec les lignes.Ici, toutes les variables de la 4e unité sont rapportée. Remarquer bien l’absence d’argument après la virgule. La fonction head() n’est pas nécessaire ici, car il y peu d’informations à extraire.Si certaines valeurs spécifiques étaient désirées, comme la valeur de la 4e unité pour la 2e variable.Enfin, à l’intérieur d’un jeu de données, les variables peuvent être commandées avec le signe de $ placé après le nom de la variable suivi du nom de la variable ou encore en identifiant les noms de variables entre crochets.","code":"# Avec $\nhead(cars$dist)\n> [1]  2 10  4 22 16 10# Entre crochets\nhead(cars[,2])\n> [1]  2 10  4 22 16 10# Entre crochets\ncars[4,]\n>   speed dist\n> 4     7   22# Entre crochets\ncars[4, 2]\n> [1] 22# Utilisation du signe $\nhead(cars$speed)\n> [1] 4 4 7 7 8 9\n\n# Nommer entre crochets\nhead(cars[\"speed\"])\n>   speed\n> 1     4\n> 2     4\n> 3     7\n> 4     7\n> 5     8\n> 6     9"},{"path":"manipuler.html","id":"référer-à-un-sous-ensemble-dunité","chapter":" 7 Manipuler","heading":"7.1.2 Référer à un sous-ensemble d’unité","text":"Pour référer à des unités ayant certaines caractéristiques, la fonction subset() peut s’avérer utile. Les arguments sont un jeu de données, le deuxième est un opérateur logique (voir Les clauses conditionnelles) en lien avec une variable du jeu de données.Cette fonction est utile s’il faut extraire les données d’un certain sexe ou les participants plus jeune ou plus vieux qu’un certain âge, par exemple.","code":"# Extraire les données pour toutes les unités ayant une vitesse égale à 24\nsubset(cars, speed == 24)\n>    speed dist\n> 46    24   70\n> 47    24   92\n> 48    24   93\n> 49    24  120"},{"path":"manipuler.html","id":"nommer-des-variables-dans-un-jeu-de-données","chapter":" 7 Manipuler","heading":"7.1.3 Nommer des variables dans un jeu de données","text":"Il est possible d’attribuer ou de modifier des noms à des colonnes ou des lignes d’un tableau de données. Les fonctions colnames() et rownames() sont alors utilisées. Contrairement aux autres fonctions, celles-ci se retrouvent à gauche de l’équation.Il importe de fournir autant de noms qu’il y de colonnes (ou lignes), et ce, en chaîne de caractères.","code":"colnames(cars) <-  c(\"vitesse\", \"distance\")\nhead(cars)\n>   vitesse distance\n> 1       4        2\n> 2       4       10\n> 3       7        4\n> 4       7       22\n> 5       8       16\n> 6       9       10"},{"path":"manipuler.html","id":"données-manquantes","chapter":" 7 Manipuler","heading":"7.1.4 Données manquantes","text":"Les devis de recherche et les jeux de données empiriques sont rarement parfaits et peuvent souvent contenir des données manquantes. R reconnaît les données manquantes lorsqu’elles sont identifiées comme NA (available). Plusieurs méthodes permettent de gérer les données manquantes. La méthode la plus simple est d’éliminer les unités ayant une donnée manquante, soit la suppression par liste (listwise suppression). Les fonctions natives de R recourrent à l’argument na.rm = TRUE. Si cela est impossible, la fonction na.omit() permet de créer des jeux de données sans les valeurs manquantes. Certaines fonctions, comme mean() ont des arguments pour gérer les données manquantes.","code":"# Un vecteur\nvaleurs <-  c(10, 12, 14, NA, 18)\n\n# La présence de NA empêche la moyenne d'être calculée\nmean(valeurs)\n> [1] NA\n\n# L'argument \"na.rm = TRUE\" gère les NA\nmean(valeurs, na.rm = TRUE)\n> [1] 13.5\n\n# na.omit omet les valeurs NA dans la nouvelle variable.\nvaleurs.nettoyees <-  na.omit(valeurs)\nmean(valeurs.nettoyees)\n> [1] 13.5"},{"path":"manipuler.html","id":"le-tidyverse","chapter":" 7 Manipuler","heading":"7.2 Le tidyverse","text":"Le nom tidyverse (Wickham et al., 2019) est une contraction de tidy (bien rangé) et de universe. Le tidyverse est fondé sur le concept de tidy data, développé par Hadley (Wickham, 2014). Il repose sur une philosophie d’organisation des données facilitant la gestion, la préparation et le nettoyage préalable aux analyses quantitatives.Plusieurs packages respectent cette philosophie et font partie intégrante du tidyverse, comme ggplot2 (présentation graphique), dplyr (manipulation de données), readr (importation de données), tibble (nouvelle catégorie de data frame), mais bien d’autres également. Ces packages sont tous chargés simultanément avec le package tidyverse.Pour utiliser le package tidyverse, il faut d’abord l’installer puis l’appeler.","code":"\n# Installer le package\ninstall.packages(\"tidyverse\")\n\n# Rendre le package accessible\nlibrary(tidyverse)"},{"path":"manipuler.html","id":"les-fonctions-utiles","chapter":" 7 Manipuler","heading":"7.2.1 Les fonctions utiles","text":"Un des avantages et originalité d’utiliser le tidyverse est d’obtenir l’opérateur %>% (appelée pipe en anglais que l’peut traduire par tuyau) qui provient originellement du package magrittr (Bache & Wickham, 2020) et est importé par dplyr. L’opérateur favorise la lisibilité et la productivité, car il est plus facile de suivre le flux de plusieurs fonctions à travers ces tuyaux que de revenir en arrière lorsque plusieurs fonctions sont imbriquées. En fait, il favorise la lecture par verbes, soit par action (fonction), dans une séquence temporelle intuitive. Si les arguments sont placés en une seule ligne, non seulement la ligne est-elle longue et complexe, voire illisible, mais, en plus, les éléments les plus à gauche (les premiers à la lecture) sont les derniers opérés. Si chacune des fonctions était en ligne, alors il faudrait écraser ou créer des variables temporaires inutiles tout simplement pour arriver à réaliser les fonctions. La philosophie tidyverse, par l’usage de %>%, évite tous ses problèmes.L’opérateur %>% s’ajoute à la fin d’une ligne syntaxe. Son fonctionnement se traduit par l’argument de la ligne à gauche est introduit dans la fonction de droite, et ce, du haut vers le bas. Il peut être commandé plus rapidement avec le raccourci Ctrl + Shift + M sur Windows ou Cmd + Shift + M sur Mac. En plus de l’opérateur %>% , dplyr offre de nouvelles fonctions pour gérer un jeu de données. Quelques-unes des plus importantes sont décrites ici. Par la suite, une mise en situation permet de mieux comprendre leur fonctionnement.","code":""},{"path":"manipuler.html","id":"sélectionner-des-variables","chapter":" 7 Manipuler","heading":"7.2.1.1 Sélectionner des variables","text":"Pour sélectionner des données d’un très grand jeu de données, la fonction select() permet de choisir les variables à conserver. Pour utiliser la fonction, il suffit d’indiquer les variables par leur nom de colonne dans la fonction. Aucun besoin de guillemets anglophones.","code":""},{"path":"manipuler.html","id":"sélectionner-des-participants","chapter":" 7 Manipuler","heading":"7.2.1.2 Sélectionner des participants","text":"Pour filtrer les participants selon les caractéristiques désirées, la fonction filter() permet de sélectionner les unités satisfaisant les conditions spécifiées. Pour utiliser la fonction, il faut indiquer le ou les arguments conditionnels à respecter et sur quelle variable.Voici quelques exemples en rafales à partir du [Jeux de données provenant de R et de packages][jeu de données] cars.Pour filtrer en fonction d’une valeur plus petite < (inclusivement avec <=) ou plus grande > (inclusivement avec >=) ou égale ==.Pour les groupes avec une dénomination en chaîne de caractères, peut faire == \"groupe\" pour choisir un groupe. Les nombres dans les exemples suivant sont entre guillemets anglophones pour rappeler que les noms de groupes sont plus souvent des chaînes de caractères que des nombres.Ou != \"groupe\" pour exclure un groupe.Ou %%  c(\"groupe1\",\"groupe2\") pour choisir les groupes 1 et 2 (mettre tous les groupes désirés dans le c)Ou enfin, !( %% c(\"groupe1)).Voir la section Les clauses conditionnelles pour plus de détails. Il est possible de faire plusieurs agencements de clauses conditionnelles.","code":"\ndata(\"cars\",cars)# Plus petit que\ncars %>% \n  filter(speed < 9)\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n\n# Plus petit ou égale à \ncars %>% \n  filter(speed <= 9)\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6     9   10\n\n# Égale à\ncars %>% \n  filter(speed == 9)\n>   speed dist\n> 1     9   10cars %>% \n  filter(speed == \"10\")\n>   speed dist\n> 1    10   18\n> 2    10   26\n> 3    10   34\ncars %>% \n  filter(speed != \"10\")>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6     9   10# Inclure seulement ces groupes\ncars %>% \n  filter(speed %in% c(\"4\",\"10\"))\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3    10   18\n> 4    10   26\n> 5    10   34# Exclure tous ces groupes\ncars %>% \n  filter(!(speed %in% c(\"4\",\"7\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\")))\n>   speed dist\n> 1     8   16\n> 2     9   10\n> 3    22   66\n> 4    23   54\n> 5    24   70\n> 6    24   92\n> 7    24   93\n> 8    24  120\n> 9    25   85"},{"path":"manipuler.html","id":"transformer-et-créer-des-variables","chapter":" 7 Manipuler","heading":"7.2.1.3 Transformer et créer des variables","text":"Pour créer ou transformer des variables, la fonction mutate() permettra de créer de nouvelles variables à partir des valeurs déjà dans le jeu de données. Il suffit d’indiquer dans la fonction, le calcul qui doit être opérer. Si la nouvelle variable porte le même nom, alors la colonne du jeu de données sera remplacée.Dans ce contexte, la fonction na_if() peut être utile pour retirer une ou des valeurs aberrantes tout en gardant le participant (la ligne) pour les autres variables.Dans cette syntaxe, la valeur 9 (deuxième argument) est remplacée par NA dans la variable speed (premier argument) en appliquant speed = na_if(speed, 9).Si les opérations mathématiques comme la soustraction, division, multiplication, addition, vont de soi avec mutate(), il est possible d’utiliser la fonction pour communément recoder une variable en utilisant case_when(). La fonction prend un argument une série de conditions (à gauche du ~) et retourne le nouvelle valeur (à droite du ~). Par exemple, dist < 5 ~ \"Pas loin\" retourne toutes les valeurs plus petites que 5 (< 5) comme \"Pas loin\". Chaque condition se termine d’une virgule sauf la dernière. Pour identifier toutes autres les conditions non identifiées, dans cet exemple, dist >= 17 qui n’est pas identifiée, il est possible d’utiliser TRUE, qui se traduit par retourne tout les autres valeurs comme trop loin.","code":"cars %>% \n  filter(speed <= 10) %>%         # Utiliser pour raccourcir la sortie\n  mutate(speed = na_if(speed, 9))\n>   speed dist\n> 1     4    2\n> 2     4   10\n> 3     7    4\n> 4     7   22\n> 5     8   16\n> 6    NA   10\n> 7    10   18\n> 8    10   26\n> 9    10   34cars %>% \n  filter(speed < 10) %>% \n  mutate(categorie = case_when(dist < 5 ~ \"Pas loin\",\n                               dist < 17 ~ \"Assez loin\",\n                               TRUE ~ \"Trop loin\"))\n>   speed dist  categorie\n> 1     4    2   Pas loin\n> 2     4   10 Assez loin\n> 3     7    4   Pas loin\n> 4     7   22  Trop loin\n> 5     8   16 Assez loin\n> 6     9   10 Assez loin"},{"path":"manipuler.html","id":"sommariser-les-informations-pertinentes","chapter":" 7 Manipuler","heading":"7.2.1.4 Sommariser les informations pertinentes","text":"Pour obtenir des informations sur le jeu de données ainsi créées, la fonction summarise() permettra notamment d’obtenir des statistiques d’intérêt. En ajoutant, dans la fonction, les fonctions désirées, comme mean() ou sd(), avec les variables sur lesquelles elles devraient être opérées ou encore n() pour connaître la taille des groupes.S’il y des groupes ou des catégories, le sommaire peut être divisé avec la fonction group_by() où la variable nominale est spécifiée.","code":""},{"path":"manipuler.html","id":"autres-fonctions","chapter":" 7 Manipuler","heading":"7.2.1.5 Autres fonctions","text":"Il existe plusieurs autres fonctions possibles. Notamment, slice() permet de choisir les unités désirées en passant comme argument la base de données et le ou les numéros de ligne; sample_slice() qui est très similaire, retourne des lignes aléatoires; rename(), similaire à select(), permet de renommer les variables; arrange reclasse par ordre croissant en fonction d’une variable placée en argument. Et il y en plusieurs autres.","code":""},{"path":"manipuler.html","id":"mise-en-situation","chapter":" 7 Manipuler","heading":"7.2.2 Mise en situation","text":"Pour mettre en pratique la philosophie tidyverse, voici un exemple tiré du jeu de données starwars (disponible du tidyverse). Ce jeu de données possède de nombreuses caractéristiques (diversité de variables, de mesures, données manquantes) qui en font un jeu de données similaires à ce qu’un expérimentateur pourrait obtenir.Sans plus de préliminaire, la fonction head() donne un aperçu du jeu de donnéesPour obtenir de l’information sur ce jeu de données.Voici la description du jeu de données (traduction libre),Les données d’origine, issues de SWAPI, l’API de Star Wars, https://swapi.dev/, ont été révisées pour tenir compte des recherches supplémentaires sur la détermination du genre et du sexe des personnages.Peu utile comme descripteur, une inspection des données est plus informative. Pour afficher le jeu de données dans un nouvel onglet.Le fichier contient, le nom de 87 personnages mesurés sur 14 variables, soitle nom;le nom;la taille (cm);la taille (cm);le poids (kg);le poids (kg);la couleur des cheveux, de la peau et des yeux (trois variables);la couleur des cheveux, de la peau et des yeux (trois variables);l’année de naissance;l’année de naissance;le sexe biologique (mâle, femelle, hermaphrodite ou aucun);le sexe biologique (mâle, femelle, hermaphrodite ou aucun);le genre;le genre;la planète natale;la planète natale;l’espèce;l’espèce;une liste de films où le personnage apparaît;une liste de films où le personnage apparaît;une liste des véhicules que le personnage piloté;une liste des véhicules que le personnage piloté;une liste des vaisseaux que le personnage piloté.une liste des vaisseaux que le personnage piloté.À partir de ce jeu de données, l’objectif de cette mise en situation est de comparer les hommes et les femmes humaines par rapport à leur indice de masse corporelle (IMC) ou body mass index (BMI). Le calcul de l’IMC consiste à diviser le poids par la taille au carré (kg/m2).Les étapes à considérer sont les suivantes : sélectionner les variables pertinentes, filtrer en retirant les unités d’espèces non humaines, tenir compte des données manquantes, corriger la taille des unités qui devrait être en mètre et non en centimètre (divisé par 100) et créer l’indice de masse corporelle.Les étapes de la syntaxe se lisent comme suit :La première ligne starwars %>% indique l’objet sur lequel il faut passer les fonctions subséquentes et la sortie est assignée à jd;La première ligne starwars %>% indique l’objet sur lequel il faut passer les fonctions subséquentes et la sortie est assignée à jd;puis, select(sex, mass, height, species) %>% indique les variables à conserver pour les fonctions subséquentes;puis, select(sex, mass, height, species) %>% indique les variables à conserver pour les fonctions subséquentes;puis, filter(species == \"Human\") filtre les unités qui sont humains et passe aux fonctions subséquentes;puis, filter(species == \"Human\") filtre les unités qui sont humains et passe aux fonctions subséquentes;puis, na.omit() %>% retire les valeurs manquantes des unités dans le jeu de données et passe aux fonctions subséquentes;puis, na.omit() %>% retire les valeurs manquantes des unités dans le jeu de données et passe aux fonctions subséquentes;puis, mutate(height = height  / 100) %>%, transforme la variable height et passe à la dernière fonction;puis, mutate(height = height  / 100) %>%, transforme la variable height et passe à la dernière fonction;enfin, mutate(IMC = mass / height^2) crée la variable d’IMC.enfin, mutate(IMC = mass / height^2) crée la variable d’IMC.Si une méthode plus traditionnelle avait été utilisée, la syntaxe pourrait ressembler à ceci.Le jeu de données est créé en autant de ligne de syntaxe. Par contre, la lecture n’est pas aussi intuitive que l’opérateur %>% et les fonctions select(), filter(), mutate(). Il ne faut pas trop penser à quoi ressemblerait ces manipulations en une seule ligne de syntaxe.","code":"starwars[,1:6]\n> # A tibble: 87 × 6\n>    name         height  mass hair_color skin_color eye_color\n>    <chr>         <int> <dbl> <chr>      <chr>      <chr>    \n>  1 Luke Skywal…    172    77 blond      fair       blue     \n>  2 C-3PO           167    75 <NA>       gold       yellow   \n>  3 R2-D2            96    32 <NA>       white, bl… red      \n>  4 Darth Vader     202   136 none       white      yellow   \n>  5 Leia Organa     150    49 brown      light      brown    \n>  6 Owen Lars       178   120 brown, gr… light      blue     \n>  7 Beru Whites…    165    75 brown      light      blue     \n>  8 R5-D4            97    32 <NA>       white, red red      \n>  9 Biggs Darkl…    183    84 black      light      brown    \n> 10 Obi-Wan Ken…    182    77 auburn, w… fair       blue-gray\n> # ℹ 77 more rows\n?starwars\nView(starwars)jd <-  starwars %>% \n  select(sex, mass, height, species) %>% \n  filter(species == \"Human\") %>% \n  na.omit() %>% \n  mutate(height = height  / 100) %>% \n  mutate(IMC = mass / height^2)  \njd\n> # A tibble: 22 × 5\n>    sex     mass height species   IMC\n>    <chr>  <dbl>  <dbl> <chr>   <dbl>\n>  1 male      77   1.72 Human    26.0\n>  2 male     136   2.02 Human    33.3\n>  3 female    49   1.5  Human    21.8\n>  4 male     120   1.78 Human    37.9\n>  5 female    75   1.65 Human    27.5\n>  6 male      84   1.83 Human    25.1\n>  7 male      77   1.82 Human    23.2\n>  8 male      84   1.88 Human    23.8\n>  9 male      80   1.8  Human    24.7\n> 10 male      77   1.7  Human    26.6\n> # ℹ 12 more rowsjd <- starwars[, c(\"sex\", \"mass\", \"height\", \"species\")]  # select()\njd <- jd[jd[, \"species\"] == \"Human\",]                    # filter()\njd <- na.omit(jd)                                        # na.omit()\njd[,\"height\"] <- jd[,\"height\"] / 100                     # mutate()\njd[,\"IMC\"] <- jd[,\"mass\"] / jd[,\"height\"]^2              # mutate()\njd\n> # A tibble: 22 × 5\n>    sex     mass height species   IMC\n>    <chr>  <dbl>  <dbl> <chr>   <dbl>\n>  1 male      77   1.72 Human    26.0\n>  2 male     136   2.02 Human    33.3\n>  3 female    49   1.5  Human    21.8\n>  4 male     120   1.78 Human    37.9\n>  5 female    75   1.65 Human    27.5\n>  6 male      84   1.83 Human    25.1\n>  7 male      77   1.82 Human    23.2\n>  8 male      84   1.88 Human    23.8\n>  9 male      80   1.8  Human    24.7\n> 10 male      77   1.7  Human    26.6\n> # ℹ 12 more rows"},{"path":"manipuler.html","id":"décrire-et-analyser-en-tidyverse","chapter":" 7 Manipuler","heading":"7.3 Décrire et analyser en tidyverse","text":"Une fois le jeu de données prêt, il est possible d’obtenir les informations sommaires. Ici, la moyenne, l’écart type, la valeur minimale, maximale et la taille de chaque sont demandés en fonction du sexe. À cette étape, l’avantage d’embrasser la philosophie tidyverse apparaît, en quelques lignes rudimentaires, les cinq statistiques demandées (mean(),sd(),min(),max(),n()) sont affichées, et ce, par groupes (group_by()) l’aide de la fonction summarise().Le jeu de données issu de ces opérations peut être utilisé normalement pour réaliser des analyses statistiques. Cela sera abordé dans les prochains chapitres. Il existe toutefois des packages comme rstatix avec lequel il est possible de faire des test-\\(t\\) avec test_t(), des corrélations avec cor_test() et des analyses de variance anova_test() tout en demeurant dans le tidyverse7.Pour le test-\\(t\\) avec t_test() et anova_test(), il faut demander la variable dépendante à gauche et la variable de groupement à gauche. Les deux variables sont séparées par le ~ (tilde, voir L’analyse de régression avec R pour plus de renseignements). Pour la corrélation avec cor_test(), il faut s’assurer de sélectionner uniquement les variables ayant une échelle continue.Cela dit, l’utilisateur préférera probablement utiliser d’autres méthodes lorsque des analyses statistiques seront nécessaires.","code":"jd %>% \n  group_by(sex) %>% \n  summarise(mean(IMC), sd(IMC), min(IMC), max(IMC), n()) \n> # A tibble: 2 × 6\n>   sex    `mean(IMC)` `sd(IMC)` `min(IMC)` `max(IMC)` `n()`\n>   <chr>        <dbl>     <dbl>      <dbl>      <dbl> <int>\n> 1 female        22.0      5.51       16.5       27.5     3\n> 2 male          26.0      4.29       21.5       37.9    19library(rstatix)\n\n# Test-t sur l'IMC en fonction du sexe\njd %>% \n  t_test(IMC ~ sex)\n> # A tibble: 1 × 8\n>   .y.   group1 group2    n1    n2 statistic    df     p\n> * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n> 1 IMC   female male       3    19     -1.23  2.40 0.326\n\n# Analyse de corrélations\njd %>% \n  select(IMC, mass, height) %>% \n  cor_test() \n> # A tibble: 9 × 8\n>   var1   var2     cor statistic         p conf.low conf.high\n>   <chr>  <chr>  <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n> 1 IMC    IMC     1      2.12e+8 5.26e-155    1.00      1    \n> 2 IMC    mass    0.85   7.32e+0 4.47e-  7    0.674     0.938\n> 3 IMC    height  0.18   8.13e-1 4.26e-  1   -0.262     0.558\n> 4 mass   IMC     0.85   7.32e+0 4.47e-  7    0.674     0.938\n> 5 mass   mass    1      3.00e+8 5.13e-158    1         1    \n> 6 mass   height  0.65   3.84e+0 1.02e-  3    0.317     0.842\n> 7 height IMC     0.18   8.13e-1 4.26e-  1   -0.262     0.558\n> 8 height mass    0.65   3.84e+0 1.02e-  3    0.317     0.842\n> 9 height height  1    Inf       0            1         1    \n> # ℹ 1 more variable: method <chr>\n\n# Anayse de variances (ANOVA)\njd %>% \n  anova_test(IMC ~ sex) \n> ANOVA Table (type II tests)\n> \n>   Effect DFn DFd    F     p p<.05 ges\n> 1    sex   1  20 2.21 0.152       0.1"},{"path":"visualiser.html","id":"visualiser","chapter":" 8 Visualiser","heading":" 8 Visualiser","text":"La visualisation de données est l’un des deux objectifs fondamentaux de R (l’autre étant évidemment de faire des statistiques). Il existe plusieurs méthodes et packages pour produire rapidement et simplement des graphiques. Beaucoup de matériel se retrouve en ligne pour maîtriser les graphiques, mais surtout les personnaliser. L’objectif, bien modeste, de cette section n’est pas de rendre le lecteur maître de la production de figures, mais bien de lui faire faire ses premiers pas et de l’outiller pour qu’il puisse produire simplement et rapidement des graphiques de qualité.L’exemple de cette section est basé sur celui de la section Manipuler. Voici la syntaxe pour obtenir le jeu de données de nouveau.","code":"\njd <- starwars %>% \n  select(name, sex, mass, height, species) %>% \n  filter(species == \"Human\") %>% \n  na.omit() %>% \n  mutate(height = height  / 100) %>% \n  mutate(IMC = mass / height^2)  "},{"path":"visualiser.html","id":"ggplot2","chapter":" 8 Visualiser","heading":"8.1 ggplot2","text":"Le package ggplot2 est une extension du tidyverse avec lequel il est possible de créer simplement et rapidement des graphiques. Ces graphiques sont de qualité de publications, idéale pour les articles scientifiques. Le package fournit un langage graphique pour la création intuitive de graphiques compliqués. Il permet à l’utilisateur de créer des graphiques qui représentent des données numériques et catégorielles univariées et multivariées.La logique de ggplot2 repose sur la grammaire des graphiques (Grammar Graphics), l’idée selon laquelle toutes les figures peuvent être construites à partir des mêmes composantes. Il s’agit de la deuxième version du package. Voilà pour l’appellation ggplot2.Dans la grammaire de graphique, une figure possède huit niveaux, dont les trois principaux sont les suivants :data, les données utilisées;data, les données utilisées;mapping (aesthetic), cartographier les variables, c’est-à-dire, établir la carte des variables (abscisses, ordonnées, couleur, forme, taille, etc.);mapping (aesthetic), cartographier les variables, c’est-à-dire, établir la carte des variables (abscisses, ordonnées, couleur, forme, taille, etc.);geometric représentation, la représentation géométrique ou le type de représentation graphique, par exemple, diagramme de dispersion, histogramme, boîte à moustache, etc.geometric représentation, la représentation géométrique ou le type de représentation graphique, par exemple, diagramme de dispersion, histogramme, boîte à moustache, etc.Les cinq autres statistics, facet, coordinate space, labels, theme permettent de personnaliser la figure.Les composantes les plus importantes sont les trois premières, soit les données, la cartographie et la représentation géométrique. Ce sont les éléments de base pour débuter le graphique. Les autres composantes viennent bonifier la figure tout en l’ajustant au besoin de l’utilisateur.La fonction ggplot() met en place la figure. Le résultat d’utiliser la fonction ggplot() seule est illustrée à la Figure 8.1\nFigure 8.1: La fonction ggplot() seule - Rien\nIl est aussi possible de piper (prononcé avec un fort accent anglophone) les données dans la fonction.Pour afficher des graphiques, il faut ajouter +, puis une représentation géométrique ainsi que la cartographie (mapping). La cartographie (aes(mapping = ), où aes désigne l’esthétisme, aesthetic) peut se trouver dans ggplot() ou dans la représentation géométrique. Si elle est dans ggplot, elle est passée aux autres niveaux.Voici une liste des représentations géométriques possibles :geom_line() crée une ligne qui lie toutes les valeurs, très utiles pour une série temporelle (abscisse = temps, ordonnée = variable dépendante);geom_line() crée une ligne qui lie toutes les valeurs, très utiles pour une série temporelle (abscisse = temps, ordonnée = variable dépendante);geom_point() crée un diagramme de dispersion ou un nuage de point, très utile pour les corrélations;geom_point() crée un diagramme de dispersion ou un nuage de point, très utile pour les corrélations;geom_bar() crée un diagramme à bâton, idéal pour présenter des proportions, des fréquences ou des données comptées;geom_bar() crée un diagramme à bâton, idéal pour présenter des proportions, des fréquences ou des données comptées;geom_histogram() crée un histogramme des variables;geom_histogram() crée un histogramme des variables;geom_box() crée une boîte à moustache, idéal pour identifier des valeurs aberrantes et comparer la variabilité entre des groupes;geom_box() crée une boîte à moustache, idéal pour identifier des valeurs aberrantes et comparer la variabilité entre des groupes;geom_smooth() crée la ligne de prédiction des données avec des intervalles de confiances, la plupart des utilisateurs voudront certainement ces arguments method = lm (par défaut) ou sans l’erreur standard (se = FALSE);geom_smooth() crée la ligne de prédiction des données avec des intervalles de confiances, la plupart des utilisateurs voudront certainement ces arguments method = lm (par défaut) ou sans l’erreur standard (se = FALSE);geom_errorbar() ajoute des barres d’erreur ou des intervalles de confiances spécifiées.geom_errorbar() ajoute des barres d’erreur ou des intervalles de confiances spécifiées.Certaines cartographies sont d’ailleurs compatibles, geom_smooth() et geom_point(), par exemple.La Figure 8.2 montre un diagramme de dispersion construit à partir du jeu de données jd pipé dans la fonction ggplot(). Dans cette fonction, la cartographie est passée mapping = aes(x = mass, y = height) à un second niveau, geom_point) par le + et la représentation est produite.\nFigure 8.2: Diagramme de dispersion\nVoici une liste d’exemples de différentes représentations visuelles des données.","code":"\nggplot(data = jd)\njd %>% \n  ggplot()\njd %>% \n  ggplot(mapping = aes(x = mass, y = height)) + \n  geom_point()"},{"path":"visualiser.html","id":"diagramme-de-dispersion","chapter":" 8 Visualiser","heading":"8.2 Diagramme de dispersion","text":"Pour réaliser un diagramme de dispersion, la fonction se nomme geom_point(). La cartographie identifie la variable à l’axe des \\(x\\) (horizontal) et des \\(y\\) (vertical). Dans cet exemple, il s’agit du poids (\\(x\\)) et de la taille (\\(y\\)). La cartographie ne se limite pas aux axes par contre. Dans cet exemple, la forme shape est aussi un dimension manipulée. Il peut s’agir de color et même de size. Dans la syntaxe ci-dessous, l’argument size est placé à l’extérieur de mapping. Il s’agit alors d’une constante (elle change la taille des points), c’est-à-dire qu’elle ne varie pas relativement à une variable.\nFigure 8.3: Le lien entre le poids et la taille en fonction du sexe\nLa Figure 8.4 montre le résultat si `size``est ajouté au mapping pour identifier l’IMC. Les unités avec un plus grand IMC obtiennent un plus gros pointeur.\nFigure 8.4: Le lien entre le poids et la taille en fonction de l’IMC et du sexe\npeut y ajouter la droite de régression, comme la Figure 8.5 le montre. Sans geom_point(), la figure ne produit la droite. Les arguments de geom_smooth() indique l’utilisation du modèle linéaire, method = lm, et l’absence des intervalles de confiance, se = FALSE. Dans cette syntaxe, comme le mapping est ajouté à ggplot directement, il se généralise directement à geom_point() et geom_smooth()\nFigure 8.5: Le lien entre le poids et la taille en fonction de l’IMC\n","code":"\njd %>% \n  ggplot() + \n  geom_point(mapping = aes(x = mass, y = height, shape = sex), size = 2) \njd %>% \n  ggplot() + \n  geom_point(mapping = aes(x = mass, y = height, shape = sex, size = IMC)) jd %>% \n  ggplot(mapping = aes(x = mass, y = height)) + \n  geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\")\n> `geom_smooth()` using formula = 'y ~ x'"},{"path":"visualiser.html","id":"boîte-à-moustache","chapter":" 8 Visualiser","heading":"8.3 Boîte à moustache","text":"La boîte à moustaches (box--whisker plot) est une figure permettant de voir la variabilité des données. Elle résume seulement quelques indicateurs de position soit la médiane, les quartiles, le minimum, et le maximum. Ce diagramme est utilisé principalement pour détecter des valeurs aberrantes et comparer la variabilité entre les groupes. C’est la représentation géométrique geom_boxplot() qui permettra de créer des boîtes à moustache. La cartographie prend en argument une variable nominale en x et une variable continue en y. La Figure 8.6 montre un exemple de boîte à moustache.\nFigure 8.6: Boîte à moustache de l’IMC en fonction du sexe\nUne fonction intéressante est la fonction coord_flip() qui tourne (flip) les axes, les coordonnées. L’axe \\(x\\) prend la place de \\(y\\); \\(y\\) prend la place de \\(x\\). Elle peut être pratique pour améliorer la qualité visuelle de certains graphiques.","code":"\nggplot(data = jd) + \n  geom_boxplot(mapping = aes(x = sex, y = IMC)) +\n  coord_flip()"},{"path":"visualiser.html","id":"histogramme","chapter":" 8 Visualiser","heading":"8.4 Histogramme","text":"Un histogramme permet de représenter la répartition empirique d’une variable. Il donne un aperçu de la distribution sous-jacente, soit comment les données sont distribuées. Cette figure permet de voir la forme de la distribution et permet de voir si elle ne démontre pas d’anomalie. La représentation graphique geom_histogram() produit des histogrammes. S’il faut en produire pour différentes variables, une stratégie simple est de les produire en série.Des techniques plus avancées permettent de créer la Figure 8.7 d’un seul coup8.\nFigure 8.7: Histogrammes des variables continues\nEnfin, s’il est désiré de comparer deux distributions de groupes différents, l’argument fill dans la cartographie indique à la fonction de différencier les valeurs selon le remplissage des histogrammes.\nFigure 8.8: Histogrammes de l’IMC par rapport au sexe\nDans la Figure 8.8, l’argument position = \"identity\" indique de traiter les deux groupes comme différents, autrement les colonnes s’additionnent dans le graphique. L’argument alpha = .7 permet une transparence entre les couleurs, autrement, les valeurs derrière les autres ne paraissent pas. La valeur de alpha va de 0 (transparent) à 1 (opaque) et fonctionne dans la plupart des contextes, surtout ceux liés à ggplot2.","code":"\n# Trois histogrammes en trois figures\nggplot(data = jd) + \n  geom_histogram(mapping = aes(x = height))\n\nggplot(data = jd) + \n  geom_histogram(mapping = aes(x = mass))\n\nggplot(data = jd) + \n  geom_histogram(mapping = aes(x = IMC))\n# Trois histogrammes en une seule figure\n# en optimisant avec le tidyverse\njd %>%\n  keep(is.numeric) %>% \n  gather() %>% \n  ggplot(aes(value)) +\n  facet_wrap(~ key, scales = \"free\") +\n  geom_histogram()\njd %>% \n  ggplot(mapping = aes(x = IMC, fill = sex)) + \n  geom_histogram(position = \"identity\", alpha = .7) + \n  scale_fill_grey()"},{"path":"visualiser.html","id":"les-barres-derreurs","chapter":" 8 Visualiser","heading":"8.5 Les barres d’erreurs","text":"Les barres d’erreur sont une représentation géométrique à part entière. La fonction pour les commandées est geom_errorbar(). Elle nécessite deux arguments, soit l’intervalle de confiance maximale et minimale autour des moyennes à afficher.La Figure 8.9 illustre les différences entre moyennes avec des barres d’erreur à partir de la base de données ToothGrowth, une étude de l’effet de la vitamine C (dose) selon leur administration (jus ou supplément supp) sur la longueur des dents des cochons d’inde. Il y deux facteurs et une variable continue.La première étape est de tirer les statistiques sommaires, moyennes, écart type, tailles des groupes. La syntaxe tire profit de group_by() pour tirer les groupes et en faire le sommaire. Le sommaire summarise permet d’obtenir les statistiques, notamment la moyenne, l’erreur standard (se) pour en calculer l’intervalle autour de la moyenne ci.\nFigure 8.9: Les effets de la vitamine C sur les cochons d’inde\nUne fois ces statistiques calculées et enregistrées dans le nouveau jeu de données jd, il est possible de créer le graphique avec les représentations géométriques désirées. Remarquez comment spécifier la cartographie dans le niveau ggplot() rend la syntaxe moins compliquée. Cette syntaxe produit un graphique avec dose à l’axe des \\(x\\), supp comme pointeurs et les moyennes de len (longueur moyenne des dents). La fonction geom_errorbar() indique où placer les limites inférieures et supérieures des intervalles. Les arguments size = 5 et width = .05 sont ajoutés simplment pour l’esthétisme. L’argument .groups = \"drop\" de summarise permet d’éviter une avertissement expliquant qu’une variable de groupement est utilisé pour regrouper les résultats à la fin. Ajouter ou retirer cet argument ne change pas les calculs, ni la Figure 8.9.","code":"\nstat.descr <- ToothGrowth %>% \n  group_by(dose, supp) %>% \n  summarise(mlen = mean(len),\n            sdlen = sd(len),\n            nlen = n(), \n            se = sd(len)/sqrt(n()), \n            ci = qt(.975, df = n()-1) * se,\n            .groups = \"drop\")\n\nstat.descr %>% \n  ggplot(aes(x = dose,\n             y = mlen, \n             shape = supp),\n         size = 5) + \n    geom_errorbar(aes(ymin = mlen - ci,\n                      ymax = mlen + ci), \n                  width = .05) +\n    geom_line() +\n    geom_point()"},{"path":"visualiser.html","id":"de-meilleures-barres-derreurs-pour-les-devis-inter-participants","chapter":" 8 Visualiser","heading":"8.6 De meilleures barres d’erreurs pour les devis inter participants","text":"Il existe un package superb (Cousineau et al., 2021) qui permet d’obtenir des graphiques à barre d’erreur avec précision et facilement ajustable. Une fois installé et importé dans l’environnement, superb offre la fonction principale superbPlot permet la création de ces figures.Il y deux avantages principaux utilisé superb. La première est qu’elle permet des ajustements avec l’argument adjustments afin de préciser le type de barres d’erreurs9, comme \"single\", \"difference\", ou \"tryon\". Généralement, ce sera l’option purpose = \"difference\" qui sera désirée. Deuxièmement, superb tient aussi compte des devis intra participants avec l’argument WSFactors, ce qui permet l’utilisation de différentes techniques de décorrélation des temps de mesure. La fonction produit de bien meilleurs graphiques à barres d’erreurs avec plus d’ajustement et de précision.La Figure 8.10 reproduit la Figure 8.9. Dans le code, il faut préciser les facteurs inter participants BSFactors (pour subject ou BS) et la variable dépendante, variable. La fonction contrôle aussi le type de graphique avec plotStyle.\nFigure 8.10: Les effets de la vitamine C sur les cochons d’inde avec superb\nLa fonction retourne souvent des messages d’avertissement (orange) pour préciser certaines décisions qu’elle peut avoir pris. Le code ci-dessus retourne le message : superb::FYI: variables plotted order: dose, supp (use factorOrder change).. C’est à l’utilisateur d’en prendre note et de s’assurer que c’était bien ce qui était désiré, ce qui est le cas ici.Pour plus de flexibilité pour l’utilisateur, les statistiques descriptives peuvent être obtenues afin de produire personnellement les figures, comme cela avait été fait dans le premier exemple sur Les barres d’erreurs.Attention! Cette variable est une liste contenant deux éléments, les statistiques descriptives ($summaryStatistics) et les données brutes ($rawData). Elle peuvent être extraites avec le signe $, comme stat.descr$summaryStatistics.Pour reproduire la Figure 8.9 avec le jeu de données extrait de superbData(), il faut procéder à quelques ajustements, comme le nom des variables qui ne sont pas les mêmes, et le fait que la variable dose est maintenant traitée en variable nominale, alors qu’il est souhaitable qu’elle soit numérique pour utiliser la représentation géométrique geom_line(). La Figure 8.11 illustre le résultat.\nFigure 8.11: Les effets de la vitamine C sur les cochons d’inde avec superb\n","code":"\nlibrary(superb)\nsuperbPlot(ToothGrowth, \n    BSFactors = c(\"dose\",\"supp\"), \n    variables = \"len\",\n    plotStyle = \"line\")\nstat.descr <- superbData(ToothGrowth, \n                         BSFactors = c(\"dose\",\"supp\"), \n                         variables = \"len\",\n                         adjustments = list(purpose = \"difference\"))\nstat.descr$summaryStatistics %>% \n  ggplot(aes(x = as.numeric(dose),\n             y = center, \n             shape = supp),\n         size = 5) + \n    geom_errorbar(aes(ymin = center + lowerwidth,\n                      ymax = center + upperwidth), \n                  width = .05) +\n    geom_line() +\n    geom_point()"},{"path":"visualiser.html","id":"de-meilleures-barres-derreur-pour-les-devis-intra-participants","chapter":" 8 Visualiser","heading":"8.7 De meilleures barres d’erreur pour les devis intra participants","text":"La fonction superbPlot() permet non seulement de produire des barres d’erreurs pour les devis inter participants (deux ou plusieurs groupes de participants différent dans chaque groupe), mais également pour les devis intra partipants (les mêmes participants mesurés plusieurs fois). Elle excelle d’ailleurs dans ce domaine, car elle permet d’ajuster pour type d’intervalle de confiance désiré, mais aussi pour l’ajustement intra participant.Voici un jeu de données synthétiques pour réaliser une figure avec un devis intra participant. Voir Le test-\\(t\\) dépendant pour plus de détails sur la création de ces données.Pour produire la figure, il faut définir le facteur intra participant (within subject ou WS) par l’argument WSFactors. Cette argument est particulier, il nécessite un mot arbitraire pour identifier l’effet temporelle, ici WSFactors = \"temps, mais aussi entre parenthèses, le nombre de temps de mesures, ici \"(2)\", ce qui forme l’argument complet WSFactors = \"temps(2)\". Ensuite, pour la variable dépendante, combine ensemble tous les temps de mesure spécifiés, ici variables = c(\"temps1\", \"temps2\"). Il reste à définir le style de graphique et les ajustements. Pour l’objectif (purpose), ce sont les mêmes options que pour les devis inter participants, soit (\"single\", \"difference\" ou \"tryon\").\nFigure 8.12: Comparaison de deux temps de mesure avec superb (sans décorrélation)\nLa Figure 8.12 montre le résultat obtenu.Le package superb permet aussi l’utilisation de techniques de décorrélation comme \"CM\", \"LM\", \"CA\" ou \"none\" (par défaut) pour améliorer les intervalles de confiance. Consultez la documentation pour en savoir plus sur son fonctionnement et ce qui conviendra le mieux à la situation qui se présente. Pour l’implantation, il suffit d’ajouter à la liste d’arguments fournie à adjustements, le type de décorrlation désirée, ici decorrelation = \"CM\").\nFigure 8.13: Comparaison de deux temps de mesure avec superb (avec décorrélation)\nLes barres d’erreur de la Figure 8.13 sont légèrement différentes de la Figure 8.12, mais plus adéquates pour illustrer les résultats.","code":"\n# Un exemple de jeu de données\nset.seed(148)\ntemps1 <- rnorm(n = 25, mean = 0, sd = 2)\ndifference <- rnorm(n = 25, mean = 2, sd = 2)\ntemps2 <- temps1 + difference\njd_intra <- data.frame(temps1 = temps1,\n                 temps2 = temps2)\nsuperbPlot(jd_intra, \n    WSFactors = \"temps(2)\", \n    variables = c(\"temps1\", \"temps2\"),\n    plotStyle = \"line\",\n    adjustments = list(purpose = \"difference\"))\nsuperbPlot(jd_intra, \n    WSFactors = \"temps(2)\", \n    variables = c(\"temps1\", \"temps2\"),\n    plotStyle = \"line\",\n    adjustments = list(purpose = \"difference\", decorrelation = \"CM\"))"},{"path":"visualiser.html","id":"quelques-trucs-en-rafale","chapter":" 8 Visualiser","heading":"8.8 Quelques trucs en rafale","text":"Il est possible de renommer les axes avec xlab() et ylab() et le titre avec labs().Plusieurs ajustements des axes sont possibles avec scale_y_continuous() et scale_x_continuous() et leur équivalent nominal scale_y_discrete(), scale_x_discrete() , comme ajuster les limites (limits), les marqueurs (breaks) et les libellées des marqueurs (labels).Il est possible de séparer une Figure en différents cadran en spécifiant une variable de séparation avec facet_wrap() ou facet_grid().","code":""},{"path":"visualiser.html","id":"exporter-la-figure","chapter":" 8 Visualiser","heading":"8.9 Exporter la figure","text":"Pour exporter une figure, ggplot2 offre la très conviviale fonction ggsave() qui permet d’enregistrer la dernière figure produite en fichier. Celle-ci vient avec plusieurs options pour gérer l’enregistrement.L’option filename gère le nom du fichier le nom et le type de fichier, comme les usuels \"pdf\", \"jpeg\", \"png\", ou les moins fréquents, mais aussi pratiques \"bmp\", \"eps\", \"tiff\", \"ps\", \"tex\", \"svg\" et \"wmf\".Les options width (largeur), height (hauteur), et units (unités, comme \"\", pouce, \"cm\", centimètre, \"mm\", millimètre ou \"px\", pixel) gère la taille de la figure.La taille de résolution de la figure est gérée avec l’argument dpi, ce qui peut être utile pour augmenter la qualité de la figure produite.Voici un exemple.Il faudra éventuellement ajuster la taille et la qualité de la figure en fonction de la sortie désirée. Quelques essais seront probablement nécessaires pour y arriver.","code":"# Préalablement produire une figure\nggsave(filename = \"mafigure.pdf\",\n       width = \"6\"\n       heigth = \"8\",\n       unit = \"cm\")"},{"path":"visualiser.html","id":"pour-aller-plus-loin-1","chapter":" 8 Visualiser","heading":"8.10 Pour aller plus loin","text":"Il existe une multitudes de livres, de sites web, de tutoriels en ligne et d’atelier pour donner l’occasion au lecteur d’aller plus loin dans sa conception graphique. Voici quelques ouvrages de références : Le R Graphics Cookbook (Chang) repérable à https://r-graphics.org/, ggplot2: elegant graphics data analysis (Wickham) repérable à https://ggplot2-book.org/ ou R Graphics (Murrel) repérable à https://www.stat.auckland.ac.nz/~paul/RG2e/.","code":""},{"path":"exercice-gestion.html","id":"exercice-gestion","chapter":"Exercices","heading":"Exercices","text":"À l’aide de data_edit() du package DataEditR, créez un jeu données contenant trois participants ayant les caractéristiques suivantes, nom = Alexandre, Samuel et Vincent et age = 20, 22 et 31.À l’aide de data_edit() du package DataEditR, créez un jeu données contenant trois participants ayant les caractéristiques suivantes, nom = Alexandre, Samuel et Vincent et age = 20, 22 et 31.Prendre le jeu de données cars, sélectionner la variable dist et transformer la en mètre, plutôt qu’en pieds. Rappel: un mètre égale 3.2808 pieds.Prendre le jeu de données cars, sélectionner la variable dist et transformer la en mètre, plutôt qu’en pieds. Rappel: un mètre égale 3.2808 pieds.Dans le jeu de données iris, calculer la moyenne et l’écart type de la longueur de sépale (Petal.Length) en fonction de l’espèce (species). Représenter ensuite la moyenne à l’aide d’un diagramme à barreDans le jeu de données iris, calculer la moyenne et l’écart type de la longueur de sépale (Petal.Length) en fonction de l’espèce (species). Représenter ensuite la moyenne à l’aide d’un diagramme à barrePrenez le jeu de données mtcars et produisez un diagramme de dispersion montrant la puissance brute (en chevaux) (hp) par rapport à consommation en km/l (basé sur mpg) tout en soulignant l’effet du nombre de cylindres (cyl). Attention la fonction as_factor permettra d’utiliser cyl en facteur et le rapprt mpg vers kml approximativement \\(.425\\).Prenez le jeu de données mtcars et produisez un diagramme de dispersion montrant la puissance brute (en chevaux) (hp) par rapport à consommation en km/l (basé sur mpg) tout en soulignant l’effet du nombre de cylindres (cyl). Attention la fonction as_factor permettra d’utiliser cyl en facteur et le rapprt mpg vers kml approximativement \\(.425\\).Avec le même jeu de données et objectif que la question précédente, ajouter une droite de prédiction avec geom_smooth() selon un modèle linéare (lm) et sans erreur standard (se).Avec le même jeu de données et objectif que la question précédente, ajouter une droite de prédiction avec geom_smooth() selon un modèle linéare (lm) et sans erreur standard (se).Avec le jeu de données chickwts, produire une boîte à moustache du poids des poulets en fonction de leur alimentation.Avec le jeu de données chickwts, produire une boîte à moustache du poids des poulets en fonction de leur alimentation.Prenez le jeu de données mtcars et produisez un histogramme montrant la variabilité de la consommation mpg par rapport à la transmission (). Attention la fonction as_factor permettra d’utiliser en facteur.Prenez le jeu de données mtcars et produisez un histogramme montrant la variabilité de la consommation mpg par rapport à la transmission (). Attention la fonction as_factor permettra d’utiliser en facteur.Prendre le jeu de données msleep et produire un diagramme à barres pour observer la fréquence des régimes alimentaires.Prendre le jeu de données msleep et produire un diagramme à barres pour observer la fréquence des régimes alimentaires.Prendre le jeu de données msleep et produisez une boîte à moustache pour observer le temps total de sommeil (sleep_total) moyen par rapport aux régimes (vore). Attention aux données manquantes.Prendre le jeu de données msleep et produisez une boîte à moustache pour observer le temps total de sommeil (sleep_total) moyen par rapport aux régimes (vore). Attention aux données manquantes.Avec le jeu de données chickwts, produire un diagramme à barres du poids moyen des poulets par rapport à leur alimentation en ne conservant que les graines de tournesols et les fèveroles.Avec le jeu de données chickwts, produire un diagramme à barres du poids moyen des poulets par rapport à leur alimentation en ne conservant que les graines de tournesols et les fèveroles.","code":""},{"path":"décrire.html","id":"décrire","chapter":" 9 Décrire","heading":" 9 Décrire","text":"En se basant sur les chapitres Calculer et Manipuler, l’utilisateur possède les fondements pour obtenir les informations sommaires d’un jeu de données. Il voudra régulièrement réaliser des analyses descriptives sur ce dernier.","code":""},{"path":"décrire.html","id":"les-variables-à-échelles-continues","chapter":" 9 Décrire","heading":"9.1 Les variables à échelles continues","text":"Une méthode simple et efficace pour obtenir des indices statistiques pour des variables à échelle continue est de passer par le package psych (Revelle, 2021) qui offre deux fonctions intéressantes : describe() et describeBy(). En mettant simplement en argument le jeu de données, ces fonctions retournent la plupart des statistiques descriptives désirables, comme la moyenne, la médiane, l’étendue, le minimum, le maximum, l’écart type et la taille. Comme la sortie est un jeu de données, celle-ci peut être manipulée pour retirer les statistiques qui ne sont pas désirées.Voici un exemple avec le jeu de données ToothGrowth inclus avec R.Il est possible de conserver ou retirer les colonnes non désirées en les spécifiant (voir Référer à des sous-éléments).Il est possible de retirer les colonnes non désirées en les spécifiant (voir Référer à des sous-éléments).La fonction describeBy() permet de faire ces analyses en fonction d’une variable de groupement. Par exemple, le groupement est supp.Enfin, il faut porter attention, car describe() et describeBy() transforment les variables ordinales10 et nominales en variables continues. Les indices statistiques rapportées pourraient n’avoir aucun intérêt. Il faudra alors retirer ces lignes (comme c’est fait avec les colonnes ci-haut).","code":"psych::describe(ToothGrowth)[,-c(1,5)]\n>        n  mean   sd trimmed  mad min  max range  skew\n> len   60 18.81 7.65   18.95 9.04 4.2 33.9  29.7 -0.14\n> supp* 60  1.50 0.50    1.50 0.74 1.0  2.0   1.0  0.00\n> dose  60  1.17 0.63    1.15 0.74 0.5  2.0   1.5  0.37\n>       kurtosis   se\n> len      -1.04 0.99\n> supp*    -2.03 0.07\n> dose     -1.55 0.08# Enregistrer les analyses descriptives\ndescriptif <- psych::describe(ToothGrowth)\n# Conserver mean et sd\ndescriptif[,c(\"mean\",\"sd\")]\n>        mean   sd\n> len   18.81 7.65\n> supp*  1.50 0.50\n> dose   1.17 0.63\n\n# Ou encore\n# Retirer n (1), \n# trimmed, mad, min, max, range (6:10),\n# et se (13)\ndescriptif[,-c(1, 6:10, 13)]\n>        n  mean   sd median  skew kurtosis\n> len   60 18.81 7.65   19.2 -0.14    -1.04\n> supp* 60  1.50 0.50    1.5  0.00    -2.03\n> dose  60  1.17 0.63    1.0  0.37    -1.55# Retirer n (1), \n# trimmed, mad, min, max, range (6:10),\n# et se (13)\ndescriptif[,-c(1, 6:10, 13)]\n>        n  mean   sd median  skew kurtosis\n> len   60 18.81 7.65   19.2 -0.14    -1.04\n> supp* 60  1.50 0.50    1.5  0.00    -2.03\n> dose  60  1.17 0.63    1.0  0.37    -1.55psych::describe.by(ToothGrowth, group = ToothGrowth$supp)\n> Warning: describe.by is deprecated.  Please use the\n> describeBy function\n> \n>  Descriptive statistics by group \n> group: OJ\n>      vars  n  mean   sd median trimmed  mad min  max range\n> len     1 30 20.66 6.61   22.7   21.04 5.49 8.2 30.9  22.7\n> supp    2 30  1.00 0.00    1.0    1.00 0.00 1.0  1.0   0.0\n> dose    3 30  1.17 0.63    1.0    1.15 0.74 0.5  2.0   1.5\n>       skew kurtosis   se\n> len  -0.52    -1.03 1.21\n> supp   NaN      NaN 0.00\n> dose  0.36    -1.60 0.12\n> --------------------------------------------- \n> group: VC\n>      vars  n  mean   sd median trimmed  mad min  max range\n> len     1 30 16.96 8.27   16.5   16.58 9.27 4.2 33.9  29.7\n> supp    2 30  2.00 0.00    2.0    2.00 0.00 2.0  2.0   0.0\n> dose    3 30  1.17 0.63    1.0    1.15 0.74 0.5  2.0   1.5\n>      skew kurtosis   se\n> len  0.28    -0.93 1.51\n> supp  NaN      NaN 0.00\n> dose 0.36    -1.60 0.12"},{"path":"décrire.html","id":"les-variables-à-échelles-nominales","chapter":" 9 Décrire","heading":"9.2 Les variables à échelles nominales","text":"Pour les variables nominales et ordinales, la fonction table() qui permet de compter la fréquence des éléments contenus dans une variable.La fonction count() de dplyr produit le même résultats, mais sa sortie est toutefois plus intéressante à Manipuler.Dans les deux cas, ajouter plus d’une variable génère une table de contingence (voir Le \\(\\chi^2\\) pour table de contingence pour plus d’informations), c’est-à-dire l’association entre les deux variables.","code":"table(ToothGrowth$supp)\n> \n> OJ VC \n> 30 30dplyr::count(ToothGrowth, supp)\n>   supp  n\n> 1   OJ 30\n> 2   VC 30"},{"path":"inférer.html","id":"inférer","chapter":" 10 Inférer","heading":" 10 Inférer","text":"Le principal de toute inférence statistique est de tirer des conclusions sur une population à partir d’un échantillon (un fragment beaucoup plus petit de la population). Comme il est rarement possible de collecter des données sur l’ensemble de la population, l’expérimentateur choisi, idéalement, un échantillon représentatif tiré aléatoirement. Une fois l’échantillon recruté et mesuré, l’expérimentateur dérive des indices statistiques. Un indice statistique synthétise par une estimation basée sur l’échantillon de l’information sur le paramètre de la population. Cet indice possède un comportement, une distribution d’échantillonnage qui détermine les différentes valeurs qu’il peut prendre. En obtenant ces indices, l’expérimentateur tente de connaître le paramètre de la population. S’il s’intéresse à la relation entre l’anxiété et un cours de méthodes quantitatives, l’expérimentateur voudra savoir si cette relation n’est pas nulle, mais aussi sa force, en termes de tailles d’effet.Cette tâche peut apparaître difficile considérant le peu d’informations sur la population, sa distribution de probabilité, les paramètres et la relative petite taille de l’échantillon par rapport à la population. Pour aider l’expérimentateur, les statisticiens ont le théorème central limite. Pour eux, il est certainement l’équivalent de la théorie de l’évolution pour le biologiste ou la théorie de la relativité générale pour le physicien. Ce théorème permet de connaître comment et sous quelles conditions se comportent les variables aléatoires.","code":""},{"path":"inférer.html","id":"le-théorème-central-limite","chapter":" 10 Inférer","heading":"10.1 Le théorème central limite","text":"Les valeurs d’un échantillon sont, pour le statisticien, des variables aléatoires. Une variable aléatoire, c’est un peu comme piger dans une boîte à l’aveuglette pour obtenir une valeur. La boîte est impénétrable, personne ne sait par quel processus elle accorde telle ou telle autre valeur. Pour le statisticien, ce qui importe c’est que chaque valeur possède une chance égale aux autres d’être sélectionnée et qu’elles soient indépendantes entre elles (le fait d’en choisir est sans conséquence sur la probabilité de choisir les autres).Pour le non-initié aux fonctions permettant de créer des nombres pseudoaléatoires, une fonction R comme rnorm() ou runif() (r suivi d’un nom de distribution, voir Les distributions) joue parfaitement le rôle de cette boîte. Si l’usager demande une valeur, la fonction retourne une valeur aléatoire (imprévisible à chaque fois) sans connaître comment cette valeur est produite.Le statisticien s’intéresse à inférer comment ces valeurs sont générées. Il postule ainsi que les valeurs aléatoires suivent une distribution de probabilité. Connaître cette distribution est très important, car c’est elle qui permet de répondre à des questions comme : quelle est la probabilité d’obtenir un résultat aussi rare que \\(x\\)? Ou quelle sont les valeurs attendues pour \\(95\\%\\) des tirages? Questions tout à fait pertinentes pour l’expérimentateur. Une des distributions les plus connues est certainement la distribution normale, celle qui est derrière la fonction rnorm() d’ailleurs. Mais, il y en beaucoup, beaucoup d’autres.Lorsque plus d’une variable sont issues d’une même boîte (distribution), elles sont identiquement distribuées. Si ces variables aléatoires sont combinées, que ce soit en termes de produit, de quotient, d’addition, de soustraction, le résultat est une nouvelle variable aléatoire qui possède sa propre distribution nommée distribution d’échantillonnage. Sur le plan de la syntaxe R, il s’agit de réaliser des opérations mathématiques avec des variables aléatoires identiquement distribuées.Entre en jeu le théorème central limite: plus des variables aléatoires identiquement distribuées sont additionnées ensemble, plus la distribution de probabilité de cette somme se rapproche d’une distribution normale.Par exemple, la fonction rlnorm() génère des variables issues d’une distribution log normale. Elle la forme illustré à la Figure 10.1 (qui n’rien de normal à première vue).\nFigure 10.1: Distribution log normale\nEn calculant la somme de plusieurs variables aléatoires de cette distribution, pour diverses valeurs de tailles d’échantillons (nombre de variables échantillonnées), les résultats tendent de plus en plus vers une distribution normale. Le code ci-dessous présente la démarche utilisée et la Figure 10.2 en fait la démonstration graphique en présentant les distributions d’échantillonnage obtenues.La Figure 10.2 montre que la distribution d’échantillonnage de la somme des variables converge vers une distribution normale à mesure que la taille d’échantillon \\(n\\) augmente. Cela est vrai pour n’importe quelle distribution de probabilité de la population. Le théorème central limite en dit plus que simplement la forme de la distribution. Il affirme également qu’une distribution de probabilité d’une population ayant une moyenne \\(\\mu\\) et un écart type \\(\\sigma\\) échantillonnées sur \\(n\\) unités, génère une distribution d’échantillonnage des totaux (indicé \\(t\\)) ayant une espérance (la moyenne) de \\(n\\mu_t\\) et un écart type de \\(n\\sigma_t\\).\nFigure 10.2: Distributions des totaux de n variables log normales\nLes expérimentateurs ne connaissent pas les distributions sous-jacentes aux valeurs des unités issues de la population. Par contre, à l’aide des statisticiens et du théorème central limite, ils savent comment se comportent les sommes des variables. Les expérimentateurs s’intéressent toutefois rarement aux sommes de variable… ou le sont-ils? En fait, les expérimentateurs s’intéressent particulièrement aux sommes de variables, comme la moyenne (une somme de variables divisée par la constante \\(n\\)), la variance (la somme des écarts au carré) ou la corrélation (la somme des produits de deux variables divisée par \\(n-1\\)). Dans le cas de la moyenne, le théorème central limite stipule qu’une distribution de probabilité ayant une moyenne \\(\\mu\\) et un écart type \\(\\sigma\\) dont l’échantillon est constitué de \\(n\\) unités, génère une distribution d’échantillonnage des moyennes avec une espérance de \\(\\mu_{\\bar{x}}\\) et un écart type de \\(\\sigma_{\\bar{x}}/\\sqrt{n}\\).Dans la mesure où l’expérimentateur connaît la distribution de la population (extrêmement rare, mais permet de mieux illustrer la théorie) ou qu’il recoure à une distribution d’échantillonnage connue, il peut inférer la probabilité d’une variable aléatoire par rapport à ce qui est attendu simplement par hasard. Il juge alors si cette variable est trop rare par rapport à l’hypothèse de base (l’hypothèse nulle).La théorie traditionnelle des tests d’hypothèses repose sur l’idée selon laquelle compare la vraisemblance d’une variable aléatoire estimée auprès d’un échantillon par rapport à une hypothèse nulle (l’absence d’effet). En épistémologie des sciences, il n’est pas possible de montrer l’exactitude d’une hypothèse, seulement son inexactitude. Cela rappelle le principe du falsificationnisme selon lequel ne peut prouver une hypothèse, ne peut que la falsifier. En statistiques, c’est la rareté d’une donnée qui agira comme indice d’inexactitude. Si la variable aléatoire est trop rare pour l’hypothèse nulle, celle-ci est rejetée : d’autres hypothèses doivent être considérées pour expliquer ce résultat. Autrement, l’hypothèse nulle n’est pas rejetée, les preuves sont insuffisantes pour informer l’expérimentateur sur l’hypothèse nulle.","code":"runif(n = 1)\n> [1] 0.114# Pour répliquer\nset.seed(1)\n\n# Création de deux variables identiquement distribuées\na <- runif(n = 1)\nb <- runif(n = 1)\na ; b\n> [1] 0.266\n> [1] 0.372\n\n# Une nouvelle variable aléatoire\ntotal <- a + b\n# Cette fonction produit les nombres, mais pas les graphiques.\n# Différentes tailles d'échantillons\nN <- seq(10, 90, by = 10)\n# Nombre de tirage pour chaque élément de N\nnreps <- 1000\n\n# Une boucle pour tester toutes les possibilités\nfor(n in N){\n  total <- as.numeric()\n  for(i in 1:nreps){\n    # Faire la somme de n valeurs tirés d'une distribution log normale\n    total[i] <- sum(rlnorm(n))\n  }\n  # hist(total) \n}"},{"path":"inférer.html","id":"inférence-avec-la-distribution-normale-sur-une-unité","chapter":" 10 Inférer","heading":"10.2 Inférence avec la distribution normale sur une unité","text":"Un excellent exemple en sciences humaines et sociales où la distribution de probabilité de la population est connue est le quotient intellectuel (QI). Le QI d’une population occidentale est distribué normalement (établi intentionnellement par les psychométriciens) avec une moyenne de 100 (\\(\\mu=100\\)) et un écart type de \\(\\sigma = 15\\). Ces valeurs sont totalement arbitraires, il est tout aussi convenable de parler d’une moyenne de 0 et d’un écart type de 1 (la distribution peut être standardisée) quoiqu’il est contre-intuitif de parler d’un QI de 0. (Qui voudrait avoir une intelligence de 0?)Dans la population, bien que la moyenne et la variance peuvent être connues, sélectionner une unité au hasard génère une variable aléatoire. Chaque individu de la population une probabilité très faible d’être sélectionné et est indépendant des autres individus de la population. Il est très difficile de prédire le score exact d’une personne. Toutefois, il est possible d’avoir un idée de la variabilité des scores. La Figure 10.3 montre la distribution normale par rapport à la moyenne, \\(\\mu\\) pour différentes valeurs d’écart type, \\(\\sigma\\). Elle montre que 68.269% des personnes devraient se retrouver entre plus ou moins un écart type ou encore que 95.45% devraient se retrouver entre plus ou moins deux écarts types. Ajusté au QI, il s’agit de 85 à 115 et de 70 à 130 respectivement.\nFigure 10.3: La distribution normale du QI\nUne autre façon de fonctionner est de prendre une personne au hasard et de mesurer son QI. Le score obtenu est une valeur aléatoire. Comme la distribution est connue avec ses paramètres, il est possible de juger de la vraisemblance de ce score (est-il rare?) par rapport à la population.Voici un exemple où ces informations sont pertinentes. Un groupe d’expérimentateurs mettent en place un outil d’évaluation qui teste si un individu donné est un humain ou un reptilien (une race d’extra-terrestre). Leur outil n’est pas si sophistiqué. En fait, il se base sur le QI, car les expérimentateurs ont remarqué que les reptiliens ont un QI beaucoup plus élevé que le QI humain.La distribution normale du QI humain joue le rôle d’hypothèse nulle, les personnes mesurées sont admises humaines jusqu’à preuve du contraire (une présomption d’innocence en quelque sorte), un QI trop élevé suggérant la culpabilité.Les expérimentateurs émettent l’hypothèse que les 5 % personnes ayant le plus haut QI sont vraisemblablement reptiliens. C’est le risque qu’ils sont prêts à prendre de sélectionner un humain et de le classer erronément comme reptilien.Le groupe d’expérimentateurs teste leur instrument sur Fanny. Elle un QI de 120. La Figure 10.4 illustre la distribution de l’intelligence dans la population et où se situe Fanny parmi celle-ci. Comment tester si elle est reptilienne?\nFigure 10.4: Score de Fanny sur la distribution normale\nLa première étape est d’obtenir un score-\\(z\\). Un score-\\(z\\) est une échelle standardisée des distances d’une valeur par rapport à la moyenne. Lorsqu’une échelle de mesure est transformée en score-\\(z\\), la moyenne est de 0 et l’écart type est égal à 1. Cela permet de mieux apprécier les distances et leur probabilité. Un score-\\(z\\) s’obtient en prenant la différence entre une unité (\\(x\\)) par rapport à la moyenne (\\(\\mu\\)) divisée par l’écart type (\\(\\sigma\\)). L’équation (10.1) illustre ce calcul.\\[\\begin{equation}\nz = \\frac{x-\\mu}{\\sigma}\n\\tag{10.1}\n\\end{equation}\\]Comme un score-\\(z\\) est standardisé, la Figure 10.3 est utilisable pour tirer des conclusions, car celle-ci est applicable pour toutes sortes de situations où la distribution est vraisemblablement normale.Fanny un score-\\(z\\) de 1.333. Maintenant, il faut traduire cette valeur en probabilité.L’expectative sous l’hypothèse nulle est d’observer un score pareil ou supérieur à celui de Fanny 9.121 % du temps. Cette statistique correspond à la valeur-\\(p\\), la probabilité de l’indice par rapport à sa distribution d’échantillonnage (hypothèse nulle). Comme elle ne dépasse pas le seuil de 5%, soit la limite selon laquelle le score est jugé invraisemblable, l’hypothèse nulle n’est pas rejetée (elle est humaine!).Avec le critère d’identifier erronément les 5 % humains les plus intelligents, il s’agit, du même coup, du taux de faux positif acceptable de l’étude. Un faible sacrifice à réaliser afin identifier des reptiliens parmi les humains. La zone de rejet, c’est-à-dire la zone dans laquelle l’hypothèse nulle (humain) est rejetée, correspond à la zone ombragée à droite de la distribution de la Figure 10.4.La logique des tests statistiques inférentiels repose sur cette série d’étapes : choisir un indice, connaître sa distribution sous-jacente, déterminer l’hypothèse nulle (généralement l’absence d’effet), calculer la probabilité de l’indice par rapport à cette hypothèse nulle.","code":"\nfanny <- 120\nz.fanny <- (fanny - 100) / 15# La probabilité qu'un score de QI soit entre -Inf à z.fanny\npnorm(z.fanny)\n> [1] 0.909\n\n# En pourcentage\npnorm(z.fanny) * 100\n> [1] 90.9"},{"path":"inférer.html","id":"inférence-avec-la-distribution-normale-sur-un-échantillon","chapter":" 10 Inférer","heading":"10.3 Inférence avec la distribution normale sur un échantillon","text":"Jusqu’à maintenant, seule une unité d’observation était traitée. L’indice et la distribution étaient également spécifiés. Dans cette section, l’exemple est étendu aux échantillons (plus d’une unité d’observation).Fanny un QI de 120. Si une autre personne est sélectionnée, cette nouvelle personne aura inévitablement un autre score. Cette logique s’applique également aux échantillons. L’exemple ci-dessous échantillonne 10 unités d’une population de QI distribuée normalement avec les paramètres usuels.\nFigure 10.5: Scores des unités de l’échantillon\nDans la Figure 10.5, chaque unité est présentée par un trait vertical noir. La moyenne de cet échantillon est de 108.8 et l’écart type est de 9.762. Dans cet exemple, l’indice est clairement identifié, mais quelle est la distribution d’échantillonnage des moyennes? Selon le théorème central limite, la moyenne de la distribution d’échantillonnage est \\(\\mu=\\mu_{\\bar{x}}=100\\) et l’écart type est \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}=\\frac{15}{\\sqrt{10}}=4.743\\) . Maintenant, il est possible de calculer un score-\\(z\\).\\[\nz = \\frac{\\bar{x}-\\mu_{\\bar{x}}}{\\sigma/\\sqrt{n}}=\\frac{108.8-100}{15/\\sqrt{10}}=1.855\n\\]\nCela donne le code suivant.La fonction (1 - pnorm(z)) * 100, retourne la probabilité (en pourcentage) d’un résultat plus rare que l’indice obtenu auprès de l’échantillon par rapport à la population. Comme pour l’exemple de Fanny, ce chiffre est une valeur-\\(p\\), soit la probabilité de l’indice observé par rapport à l’hypothèse nulle. La Figure 10.6 montre l’emplacement de l’échantillon sur la distribution d’échantillonnage. La probabilité de cet échantillon par rapport à l’hypothèse nulle est de 3.178 %, juste en deçà du 5 % fixé. Dans la Figure 10.6, la moyenne de l’échantillon se retrouve à l’intérieur de la zone de rejet (la zone grise). La conclusion est par conséquent de rejeter l’hypothèse nulle, l’échantillon semble provenir d’une autre distribution (avec des paramètres différents) que celle postulée.\nFigure 10.6: Moyenne de l’échantillon sur la distribution normale\nQu’en est-il vraiment de ce résultat? Pour l’expérimentateur, il ne peut aller plus loin, car il ne connaît pas la boîte noire selon laquelle les valeurs de l’échantillon sont générées. Il ne peut que constater que plusieurs (9/10) unités ont un score plus élevé que 100. Par contre, comme il s’agit d’un exemple simulé, la boîte noire est connue. C’est la fonction, round(rnorm(n = 10, mean = 100, sd = 15)), une distribution normale ayant \\(\\mu=100,\\sigma=15\\) qui est utilisée pour générer les valeurs. L’utilisateur sait qu’il s’agit d’un faux positif (une erreur de Type ) : l’échantillon fait partie des 5 % des échantillons qui risquent de se faire rejeter accidentellement. Si l’utilisateur utilise une autre graine (seed()), la plupart (95%) des moyennes ne sont pas rejetées.Pour l’instant, seule une boîte noire été examinée - celle de l’hypothèse nulle. Qu’advient-il du vrai phénomène? Par exemple, si les reptiliens existent vraiment. Comme l’utilisateur est le maître du modèle, il peut spécifier les paramètres à sa convenance. Le QI des reptiliens est conceptualisé pour être distribué comme une distribution normale ayant \\(\\mu_{r}=130, \\sigma_r = 15\\) où l’indice \\(r\\) ne fait qu’indiquer qu’il s’agit des paramètres de la population reptilienne. Les paramètres humains sont désignés par \\(h\\), soit \\(\\mu_h = 100,\\sigma_h = 15\\).La Figure 10.7 présente les distributions de ces populations. Trois zones sont ajoutées pour illustrer les concepts statistiques d’erreur de type , d’erreur de type II et de puissance statistique. Comme les populations sont connues, ces concepts statistiques sont calculables.\nFigure 10.7: Distribution du QI des humains et des reptiliens\nL’erreur de type (présentée auparavant) représente la probabilité d’émettre un faux positif, souvent représentée par \\(\\alpha\\) (alpha). Elle correspond à la probabilité de rejeter l’hypothèse lorsqu’elle est vraie. Dans la Figure 10.7, il s’agit de la zone noire. Elle correspond à conclure qu’un vrai humain est un reptilien (ce qu’il n’est pas). Ce taux est fixé à l’avance par l’expérimentateur, ici, 5%. C’est le risque qu’il est prêt à prendre. Ainsi, 95% des humains sont correctement identifiés comme humains.La zone hachurée de la Figure 10.7 correspond à l’erreur de type II, souvent représentée par \\(\\beta\\) (beta), soit la probabilité de ne pas rejeter l’hypothèse nulle. Autrement dit, c’est la probabilité de ne pas trouver l’effet lorsque celui-ci est vrai. Pour cet exemple, il s’agit d’un reptilien assez sournois (avec un QI suffisamment faible pour sa population) qu’il passe inaperçu auprès des humains, ou, en d’autres termes, de conclure qu’un vrai reptilien est un humain (ce qu’il n’est pas). Cette probabilité est estimée à 36.124%, donc 36.124% des reptiliens passeront inaperçus.La puissance, la zone grise de la Figure 10.7, correspond à la probabilité de rejeter correctement l’hypothèse nulle. Il s’agit de rejeter l’hypothèse nulle lorsqu’elle est fausse. Cela signifie d’identifier un vrai reptilien correctement. C’est exactement ce que le groupe de chercheur désire réaliser. Elle est estimée à 63.876% en ne prenant qu’une mesure de QI. La puissance est l’une des statistiques qui intéressent le plus l’expérimentateur avant de réaliser son étude, car elle donne une approximation sur la probabilité de trouver un résultat significatif. Par contre, les expérimentateurs ne connaissent que rarement les paramètres de la vraie distribution de l’effet qu’il désire trouver. En langage statistique, il s’agit un paramètre de non-centralité (ou de décentralisation) ou ncp en syntaxe R. L’expérimentateur recourt alors à d’habiles approximations éclairées basées sur leurs connaissances du phénomène et en regard des études déjà publiées sur le sujet (ou un sujet similaire), s’il y en . Ces estimations souffrent tout de même d’être des variables aléatoires (elles respectent elles aussi une distribution d’échantillonnage), et non les paramètres recherchés.Avant de procéder davantage, une dernière notion est importante à présenter : la direction du test. Dans tous les exemples précédents, l’hypothèse nulle était rejetée si une valeur plus rare était obtenue. Il s’agit d’un test d’hypothèse unilatérale (d’un seul côté). Cela facilite grandement la présentation de certains concepts et calculs. Un test peut être unilatérale inférieure, trouver un résultat plus faible qu’une valeur critique; ou unilatérale supérieure, trouver un résultat plus élevé qu’une valeur critique; ou encore bilatérale, soit trouver un résultat moins élevée ou plus élevée que des valeurs critiques.Dans le cas d’un test bilatéral, l’erreur de type est divisée par 2, \\(\\alpha/2\\) pour couvrir l’espace des deux côtés de la distribution. L’espace total est identique même s’il est divisé sur les deux extrêmes. Cela implique des valeurs critiques plus élevées que si le test n’était que d’un côté et par conséquent une puissance statistique un peu plus faible, car l’effet est directionnel. La probabilité de rejeter l’hypothèse dans une direction été diminuée pour tenir compte de l’autre côté. La Figure 10.8 montre un exemple de chacun de ces types de tests pour \\(\\alpha = .05\\).\nFigure 10.8: Illustration de l’erreur de type \nLe choix entre unilatérale inférieure, supérieure ou bilatérale repose essentiellement sur la question de recherche de l’expérimentateur. Qu’attend-il ou que veut-il savoir du résultat? L’expérimentateur recourt généralement au test bilatéral afin d’identifier des résultats allant pour son hypothèse ou à l’opposé de son hypothèse. Cela permet notamment d’identifier des devis qui peuvent nuire aux participants. Par exemple, si un expérimentateur développe une intervention pour réduire la consommation de drogues chez les adolescents et n’est concerné que par la probabilité que l’intervention soit efficace, il peut manquer l’effet délétère d’une telle intervention, soit que l’intervention augmente la consommation de drogue. Il est certain que l’expérimentateur souhaite être rapidement mis au courant si ces résultats se dessinent.","code":"# Création d'un échantillon de 10 unités\nset.seed(824)\n\n# Dix valeurs arrondies à partir d'une moyenne de 100 et un écart type de 10\nQI <- round(rnorm(n = 10, mean = 100, sd = 15))\nQI\n>  [1] 110 111 102  99 109 102  99 110 132 114z <- (mean(QI) - 100)/(15 / sqrt(10))\nz\n> [1] 1.86\nmu.h <- 100 ; sd.h <- 15\nmu.r <- 130 ; sd.r <- 15# Erreur de type I (fixé à l'avance)\nalpha <- .05 \n\n# Valeur critique à laquelle l'expérimentateur rejette H0\nv.crit <- qnorm(1 - alpha, mean = mu.h, sd = sd.h)\nv.crit\n> [1] 125\n\n# Erreur de type II\nbeta <- pnorm(v.crit, mean = mu.r, sd = sd.r)\nbeta\n> [1] 0.361\n\n# Puissance statistique\npuissance <- 1 - beta\npuissance \n> [1] 0.639"},{"path":"inférer.html","id":"la-distribution-t","chapter":" 10 Inférer","heading":"10.4 La distribution-\\(t\\)","text":"Dans la plupart des cas, l’expérimentateur ne connaît pas la variance de la population. Il recourt alors à la meilleure estimation qui lui est disponible, celle obtenue auprès de l’échantillon. De recourir à une estimation au lieu de la vraie valeur cause un réel problème: l’estimation est une variable aléatoire respectant une distribution d’échantillonnage. La distribution des variances montre une légère asymétrie positive plus flagrante pour les petites tailles d’échantillon, car les variances sont distribuées en distribution-\\(\\chi^2\\).\nFigure 10.9: Distribution de la variance en fonction de la taille d’échantillon\nUne petite simulation faite à partir d’une distribution normale standardisée par rapport à trois tailles d’échantillon et dont la variance est estimée 10^{4} fois est présentée dans la Figure 10.9. Elle montre que l’estimation n’est pas biaisée (elles sont toutes les trois centrées à 1), mais relève l’asymétrie en question pour les petites tailles d’échantillon qui tend à décroître à mesure que \\(n\\) augmente. Autrement dit, une estimation unique (pris d’un échantillon) est plus susceptible d’être sous-estimée. Ne pas tenir compte de cet aspect augmente indûment les valeurs obtenues (à cause du dénominateur plus petit). Ainsi, il est inadéquat d’utiliser une distribution normale lorsque la variance est inconnue. Il faut plutôt opter pour la distribution-\\(t\\) qui, elle, tient compte de la variabilité de l’estimation de la variance.La distribution-\\(t\\) est symétrique, comme la distribution normale, mais des queues plus larges pour tenir compte de la surestimation des valeurs dû à la sous-estimation de la variance. La distribution-\\(t\\) tend vers une distribution normale lorsque \\(n\\) augmente, ce qui est illustré à la Figure 10.10. La ligne pleine noire montre la distribution pour 5 unités, les deux autres lignes, traits et pointillés, pour 30 et 1000 unités respectivement. Il est difficile de distinguer ces deux dernières.\nFigure 10.10: Comparaison d’une distribution normale à deux distributions-\\(t\\)\nEn ayant recours à des échantillons et une estimation de la variance (la variance est inconnue de l’expérimentateur, ce qui est généralement le cas), il faut procéder avec la distribution-\\(t\\). La procédure est la même que celle utilisée avec les scores-\\(z\\) précédemment, la différence étant que la distribution-\\(t\\) est utilisée au lieu de la distribution normale, car l’écart type est estimé.","code":""},{"path":"inférer.html","id":"le-test-t-à-échantillon-unique","chapter":" 10 Inférer","heading":"10.5 Le test-\\(t\\) à échantillon unique","text":"Le test-\\(t\\) à échantillon unique permet de tester la moyenne d’échantillon par rapport à une valeur arbitraire \\(\\mu_0\\) (généralement \\(\\mu_0 = 0\\)) qui joue le rôle d’hypothèse nulle.\\[ t_{n-1} = \\frac{\\bar{x}-\\mu_0}{(s/\\sqrt{n})} \\]La distribution-\\(t\\) un degré de liberté (l’indice de \\(t\\) dans l’équation) qui lui est associé et qui est fixé à \\(dl = n - 1\\), la taille d’échantillon moins 1. Un degré est perdu à cause de l’estimation de l’écart type de l’échantillon. Qu’en est-il de la probabilité d’obtenir cette moyenne? En recourant à la distribution intégrée de R, pt() il est possible d’obtenir la probabilité d’une valeur-\\(t\\) par rapport à l’hypothèse nulle. La fonction nécessite une valeur-\\(t\\) et le degré de liberté associé, p. ex., pt(t = , df = n - 1). La valeur produite donne la probabilité d’obtenir un score plus petit jusqu’à la valeur-\\(t\\). Une astuce permet de calculer aisément la probabilité lorsque la distribution d’échantillonnage est symétrique. Utiliser une valeur absolue permet de considérer les deux côtés de la distribution simultanément. Les valeurs-\\(t\\) négatives sont alors positives. Comme un côté est supprimé, l’espace positif est doublé, le code pour tenir compte de cet astuce est : (1 - pt(abs(t), df = n - 1)) * 2 où t est la valeur-\\(t\\) obtenue.Une fois la fonction créée, il est possible de la tester en la comparant avec la fonction R de base test.t(). Le code suivant crée un échantillon de 30 unités avec une moyenne de 1 et un écart type de 1.\nFigure 10.11: Valeur-\\(t\\) de l’échantillon sur la distribution-\\(t\\)\nQu’est-ce que ces résultats signifient? La Figure 10.11 montre la distribution d’échantillonnage de l’hypothèse nulle, c’est-à-dire l’hypothèse selon laquelle la moyenne de la population est égale à 0. Les zones grises montrent les zones de rejet. Si une valeur-\\(t\\) se retrouve dans ces zones, elle est jugée comme trop rare, il faut alors rejeter l’hypothèse nulle. C’est ce qui s’est produit ici. Rien de surprenant, l’échantillon est tiré d’une distribution normale avec une moyenne de 1 et un écart type de 1.","code":"\ntestt <- function(x, mu = 0){\n  # x est une variable continue\n  # mu est une moyenne à tester comme hypothèse nulle(H0)\n  xbar <- mean(x)\n  sdx <- sd(x)\n  n <- length(x)\n  vt <- (xbar - mu) / (sdx / sqrt(n))\n  dl <- n - 1\n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  statistique <- list(valeur.t = vt, \n                      dl = dl, \n                      valeur.p = vp)\n  return(statistique)\n}# Un exemple de jeu de données\nset.seed(20)\nx = rnorm(n = 30, mean = 1, sd = 1)\n# Vérification de la fonction maison\ntestt(x)\n> $valeur.t\n> [1] 3.69\n> \n> $dl\n> [1] 29\n> \n> $valeur.p\n> [1] 0.000919\n# Comparer avec la fonction R\nt.test(x)\n> \n>   One Sample t-test\n> \n> data:  x\n> t = 4, df = 29, p-value = 9e-04\n> alternative hypothesis: true mean is not equal to 0\n> 95 percent confidence interval:\n>  0.31 1.08\n> sample estimates:\n> mean of x \n>     0.696"},{"path":"inférer.html","id":"critiques-des-tests-dhypothèses","chapter":" 10 Inférer","heading":"10.6 Critiques des tests d’hypothèses","text":"TODOLe présent ouvrage ne couvre que l’aspect traditionnel ou classique des tests d’hypothèse. Cette approche est remise en question depuis 1950 jusqu’à aujourd’hui. D’excellents ouvrages couvrent les failles et solutions des tests d’hypothèses classiques de façon plus approfondie qu’il ne l’est fait ici.L’hypothèse nulle n’est jamais susceptible d’être vraie.La conclusion statistique n’est pas celle désirée par l’expérimentateur.La significativité statistique n’implique pas la significativité pratique.Les expérimentateurs interprètent incorrectement la valeur-\\(p\\).Une valeur-\\(p\\) faible n’est pas garantie de la reproductibilité.La dichotomisation de la preuve \\(p<=\\alpha\\).Une saine utilisation de la valeur-\\(p\\) n’accomplit pas grand-chose.L’obsession de la valeur-\\(p\\) cause plus de problèmes qu’elle n’en résout : \\(p\\)-hacking, triturage de données, abus de variables, comparaison multiple, biais pour les hypothèses originales et surprenantes.","code":""},{"path":"analyser.html","id":"analyser","chapter":" 11 Analyser","heading":" 11 Analyser","text":"En continuation de l’introduction des théories des tests d’hypothèses (voir Inférer) et l’aperçu donnée par le test-\\(t\\) à échantillon unique, cette section poursuit la présentation en introduisant des analyses statistiques de bases comme les différences de moyennes, l’association linéaire et les tests pour données nominales. Les tests-\\(t\\) indépendant et dépendant, la covariance, la corrélation ainsi que le test du \\(\\chi2\\) pour table de contingence sont présentées.","code":""},{"path":"analyser.html","id":"les-différences-de-moyennes","chapter":" 11 Analyser","heading":"11.1 Les différences de moyennes","text":"","code":""},{"path":"analyser.html","id":"le-test-t-indépendant","chapter":" 11 Analyser","heading":"11.1.1 Le test-\\(t\\) indépendant","text":"En général, l’expérimentateur ne s’intéresse pas à comparer une moyenne à une valeur arbitraire, comme c’était le cas avec le test-\\(t\\) à échantillon unique. Cela peut lui être assez trivial. Il s’intéresse plutôt à comparer une moyenne à une autre moyenne, soit une différence entre deux groupes indépendants, par exemple, quelle est la différence entre un groupe traitement et un groupe contrôle?En se basant sur le test \\(t\\) à échantillon unique, la valeur-\\(t\\) pour deux moyennes se calcule selon l’équation (11.1)\\[\\begin{equation}\nt_{n-2} = \\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{\\frac{s^2_{1}}{n_1}+\\frac{s^2_{2}}{n_2}}}\n\\tag{11.1}\n\\end{equation}\\]où l’indice de la valeur-\\(t\\) est le nombre de degrés de liberté \\(n-2\\). Comme deux variances sont estimées, deux degrés de libertés sont imputés, ce qui octroi \\(n-2\\) degrés. En plus de considérer \\(\\bar{x_2}\\) la valeur arbitaire de comparaison (l’ordre de \\(\\bar{x_2}\\) et \\(\\bar{x^1}\\) est arbitraire), les deux écarts types sont également considérés au dénominateur.Une fois le calcul réalisé, la logique du test d’hypothèse est la même, à l’exception de l’hypothèse nulle qui correspond maintenant à l’absence de différence entre les deux moyennes.Voici un exemple de programmation du test-\\(t\\) pour deux groupes indépendants.Pour générer un exemple de données, le code ci-après crée un échantillon de 15 unités réparties en deux groupes, le premier groupe (gr0) est tiré d’une distribution normale ayant une moyenne de 1 et un écart type de 1, le deuxième groupe (gr1), une moyenne de 0 et un écart type de 1. La syntaxe illustre la création de deux variables pour créer les deux groupes. Il est aussi possible d’envisager la création sur en termes d’équation linéaire, comme l’équation (11.2),\\[\\begin{equation}\ny = \\mu_0 + \\mu_1x_1 + \\epsilon\n\\tag{11.2}\n\\end{equation}\\]où \\(y\\) est le score observé des unités et les autres variables construisent ce score, \\(\\mu_0\\) correspond à la moyenne du groupe référent de population (le groupe contrôle en quelque sorte.), \\(\\mu_1\\) réfère à la différence de moyenne entre les deux groupes identifiés par \\(x_1\\) qui réfère à l’assignation au groupe, soit 0 pour le groupe contrôle et 1 pour le groupe différent. Pour ce même exemple, \\(\\mu_1 = -1\\). Par le produit \\(\\mu_1x_1\\), le groupe contrôle associé à la valeur 0 n’pas de modification de la moyenne, \\(-1*0=0\\) alors le groupe différent associé à la valeur 1, \\(-1*1=-1\\). Enfin, \\(\\epsilon\\) correspond à la variabilité entre les unités. Cette façon de programmer la création des variables illustre bien l’association linéaire qui existe même dans les différences de moyennes et sera très utile pour des modèles plus compliqués.Les données sont identiques.Une fois la fonction créée, il est possible de la tester et de la comparer avec la fonction R de base test.t().Les équations et codes précédents ne sont adéquats que si les variances sont égales entre les deux groupes. Cette notion est quelque peu trahie par la spécification dans la fonction R de l’argument var.equal = TRUE qui par défaut est FALSE. Pour un test-\\(t\\) indépendant suivant les règles de l’art, il faut appliquer une correction (approximation de Welsh) aux degrés de liberté. Les degrés de liberté deviennent moins élégants et respectent l’équation (11.3).\\[\\begin{equation}\ndl=\\frac{\\left(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}\\right)^2}{\\frac{\\left(\\frac{s^2_1}{n_1}\\right)^2}{n_1-1}+\\frac{\\left(\\frac{s^2_2}{n_2}\\right)^2}{n_2-1}}\n\\tag{11.3}\n\\end{equation}\\]En apportant cette correction au code initial.Le voici comparé à la fonction R de base.Les sorties sont identiques.\nFigure 11.1: Valeur-\\(t\\) de la différence de moyenne sur la distribution-\\(t\\)\nLa Figure 11.1 illustre où se situe la moyenne de l’échantillon par rapport à la distribution d’échantillonnage de l’hypothèse nulle. Comme la valeur se retrouve dans la zone de rejet ou, de façon équivalente, la valeur-\\(p\\) est plus petite que la valeur \\(\\alpha\\) fixée à .05, rejette l’hypothèse nulle, il y vraisemblablement une différence entre les groupes. C’est bien l’intention derrière la création des données.","code":"\ntestt.ind <- function(x1, x2){\n  # x1 est une variable continue associée au groupe 1\n  # x2 est une variable continue associée au groupe 2\n  \n  # Calcul des moyennes\n  x1bar <- mean(x1) ; x2bar <- mean(x2)\n  \n  # Calcul des variances\n  x1var <- var(x1) ; x2var <- var(x2)\n  \n  # Calcul des tailles d'échantillon\n  nx1 <- length(x1) ; nx2 <- length(x2)\n  \n  # Valeur-t, degrés de liberté et valeur-p\n  vt <- (x1bar - x2bar) / sqrt(x1var / nx1 + x2var / nx2)\n  dl <- nx1 + nx2 - 2\n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  statistique <- list(valeur.t = vt, dl = dl, valeur.p = vp)\n  return(statistique)\n}# Un exemple de jeu de données programmé de deux façons\n# Méthode 1\nset.seed(2021)\ngr0 <- rnorm(n = 15, mean = 1, sd = 1)\ngr1 <- rnorm(n = 15, mean = 0, sd = 1)\n\n# Méthode 2\nset.seed(2021)\n\nx1 <- c(rep(0, 15), rep(1, 15)) # Appartenance au groupe (0 et 1)\ne  <- rnorm(n = 30, mean = 0, sd = 1) # Erreur interindividuel\nmu0 <- 1  # La moyenne du groupe référent\nmu1 <- -1 # Le deuxième groupe a une différence de moyenne de -1\n\ny <- mu0 + mu1 * x1 + e\n\n# Présenter quelques participants; retirer les crochets pour voir tous\n(cbind(method1 = c(gr0, gr1), method2 = y))[10:20,]\n>       method1 method2\n>  [1,]  2.7300  2.7300\n>  [2,] -0.0822 -0.0822\n>  [3,]  0.7272  0.7272\n>  [4,]  1.1820  1.1820\n>  [5,]  2.5085  2.5085\n>  [6,]  2.6045  2.6045\n>  [7,] -1.8415 -1.8415\n>  [8,]  1.6233  1.6233\n>  [9,]  0.1314  0.1314\n> [10,]  1.4811  1.4811\n> [11,]  1.5133  1.5133# Vérification de la fonction maison\ntestt.ind(gr0, gr1)\n> $valeur.t\n> [1] 3.4\n> \n> $dl\n> [1] 28\n> \n> $valeur.p\n> [1] 0.00205\n\n# Comparer avec la fonction R\nt.test(gr0, gr1, var.equal = TRUE)\n> \n>   Two Sample t-test\n> \n> data:  gr0 and gr1\n> t = 3, df = 28, p-value = 0.002\n> alternative hypothesis: true difference in means is not equal to 0\n> 95 percent confidence interval:\n>  0.54 2.18\n> sample estimates:\n> mean of x mean of y \n>     1.332    -0.029\ntestt.ind2 <-function(x1, x2){\n  # x1 est une variable continue associée au groupe 1\n  # x2 est une variable continue associée au groupe 2\n  x1bar <- mean(x1) ; x2bar <- mean(x2)  # Les moyennes\n  x1var <- var(x1) ; x2var <- var(x2)    # Les variances\n  nx1 <- length(x1) ; nx2 <- length(x2)  # Les tailles\n  \n  # L'erreur type \n  et <- sqrt(x1var / nx1 + x2var / nx2)\n  \n  # Valeur-t\n  vt <- (x1bar - x2bar) / et\n  \n  # Correction des degrés de liberté\n  dl <- et^4 / \n    ((x1var^2 / (nx1^2 * (nx1 - 1))) + \n       (x2var^2 / (nx2^2 * (nx2 - 1))))\n  \n  # Valeur-p\n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  \n  statistique <- list(valeur.t = vt, dl = dl, valeur.p = vp)\n  return(statistique)\n}t.test(gr0, gr1) # Absence de l'argument var.equal = TRUE\n> \n>   Welch Two Sample t-test\n> \n> data:  gr0 and gr1\n> t = 3, df = 27, p-value = 0.002\n> alternative hypothesis: true difference in means is not equal to 0\n> 95 percent confidence interval:\n>  0.539 2.182\n> sample estimates:\n> mean of x mean of y \n>     1.332    -0.029\ntestt.ind2(gr0, gr1)\n> $valeur.t\n> [1] 3.4\n> \n> $dl\n> [1] 26.9\n> \n> $valeur.p\n> [1] 0.00213"},{"path":"analyser.html","id":"rapporter-un-test-t-indépendant","chapter":" 11 Analyser","heading":"11.1.2 Rapporter un test-\\(t\\) indépendant","text":"Voici comment il est possible de rapporter les résultats. La syntaxe pour commander l’analyse est la suivante si les groupes sont dans deux vecteurs séparés.Il est également possible d’utiliser la formule si, dans le jeu de données, une variable permet de distinguer les deux groupes et que la variable continue (variable à comparer) se trouve dans une seule variable.La présente étude compare deux groupes sur une échelle de mesure psychologique. Le test-\\(t\\) indépendant suggère que la différence entre les groupes est significative, \\(t(26.912) =3.398, p = 0.002\\).","code":"t.test(gr0, gr1) # Absence de l'argument var.equal = TRUE\n> \n>   Welch Two Sample t-test\n> \n> data:  gr0 and gr1\n> t = 3, df = 27, p-value = 0.002\n> alternative hypothesis: true difference in means is not equal to 0\n> 95 percent confidence interval:\n>  0.539 2.182\n> sample estimates:\n> mean of x mean of y \n>     1.332    -0.029# Mettre les variables en jeu de données\njd.ti <- data.frame(VD = c(gr0, gr1),\n                    VI = x1)\nt.test(VD ~ VI, data = jd.ti)\n> \n>   Welch Two Sample t-test\n> \n> data:  VD by VI\n> t = 3, df = 27, p-value = 0.002\n> alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n> 95 percent confidence interval:\n>  0.539 2.182\n> sample estimates:\n> mean in group 0 mean in group 1 \n>           1.332          -0.029"},{"path":"analyser.html","id":"le-test-t-dépendant","chapter":" 11 Analyser","heading":"11.1.3 Le test-\\(t\\) dépendant","text":"Un autre test-\\(t\\) est celui permettant de comparer deux de temps de mesure sur les mêmes participants. L’hypothèse est de vérifier si \\(\\mu_1 = \\mu_2\\), soit la moyenne du temps 1 est égale à la moyenne du temps 2. Une habile manipulation mathématique permet de poser cette hypothèse en hypothèse nulle, \\(\\mu_1-\\mu_2=0\\). Il est intéressant de noter que la différence entre deux variables est normalement distribuée si la variance est connue ou distribuée en \\(t\\) si la variance est inconnue.Ce test est utile lorsqu’il faut tester si les participants se sont améliorés ou détériorés entre deux temps de mesure. Pour calculer la valeur-\\(t\\),\\[ t_{dl_1} = \\frac{\\bar{x_1}-\\bar{x_2}}{\\sigma_d/\\sqrt{n}} \\]avec \\(n-1\\) degrés de liberté à cause de l’estimation de l’écart type des différences. Les étapes subséquentes sont identiques au test-\\(t\\) à groupe unique.Pour créer le jeu de données, les étapes sont similaires au test-\\(t\\) indépendant pour des temps de mesure indépendants (sans corrélation) où seules les populations des temps de mesure sont définis. Il est possible de spécifier une corrélation entre les deux de mesures. Cela sera introduit dans la section La corrélation.Dans le jeu de données suivant, 25 personnes sont mesurées à deux temps de reprises. Il n’y pas de corrélation entre les temps de mesure. La variance des temps de mesure est de 1. La différence de moyenne est de 2. La syntaxe simule la situation suivante, le temps1 correspond à la mesure initiale et difference correspond à la différence entre les temps de mesure. La somme de ces deux scores donnent la mesure au temps2. Cela montre assez simplement que la différence entre temps2 - temps1 permet de retrouver le vecteur de difference.La fonction base de R est encore t.test(), mais il faudra spécifier l’argument paired = TRUE pour commander un test apparié (une autre appellation pour un test-\\(t\\) dépendant).Les sorties sont identiques.\nFigure 11.2: Valeur-\\(t\\) de moyenne de différences sur la distribution-\\(t\\)\nLa Figure 11.2 montre où se situe la différence de moyenne par rapport à la distribution d’échantillonnage de l’hypothèse nulle. Comme la valeur se retrouve dans la valeur-\\(p\\) est plus petite que la valeur \\(\\alpha\\) fixée à .05, rejette l’hypothèse nulle; il y vraisemblablement une différence entre les temps de mesure, ce qui était l’intention derrière la création des données.","code":"\ntestt.dep <- function(temps1, temps2){\n  # Temps est une variable continue mesurée \n  # à deux occasions auprès des mêmes participants\n  \n  # Calculer la différence\n  difference <- temps1 - temps2\n  dbar <- mean(difference) # La moyenne des différences\n  dvar <- var(difference)  # L'écart type des différences\n  n <- length(difference)  # Taille d'échantillon\n  \n  # Valeur-t, degrés de liberté et valeur-p\n  vt <- (dbar) / sqrt(dvar / n)\n  dl <- n - 1                   \n  vp <- (1 - pt(abs(vt), df = dl)) * 2 \n  statistique <- list(valeur.t = vt, dl = dl, valeur.p = vp)\n  return(statistique)\n}\n# Un exemple de jeu de données\nset.seed(148)\ntemps1 <- rnorm(n = 25, mean = 0, sd = 2)\ndifference <- rnorm(n = 25, mean = 2, sd = 2)\ntemps2 <- temps1 + differencet.test(temps1, temps2, paired = TRUE)\n> \n>   Paired t-test\n> \n> data:  temps1 and temps2\n> t = -3, df = 24, p-value = 0.004\n> alternative hypothesis: true mean difference is not equal to 0\n> 95 percent confidence interval:\n>  -2.172 -0.465\n> sample estimates:\n> mean difference \n>           -1.32\ntestt.dep(temps1, temps2)\n> $valeur.t\n> [1] -3.19\n> \n> $dl\n> [1] 24\n> \n> $valeur.p\n> [1] 0.00395"},{"path":"analyser.html","id":"autres-syntaxes-possibles","chapter":" 11 Analyser","heading":"11.1.3.1 Autres syntaxes possibles","text":"Il y plusieurs façons de réaliser un test-\\(t\\) dépendant. La première option est celle décrite précédemment.La second option utilise la formule et le jeu de données. La fonction Pair() n’que seule objectif d’informer la fonction t.test() que les vecteurs qu’elle inclut sont pairés, ce qui permet d’utiliser la formule Pair(temps1, temps2) ~ 1.Pour la troisième option, comme l’équation sur le test-\\(t\\) dépendant le suggère, un test-\\(t\\) dépendant revient à faire un test-\\(t\\) à échantillon unique avec la différence entre les deux temps de mesure.Une quatrième option, si le temps est une colonne dans le jeu de données, alors le même style de formule peut être utilisé comme Le test-\\(t\\) indépendant toujours en indiquant paired = TRUE.","code":"# Technique précédent avec deux vecteurs\nt.test(x = temps1, y = temps2, paired = TRUE)\n> \n>   Paired t-test\n> \n> data:  temps1 and temps2\n> t = -3, df = 24, p-value = 0.004\n> alternative hypothesis: true mean difference is not equal to 0\n> 95 percent confidence interval:\n>  -2.172 -0.465\n> sample estimates:\n> mean difference \n>           -1.32# Avec la formule et les variables dans le même jeu de données\njd.td <- data.frame(temps1 = temps1,\n                    temps2 = temps2)\nt.test(Pair(temps1, temps2) ~ 1, data = jd.td)\n> \n>   Paired t-test\n> \n> data:  Pair(temps1, temps2)\n> t = -3, df = 24, p-value = 0.004\n> alternative hypothesis: true mean difference is not equal to 0\n> 95 percent confidence interval:\n>  -2.172 -0.465\n> sample estimates:\n> mean difference \n>           -1.32# Avec la différence des deux vecteurs\nt.test(temps2 - temps1)\n> \n>   One Sample t-test\n> \n> data:  temps2 - temps1\n> t = 3, df = 24, p-value = 0.004\n> alternative hypothesis: true mean is not equal to 0\n> 95 percent confidence interval:\n>  0.465 2.172\n> sample estimates:\n> mean of x \n>      1.32# Avec la formule et les variables dans le même jeu de données\n# Le temps est une seule variable\njd.td2 <- data.frame(mesure = c(temps1, temps2),\n               temps = rep(1:2, each = 25))\nt.test(mesure ~ temps, data = jd.td2, paired = TRUE)\n> \n>   Paired t-test\n> \n> data:  mesure by temps\n> t = -3, df = 24, p-value = 0.004\n> alternative hypothesis: true mean difference is not equal to 0\n> 95 percent confidence interval:\n>  -2.172 -0.465\n> sample estimates:\n> mean difference \n>           -1.32"},{"path":"analyser.html","id":"rapporter-un-test-t-dépendant","chapter":" 11 Analyser","heading":"11.1.4 Rapporter un test-\\(t\\) dépendant","text":"Voici comment rapporter dans un article.Un groupe de participants est mesuré à deux reprises sur une même mesure psychologique. Le test-\\(t\\) dépendant suggère que la différence entre les temps de mesure est significative, \\(t(24) =-3.188, p = 0.002\\).","code":""},{"path":"analyser.html","id":"lassociation-linéaire","chapter":" 11 Analyser","heading":"11.2 L’association linéaire","text":"","code":""},{"path":"analyser.html","id":"la-covariance","chapter":" 11 Analyser","heading":"11.2.1 La covariance","text":"La covariance représente une mesure du degré auquel une variable augmente lorsque l’autre augmente. Elle correspond au produit moyen entre deux variables centrées (dont les moyennes sont soustraites). retrouve la covariance sous la forme de l’équation (11.4).\\[\\begin{equation}\n\\text{cov}_{xy} = \\sigma_{xy}=\\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\tag{11.4}\n\\end{equation}\\]La covariance est une extension multivariée de la variance. La variance est le produit d’une variable centrée avec elle-même (le carré de la variable), \\[\\sigma^2=\\frac{1}{n-1}\\sum_{=1}^nx_ix_i\\].En transformant \\(x\\) et \\(y\\) pour qu’elles soient centrées, l’aspect de produit entre les deux variables devient évident dans l’expression de la covariance.\n\\[\\text{cov}_{xy} = \\frac{1}{n-1}\\sum_{=1}^nxy\\]\nLa programmation de la covariance peut se traduire ainsi.La fonction de base cov() est plus efficace (plus robuste et plus simple) que la fonction maison précédente. La fonction cov() peut tenir compte de plus de deux variables.\nLa covariance indique la direction de la covariation entre les deux variables, positives ou négatives, mais n’indique pas la force de la relation. Bien qu’une covariance de 0 indique l’absence de colinéarité, une covariance de 120 n’est pas nécessairement plus forte qu’une covariance de 1. La mesure est intrinsèquement influencée par les unités l’échelle de mesure. Il serait aberrant de considérer le système métrique comme moins efficace, car ces mesures sont moins variables que les mesures impériales. (Il y plusieurs raisons de préférer le système métrique, mais ce n’en est pas une!)","code":"\ncovariance <- function(x, y){\n  # X est une data.frame ou matrice de n sujets par p variables\n  n <- length(x)\n  # centrer variables\n  xc <- x - mean(x)\n  yc <- y - mean(y)\n  prod.xy <- xc * yc\n  cov.xy <- sum(prod.xy) / (n-1)\n  return(cov.xy)\n}"},{"path":"analyser.html","id":"la-corrélation","chapter":" 11 Analyser","heading":"11.2.2 La corrélation","text":"La corrélation représente le degré d’association linéaire entre deux variables. Une façon simple de concevoir la corrélation est comme une covariance standardisée. Elle varie entre -1 et 1, ces deux dernières valeurs impliquant, respectivement, une relation parfaitement négative et positive entre les deux variables. En plus d’indiquer la direction de la relation, la corrélation suggère une force à ce lien. Lorsque la valeur de la corrélation est nulle, \\(r=0\\), un diagramme de dispersion ne montre qu’un nuage de point épars (sans structure). Plus la corrélation augmente (en terme absolu), plus le nuage tends vers une ligne droite. La Figure 11.3 montre différentes valeurs de corrélation et un exemple de nuage de points qui lui est associé.\nFigure 11.3: Illustrations de diagrammes de dispersion associé en fonction de corrélations\nEn statistiques, la corrélation est souvent représentée par la lettre grecque \\(\\rho\\) (rho) ou l’usuel \\(r\\) lorsqu’il s’agit d’une estimation à partir d’un échantillon.Il y plusieurs méthodes pour calculer la corrélation. Une première est de considérer la corrélation comme le produit de variables standardisées (autrement dit, score-\\(z\\)). Pour la covariance, la moyenne était soustraite de la variable, c’est-à-dire des variables centrées. Pour la corrélation, divise pas l’écart type également pour obtenir des variables standardisées.En admettant que \\(x\\) et \\(y\\) sont déjà standardisée, \\(z_x,z_y\\), comme c’était le cas pour la covariance, l’aspect de produit entre les deux variables est conservé.\n\\[\\text{cor}_{xy} = r_{xy}= \\frac{1}{n-1}\\sum_{=1}^nz_xz_y\\]Il est possible de calculer directement la corrélation à partir de la covariance, soit en divisant par le produit des écarts types des variables.\\[r_{xy} = \\frac{\\text{cov}_{xy}}{\\sigma_x\\sigma_y}\\]Voici trois options de programmation de la corrélation. La première et la deuxième sont basées sur la covariance. Dans la première, la covariance est calculée de nouveau dans la fonction. La deuxième quant à elle profite avantageusement de l’existence d’une fonction maison qui calcule directement la covariance. La troisième recourt au produit de variables standardisées.Évidemment, R possède une fonction de base pour le calcul d’une corrélation entre deux variables ou d’une matrice de corrélation (à partir d’un jeu de données de deux variables ou plus). Il s’agit de cor(). Ces trois fonctions maison en plus de la fonction R sont comparées ci-dessous. Pour ce faire, un jeu de données est créé possédant les caractéristiques désirées. Dans ce cas-ci, la taille d’échantillon est fixée à \\(n=10000\\) pour assurer une bonne précision dans l’échantillon et une corrélation \\(r=.7\\).Une façon simple de produire des données à partir d’une matrice de corrélation (ou de covariance) est d’utiliser la fonction mvrnorm() du package MASS. Elle évite pour l’instant d’introduire les manipulations mathématiques nécessaires (ce sera donc pour l’instant l’une de ces boîtes noires ayant la confiance de l’utilisateur, mais élucider dans le chapitre Créer).La fonction mvrnorm() pour MultiVariate Random NORMmal nécessite trois arguments :la taille d’échantillon n;la taille d’échantillon n;les moyennes mu des \\(p\\) variables, et;les moyennes mu des \\(p\\) variables, et;la matrice de covariance Sigma (la corrélation est un cas particulier de la covariance où les variables sont standardisées).la matrice de covariance Sigma (la corrélation est un cas particulier de la covariance où les variables sont standardisées).Le lecteur avisé aura noté la ressemblance de nomenclature entre mvrnorm() (multivariée) et\nrnorm() (univariée). Enfin, la dernière étape est de produire une matrice de covariance \\(p \\times p\\). La fonction matrix() prend une variable de données, dans cet exemple , c(1, r, r, 1) qu’elle répartit ligne par colonne (voir Créer une matrice. Comme \\(p=2\\), cela crée une matrice \\(2 \\times 2\\).Les résultats sont près des attentes et identiques.Pour tester la probabilité d’obtenir cette valeur, la corrélation \\(r\\) peut se transformer monotonement en valeur-\\(t\\) avec l’équation (11.5), ce qui permet de calculer des valeurs-\\(p\\).\\[\\begin{equation}\nt_{n-2} = \\frac{r}{(\\frac{\\sqrt{1-r^2}}{\\sqrt{n-2}})}\n\\tag{11.5}\n\\end{equation}\\]L’équation (11.5) se traduit simplement en code R.Dans le code ci-dessous, l’équation (11.5) précédente est subtilement réarrangée pour être plus simple et élégante quoiqu’équivalente.\\[\\begin{equation}\nt_{n-2} = \\frac{r}{(\\frac{\\sqrt{1-r^2}}{\\sqrt{n-2}})} = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\tag{11.6}\n\\end{equation}\\]L’équation (11.5) l’avantage de montrer la relation entre l’estimateur et l’erreur type alors que l’équation (11.6) est moins encombrante.","code":"\n# Option 1 : Calcul complet\ncorrelation1 <- function(x, y){\n  \n  # La taille d'échantillon\n  n <- length(x)\n  \n  # Centrer les variables\n  xc <- x - mean(x)\n  yc <- y - mean(y)\n  \n  # Produit des variables\n  prod.xy <- xc * yc\n  \n  # La somme des produits divisée par n - 1\n  cov.xy <- sum(prod.xy) / (n - 1)  # La covariance\n  \n  # Diviser par les écarts types\n  cor.xy <- cov.xy / (sd(x) * sd(y))\n  \n  return(cor.xy)\n}\n\n# Option 2 : Basé sur la covariance\ncorrelation2 <- function(x, y){\n  \n  # Calculer la covariance\n  cov.xy <- covariance(x,y)\n  \n  # Diviser par les écarts types\n  cor.xy <- cov.xy / (sd(x) * sd(y))\n  return(cor.xy)\n}\n\n# Option 3 : Basé sur des variables standardisées\ncorrelation3 <- function(x, y){\n  \n  # La taille d'échantillon\n  n <- length(x)\n  \n  # Centrer et réduire les variables\n  xs <- (x - mean(x)) / sd(x)\n  ys <- (y - mean(y)) / sd(y)\n  \n  # La somme des produits divisé par n - 1\n  cor.xy <- sum(xs * ys)/(n - 1)\n  \n  return(cor.xy)\n}# Pour la reproductibilité\nset.seed(820)\n\n# Taille d'échantillon importante pour réduire l'erreur d'échantillonnage\nn <- 10000\n\n# La corrélation désirée\nr <- .70\n\n# Création d'une matrice de corrélation\nR <- matrix(c(1, r, r, 1), nrow = 2, ncol = 2)\n\n# Création des données \ndonnees <- data.frame(MASS::mvrnorm(n = n, \n                                    mu = c(0,0), \n                                    Sigma = R))\ncolnames(donnees) <- c(\"x\", \"y\") # Ajouter des noms de colonnes\n\n# Voici la matrice de corrélation\nR\n>      [,1] [,2]\n> [1,]  1.0  0.7\n> [2,]  0.7  1.0\n\n# Voici une visualisation partielle des données\nhead(donnees)\n>        x      y\n> 1  0.635  0.914\n> 2 -0.193  0.360\n> 3 -0.602 -1.213\n> 4 -0.841 -0.986\n> 5 -0.105 -1.771\n> 6 -2.147 -1.590\n\n# Vérifications des variables\nmean(donnees$x); mean(donnees$y); sd(donnees$x); sd(donnees$y)\n> [1] -0.00578\n> [1] -0.00531\n> [1] 0.999\n> [1] 1.01\n\n# Les résultats sont près des attentes\n# Vérifications de la corrélations\ncor(donnees$x, donnees$y)\n> [1] 0.699\ncorrelation1(donnees$x, donnees$y)\n> [1] 0.699\ncorrelation2(donnees$x, donnees$y)\n> [1] 0.699\ncorrelation3(donnees$x, donnees$y)\n> [1] 0.699# Création d'une matrice de corrélation \n# Il pourrait également s'agir d'un vecteur ou d'un scalaire.\nr <- matrix(c( 1, .5, .4,\n               .5,  1, .3,\n               .4, .3,  1), \n            nrow = 3, ncol = 3)\n\n# Nombre d'unités (valeur arbitraire ici)\nn <- 10\n\n# Valeurs-t\nvt <- r * sqrt(n - 2) / sqrt(1 - r ^ 2)\n\n# Valeurs-p\nvp <- (1 - pt(abs(vt), df = n - 2)) * 2\nvt ; vp\n>      [,1]  [,2]  [,3]\n> [1,]  Inf 1.633 1.234\n> [2,] 1.63   Inf 0.889\n> [3,] 1.23 0.889   Inf\n>       [,1]  [,2]  [,3]\n> [1,] 0.000 0.141 0.252\n> [2,] 0.141 0.000 0.400\n> [3,] 0.252 0.400 0.000"},{"path":"analyser.html","id":"rapporter-une-corrélation","chapter":" 11 Analyser","heading":"11.2.3 Rapporter une corrélation","text":"Voici comment rapporter la corrélation dans un article. Plusieurs options sont possibles. Pour une seule corrélation, R permet d’obtenir la corrélation et la valeur-\\(p\\) avec la fonction cor.test(). Il est possible de rédiger la syntaxe ainsi.ou encore comme ceci.En général, la matrice corrélation complète est intéressante à rapporter. Cela se fait rapidement et simplement avec la fonction cor(). Toutefois, elle ne rapporte pas les valeurs-\\(p\\) et n’est pas agrémentée d’étoiles scintillantes. Plusieurs packages et fonctions sont envisageables, mais psych avec sa fonction corr.test est l’une des plus pertinentes. Voici un exemple avec le jeu de données mtcars (disponible dans R de base).La syntaxe retourne les corrélations, la taille d’échantillon et les valeurs-\\(p\\). La sortie contient encore plus d’informations, mais elle n’imprime que les usuelles. Il ne reste qu’à rédiger les corrélations les plus pertinents à rapporter (selon la théorie du domaine concerné).La matrice de corrélation des variables présentés. Parmi les corrélations d’intérêt, un lien remarquable est celui entre qsec et mpg qui est significatif, \\(r(10) = 0.42, p\\ =\\ = 0.42\\).","code":"\ncor.test(donnees$x, donnees$y)cor.test(~ y + x, data = donnees)\n> \n>   Pearson's product-moment correlation\n> \n> data:  y and x\n> t = 98, df = 9998, p-value <2e-16\n> alternative hypothesis: true correlation is not equal to 0\n> 95 percent confidence interval:\n>  0.689 0.709\n> sample estimates:\n>   cor \n> 0.699psych::corr.test(mtcars)\n> Call:psych::corr.test(x = mtcars)\n> Correlation matrix \n>        mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n> mpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60\n> cyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52\n> disp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59\n> hp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24\n> drat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71\n> wt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69\n> qsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23\n> vs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17\n> am    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00\n> gear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79\n> carb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06\n>       gear  carb\n> mpg   0.48 -0.55\n> cyl  -0.49  0.53\n> disp -0.56  0.39\n> hp   -0.13  0.75\n> drat  0.70 -0.09\n> wt   -0.58  0.43\n> qsec -0.21 -0.66\n> vs    0.21 -0.57\n> am    0.79  0.06\n> gear  1.00  0.27\n> carb  0.27  1.00\n> Sample Size \n> [1] 32\n> Probability values (Entries above the diagonal are adjusted for multiple tests.) \n>       mpg cyl disp   hp drat   wt qsec   vs   am gear carb\n> mpg  0.00   0 0.00 0.00 0.00 0.00 0.22 0.00 0.01 0.10 0.02\n> cyl  0.00   0 0.00 0.00 0.00 0.00 0.01 0.00 0.04 0.08 0.04\n> disp 0.00   0 0.00 0.00 0.00 0.00 0.20 0.00 0.01 0.02 0.30\n> hp   0.00   0 0.00 0.00 0.17 0.00 0.00 0.00 1.00 1.00 0.00\n> drat 0.00   0 0.00 0.01 0.00 0.00 1.00 0.19 0.00 0.00 1.00\n> wt   0.00   0 0.00 0.00 0.00 0.00 1.00 0.02 0.00 0.01 0.20\n> qsec 0.02   0 0.01 0.00 0.62 0.34 0.00 0.00 1.00 1.00 0.00\n> vs   0.00   0 0.00 0.00 0.01 0.00 0.00 0.00 1.00 1.00 0.02\n> am   0.00   0 0.00 0.18 0.00 0.00 0.21 0.36 0.00 0.00 1.00\n> gear 0.01   0 0.00 0.49 0.00 0.00 0.24 0.26 0.00 0.00 1.00\n> carb 0.00   0 0.03 0.00 0.62 0.01 0.00 0.00 0.75 0.13 0.00\n> \n>  To see confidence intervals of the correlations, print with the short=FALSE option"},{"path":"analyser.html","id":"les-données-nominales","chapter":" 11 Analyser","heading":"11.3 Les données nominales","text":"Jusqu’à présent, deux types d’association de données ont été présenté : une variable nominale (identifiant des groupes) avec une variable continue (différences de moyenne) et deux variables continues (association linéaire). Dans cette section, l’association entre deux variables nominales est présentée. Une façon de représenter l’association entre deux variables nominales est le tableau de contingence, soit l’illustration d’une distribution d’une variable (en ligne) pour chaque catégorie de l’autre (en colonne). En voici, un exemple.\nTable 11.1: Tableau de contingence de la relation entre posséder une voiture et le climatoscepticisme\nLe tableau 11.1 présente deux variables nominales (climatosceptique, à la verticale, et posséder une voiture, à l’horizontale) ayant chacune deux catégories (oui et non). Il montre les proportions observées. Ces variables sont-elles associées? Pour traiter cette question, les proportions attendues jouent un rôle crucial. En fait, l’hypothèse nulle sous la table de contingence postule que toutes les proportions du tableau de contingence sont indépendantes. Mathématiquement, il s’agit de postuler que les proportions d’une variable (en ligne ou en colonne) n’influencent pas celles de l’autre variable. Pour obtenir les proportions théoriques, les totaux des lignes et colonnes sont calculés ainsi que le grand total. La fréquence attendue d’une cellule est obtenue en calculant, pour chaque cellule, le produit de sa colonne et de sa ligne respective divisé par le grand total.\\[\\begin{equation}\nt_{ij} = \\frac{\\text{total}_i \\times \\text{total}_j}{\\sum \\text{total}}\n\\tag{11.7}\n\\end{equation}\\]L’équation (11.7) ci-dessus illustre l’idée sous-jacente à la proportion attendue, \\(t\\) d’une cellule de ligne, \\(\\), et colonne \\(j\\).\nTable 11.2: Tableau de contingence étendu avec les totaux\n","code":""},{"path":"analyser.html","id":"le-chi2-pour-table-de-contingence","chapter":" 11 Analyser","heading":"11.3.1 Le \\(\\chi^2\\) pour table de contingence","text":"Maintenant que la question statistique et que l’hypothèse nulle sont posées, comment mesurer le degré selon lequel les données s’écartent des attentes théoriques. Le \\(\\chi^2\\) (khi-carré) permet de calculer une telle mesure. Le \\(\\chi^2\\) correspond à la distance entre une valeur observée et théorique au carré, pondérée par la valeur théorique.\n\\[ \\chi^2_v = \\sum_{=1}^{l}\\sum_{j=1}^c(\\frac{(o_{ij}-t_{ij})^2}{t_{ij}})\\]\noù \\(o\\) correspond aux valeurs observées, \\(t\\) réfère aux valeurs théoriques, \\(v\\) représente les degrés de liberté et \\(\\) et \\(j\\) les lignes et colonnes respectivement. Le degré de liberté est\n\\[v = (n_{\\text{ligne}}-1)(n_{\\text{colonne}}-1)\\]Si l’hypothèse nulle est vraie, les valeurs observées et théoriques devraient être très près. Le carré permet de calculer une distance euclidienne et le dénominateur pondère la distance. Comme l’analyse de variance, le test de \\(\\chi^2\\) pour table de contingence est global, il n’informe pas d’où provient la dépendance, mais bien s’il y au moins une dépendance.Il y plusieurs techniques pour créer une base de données, l’essentiel étant de lier les proportions désirées. Ici, une variable sexe est créée avec 50-50% de chance d’être l’un ou l’autre sexe. Par la suite, une proportion différente liée au sexe est utilisée pour générer la fréquence de consommation de tabac. La syntaxe sert principalement à identifier les valeurs homme et femme de la première variable pour en associer une valeur tabac.La fonction table() de R génère une table de contingence. La fonction maison et deux méthodes de base R sont présentées et produisent les mêmes résultats pour le \\(\\chi^2\\). Un autre, le test exact de Fisher est également présenté. Celui-ci est plus robuste, mais peut requérir plus intensive des ressources de l’ordinateur pour les grosses tables de contingences.Le test du \\(\\chi^2\\) est sujet à certains problèmes qu’il est important de considérer sans quoi l’interprétation peut être faussée. Si la taille d’échantillon (le nombre d’unités d’observation observées est trop faible), alors le test est biaisé. Par exemple, trois lancers de pile ou face ne sont pas suffisants pour tester une hypothèse de khi-carré, le nombre de résultats différents est de 2, ce qui ne donne pas une approximation satisfaisante de la distribution d’échantillonnage. La convention est de dire qu’une fréquence théorique est trop petite si elle est plus petite que 5. Si ce n’est pas cas, des corrections ou d’autres options doivent être considérées.Dans le présent exemple, bien que les fréquences attendues respectent les critères usuels, il peut être utile d’envisager un test plus robuste comme le test exact de Fisher ou le \\(\\chi^2\\) avec correction de continuité. Le premier se commande avec la fonction fisher.test() et le second est la fonction par défaut de chisq.test(). C’est deux fonctions prennet comme argument une table de contigence (la sortir de table()) ou deux variables nominales. Pour montrer l’équivalence entre la fonction chisq.test() et la fonction maison, l’option de correction est désactivée avec l’argument correct = FALSE.","code":"\nkhicarre <- function(obs){\n  # Obs est une table de contingence\n  # Somme colonne\n  SC <- as.matrix(colSums(obs))\n  # Somme ligne\n  SL <- as.matrix(rowSums(obs))\n  # Grand total\n  TT <- sum(obs)\n  # Proportion théoriques\n  theo <- SL %*% t(SC) / TT\n  # khi carré\n  khi2 <- sum((obs - theo) ^ 2 / theo)\n  dl <- (length(SL) - 1) * (length(SC) - 1)\n  vp <- 1 - pchisq(khi2, dl)\n  # Sortie\n  statistiques <- list(khi2 = khi2, dl = dl, valeur.p = vp)\n  return(statistiques)\n}set.seed(54)\nn <- 100\n\n# Première variable\nsexe <- sample(c(\"homme\",\"femme\"), \n               size = n, \n               replace = TRUE, \n               prob = c(.5,.5))\ntabac <- rep(0, 100)\n\n# Proportions conditionnelles\ntabac[sexe == \"femme\"] <- sample(c(\"fumeur\",\"non-fumeur\"), \n                                 size = sum(sexe == \"femme\"), \n                                 replace = TRUE, \n                                 prob = c(.2,.8))\n\ntabac[sexe == \"homme\"] <- sample(c(\"fumeur\",\"non-fumeur\"), \n                                 size = sum(sexe == \"homme\"), \n                                 replace = TRUE, \n                                 prob = c(.1,.9))\n#base données\ndonnees <- data.frame(sexe, tabac)\ntable(donnees)\n>        tabac\n> sexe    fumeur non-fumeur\n>   femme      8         38\n>   homme      4         50# Table de contingence\nTC <- table(donnees)\n# Fonction maison\nkhicarre(TC)\n> $khi2\n> [1] 2.34\n> \n> $dl\n> [1] 1\n> \n> $valeur.p\n> [1] 0.126\n\n# Fonction de base\nsummary(TC)\n> Number of cases in table: 100 \n> Number of factors: 2 \n> Test for independence of all factors:\n>   Chisq = 2.3, df = 1, p-value = 0.1\n\n# Autre fonction de base\nresultat <- chisq.test(TC, correct = FALSE)\n# ou encore\nresultat <- chisq.test(donnees$sexe, donnees$tabac, correct = FALSE)\n# Leur sortie\nresultat\n> \n>   Pearson's Chi-squared test\n> \n> data:  donnees$sexe and donnees$tabac\n> X-squared = 2, df = 1, p-value = 0.1\n\n# Et pour plus de précision...\nresultat$statistic\n> X-squared \n>      2.34\nresultat$p.value\n> [1] 0.126\n\n# Le test exact de Fisher\nresultat.fisher <- fisher.test(TC)\n# ou encore\nresultat.fisher <- fisher.test(donnees$sexe, donnees$tabac)\n# Leur sortie\nresultat.fisher\n> \n>   Fisher's Exact Test for Count Data\n> \n> data:  donnees$sexe and donnees$tabac\n> p-value = 0.2\n> alternative hypothesis: true odds ratio is not equal to 1\n> 95 percent confidence interval:\n>   0.641 12.731\n> sample estimates:\n> odds ratio \n>       2.61\nresultat.fisher$p.value\n> [1] 0.216"},{"path":"analyser.html","id":"rapporter-un-chi2","chapter":" 11 Analyser","heading":"11.3.2 Rapporter un \\(\\chi^2\\)","text":"Voici comment rapporter un test de \\(\\chi^2\\) dans un article. Comme il été vu ci-haut, plusieurs options sont possiblesLa relation entre le sexe et le consommation de tabac est investiguée. Il appert que cette relation n’est pas statistiquement significative avec une seuil \\(\\alpha = .05\\), \\(chi^2(1) = 2.345\\), \\(p = 0.126\\). Le test exact de Fisher est calculé également et, pareillement, ne rejette pas l’hypothèse nulle, \\(p =0.216\\).","code":"# Pour le khi carré de Pearson\nchisq.test(donnees$sexe, donnees$tabac, correct = FALSE)\n> \n>   Pearson's Chi-squared test\n> \n> data:  donnees$sexe and donnees$tabac\n> X-squared = 2, df = 1, p-value = 0.1\n\n# Pour le test exact de Fisher\nfisher.test(donnees$sexe, donnees$tabac)\n> \n>   Fisher's Exact Test for Count Data\n> \n> data:  donnees$sexe and donnees$tabac\n> p-value = 0.2\n> alternative hypothesis: true odds ratio is not equal to 1\n> 95 percent confidence interval:\n>   0.641 12.731\n> sample estimates:\n> odds ratio \n>       2.61"},{"path":"analyser.html","id":"la-correction-de-yates","chapter":" 11 Analyser","heading":"11.3.3 La correction de Yates","text":"Même si c’est généralement le test classique de Pearson qui est utilisé, le \\(\\chi^2\\) avec correction de Yates, l’option par défaut de chisq.test(), est préférable. C’est un test un peu plus conservateur qui compense pour les déviations de la distribution de probabilité théorique. La correction consiste à faire la différence absolue entre la fréquence observée et théorique, puis de soustraire \\(\\frac{1}{2}\\), ce qui est représentée par l’équation suivante.\\[ \\chi^2_v = \\sum_{=1}^{l}\\sum_{j=1}^c(\\frac{(|o_{ij}-t_{ij}|-\\frac{1}{2})^2}{t_{ij}})\\]","code":""},{"path":"simuler.html","id":"simuler","chapter":" 12 Simuler","heading":" 12 Simuler","text":"Dans plusieurs contextes statistiques, les informations cruciales pour réaliser une analyse statistique ou tirer des résultats spécifiques ne sont pas connues (par exemple, le chapitre Analyser présente des analyses soutenues par des théorèmes). Pire, parfois certains postulats sont violés rendant les démarches usuelles caduques. Dans certains contextes, une analyse formelle pourrait s’avérer fort complexe, voire irrésolvable, alors qu’une analyse plus empirique usant des techniques de rééchantillonnage résoudra ces problèmes simplement et immédiatement.Jusqu’à présent, les situations des analyses statistiques étaient connues. Maintenant, ce sont les techniques nécessitant moins de prérequis statistiques, comme le bootstrap et les simulations Monte-Carlo qui seront présentés.","code":""},{"path":"simuler.html","id":"les-simulations-monte-carlo","chapter":" 12 Simuler","heading":"12.1 Les simulations Monte-Carlo","text":"Les simulations Monte-Carlo sont une famille de méthode algorithmique qui permet de calculer une valeur numérique en utilisant des procédés aléatoires. Le nom provient du prestigieux casino de Monte-Carlo à Monaco sur la Côte d’Azur, où des jeux de hasard se jouent constamment, et ayant ainsi une connotation très forte avec le hasard et les nombres aléatoires.Une simulation de Monte-Carlo prédit une étendue de résultats possibles à partir d’un ensemble de valeurs d’entrée fixes et en tenant compte de l’incertitude inhérente de certaines variables. En d’autres termes, une simulation de Monte-Carlo produit les résultats possibles (sorties) d’un modèle à partir des entrées fixes et d’autres entrées variables. Ces sorties sont calculées encore et encore (des milliers de fois, parfois plus), en utilisant à chaque fois un ensemble différent de nombres aléatoires, générant des sorties probables, mais différentes à chaque fois. Les estimations (et leur tendance) obtenues peuvent alors être interprétées.Les simulations Monte-Carlo sont particulièrement utiles, car elles permettent, en mathématiques, de calculer des intégrales très complexes; en physique, d’estimer la forme d’un signal ou la sensibilité; en finance, de recréer des conditions permettant de prévoir le marché; et en psychologie, de simuler des processus comportementaux ou cognitifs.Les simulations Monte-Carlo possèdent trois caractéristiques :Construire un modèle ayant des variables indépendantes (entrées) et dépendantes (sorties);Construire un modèle ayant des variables indépendantes (entrées) et dépendantes (sorties);Spécifier les caractéristiques aléatoires des variables indépendantes et définir des valeurs crédibles;Spécifier les caractéristiques aléatoires des variables indépendantes et définir des valeurs crédibles;Rouler les simulations de façon répétitive jusqu’à ce que suffisamment d’itérations soient produites et que les résultats convergent vers une solution.Rouler les simulations de façon répétitive jusqu’à ce que suffisamment d’itérations soient produites et que les résultats convergent vers une solution.Maintenant, ces étapes sont mises en contexte avec un problème.","code":""},{"path":"simuler.html","id":"le-problème-de-monty-hall","chapter":" 12 Simuler","heading":"12.1.1 Le problème de Monty-Hall","text":"Rien de mieux pour confronter le sens commun que d’être confronté à un problème contre-intuitif. Les simulations Monte-Carlo nous permettront de vérifier les résultats de façon empirique, parallèlement à ce qu’une analyse formelle fournit.Le problème de Monty Hall (attribuables à Selvin, 1975) présente un joueur devant un présentateur (Monty Hall). Trois portes sont offertes au choix du joueur. Derrière l’une d’entre elles se retrouve un magnifique prix : une voiture électrique de luxe. Derrière chacune des deux autres portes se retrouvent un prix citron : une chèvre chacune. Le joueur ne sait pas ce qu’il y derrière les portes. Le joueur peut alors choisir une porte. Évidemment, à ce moment le joueur à une chance sur trois de choisir la voiture.\nFigure 12.1: Illustration du problème de Monty Hall\nPar la suite, le présentateur, qui connaît le contenu derrière les portes, ouvre une porte qui () ne cache pas la voiture et (b) que le participant n’pas choisie. Le joueur peut alors choisir () de conserver la porte choisie ou (b) de changer de porte. Quelle option, s’il y en une, assurera la meilleure probabilité de choisir la voiture? Rester ou changer? Par exemple, suivant l’illustration de la Figure 12.1, le joueur choisit la porte 1, alors le présentateur doit ouvrir la porte 2 (non choisie et ne contient pas la voiture). Le joueur doit-il changer son choix? Ici, la réponse est évidente, car la réponse est connue. Qu’en est-il alors si le joueur choisi la porte 3 et le présentateur ouvre l’une des deux autres porte? Le joueur doit-il changer ou rester? La question sous-jacente est qu’elles sont les probabilités de rester et de changer respectivement. Sont-elles différentes?Comme le résultat n’est pas des plus intuitif (et sans divulgâcher le résultat), une petite simulation s’impose (ou une analyse formelle pour les lecteurs enclins mathématiquement). La situation sera recréée un millier de fois pour vérifier l’option (rester ou changer) qui maximise de gagner la voiture.En se référant aux trois points caractérisant une simulation Monte-Carlo susmentionné :Le problème de Monty Hall tel qu’imminemment décrit.Le problème de Monty Hall tel qu’imminemment décrit.Les variables indépendantes sontLes variables indépendantes sontle tirage aléatoire du contenu derrière les portes (distribution uniforme, chaque porte à la même probabilité d’avoir un prix ou non);le tirage aléatoire du contenu derrière les portes (distribution uniforme, chaque porte à la même probabilité d’avoir un prix ou non);le choix du joueur (distribution uniforme, pourrait être différent), et;le choix du joueur (distribution uniforme, pourrait être différent), et;la porte ouverte par le présentateur,la porte ouverte par le présentateur,l’action de rester ou de changer (les deux options sont étudiées).l’action de rester ou de changer (les deux options sont étudiées).Il y une variable dépendante :le succès (gagner le prix) ou l’échec (ne pas gagner le prix).La simulation est reprise 1000 fois.Les étapes de la simulation se déroulent ainsi.L’attribution des portes (une gagnante et deux perdantes) est faite aléatoirement11.L’attribution des portes (une gagnante et deux perdantes) est faite aléatoirement11.Le joueur fait son premier choix.Le joueur fait son premier choix.Plusieurs modèles peuvent être utilisés, comme systématiquement choisir la même porte, ce qui simplifie la situation et ne change pas les probabilités, car le contenu derrière les portes est aléatoire. Pour rester vrai à la situation, le joueur fait un choix aléatoire.Le comportement du présentateur s’enclenche, il doit “ouvrir” une porte non choisie et qui ne contient pas le prix.Le comportement du présentateur s’enclenche, il doit “ouvrir” une porte non choisie et qui ne contient pas le prix.Les deux stratégies sont simulées, rester ou changer.Les deux stratégies sont simulées, rester ou changer.La simulation enregistre s’il y eu gain ou non et l’additionne au total de chacun.La simulation enregistre s’il y eu gain ou non et l’additionne au total de chacun.La simulation est reprise un nombre important de fois, ici, 1000 suffit, mais des situations compliquées peuvent demander beaucoup plus d’itérations.La simulation est reprise un nombre important de fois, ici, 1000 suffit, mais des situations compliquées peuvent demander beaucoup plus d’itérations.Une petite digression avant de poursuivre. Il faut être naïf pour croire que le code fonctionne tel que prévu. À cause d’un inconvénient de la fonction sample(), une approche conditionnelle doit être utilisée. En fait, la fonction échantillonne les éléments de x (premier argument) jusqu’à obtenir size objets. Par contre, si une seule valeur est donnée à x et qu’elle est numérique, alors la fonction utilise les éléments de 1:x pour rééchantillonner au lieu de retourner x. Ainsi, si le choix et le prix (porte 2) sont derrière des portes différentes (porte 1 et 2, par exemple), alors une seule valeur est retournée (3), et sample() utilise 1:3 au lieu de 3. Ces particularités expliquent et justifient l’utilisation d’une formule conditionnelle. En programmation, il faut toujours s’assurer que les fonctions s’accordent avec les attentes. Ce cas est documenté et délibéré (voir ?sample), mais surprend.Quelles sont les résultats de cette simulation? Le fait de rester sur le premier choix devrait obtenir 33% de chance de réussir, comme le joueur initialement une chance sur trois d’avoir la bonne réponse. Qu’en est-il pour changer? Est-ce qu’ouvrir une porte chèvre modifie les probabilités? Les résultats de la simulation montre que rester gagne 31.6% et que changer gagne 68.4%. À long terme, il est plus avantageux de changer.Une explication simple est de dénombrer les possibilités. Dans le cas où le joueur ne change pas d’idée, il une chance sur trois, soit : choisir la chèvre 1, et garder la chèvre 1; choisir la chèvre 2, et garder la chèvre 2; et choisir la voiture, et garder la voiture. Si le joueur change d’idée après l’ouverture des portes, les chances sont maintenant de deux sur trois, soit choisir la chèvre 1, changer pour le prix; choisir la chèvre 2 et changer pour le prix; ou choisir le prix et changer pour une chèvre.Une autre façon de rendre se problème plus intuitif est de considérer le problème avec 100 portes au lieu de 3. Le présentateur ouvre les 98 portes qui ne sont pas un prix. Si le joueur choisit une porte et garde, il effectivement 1% de chance de remporter le prix. Par contre, s’il choisit une porte et que le présentateur lui ouvre toutes les autres portes chèvres, le joueur gagne à tout les coups s’il change, sauf s’il choisi le prix au premier coup. C’est comme si le participant ouvrait les 99 portent, sauf celle qu’il choisit au début.Avec quelques modifications de la syntaxe précédente, le code suivant illustre le cas à 100 portes. À noter que l’usage du conditionnelle n’est plus nécessaire au bon fonctionnement de sample(), car il y maintenant plus de trois portes.La simulation montre que rester gagne 0.9% et que changer gagne 99.1%.Des situations fort plus compliquées peuvent être apportées en simulations Monte-Carlo. Le jeu de Black Jack en est un exemple, bien qu’il soit déjà résolu pour des résultats optimaux (Millman, 1983). Peu importe la situation à simuler, il suffit d’avoir la patience de la programmer pour confirmer les résultats analytiques.Sur le plan pratique pour l’expérimentateur, les simulations Monte-Carlo peuvent être utiles pour calculer des tailles d’échantillons pour des modèles complexes, comme des modèles par équations structurelles compliquées, multiniveaux ou de classes latentes. Il peut être utile également pour imiter des processus cognitifs ou comportementaux. Toutefois, une sous-famille est d’une importance significative pour l’expérimentateur, celle qui sera abordée maintenant, le bootstrap.","code":"# Simulation du problème de Monty Hall\n# Valeurs possibles des portes\nportes <- c(\"Chèvre\", \"Chèvre\", \"Voiture\")\n# Nombre de répétitions\nnreps <- 1000\n# Pour la reproductibilité\nset.seed(7896)\n\n# Valeur initiale des gains\ngain.rester <- 0\ngain.changer <- 0\n\n# Boucle pour répéter `nreps` fois le scénario\nfor(i in 1:nreps){\n  \n  # Arrangement initial des portes\n  tirage <- sample(portes)\n  \n  # Trouver le prix\n  prix <- which(tirage %in% \"Voiture\")\n  \n  # Choix aléatoire\n  choix1 <- sample(length(portes), size = 1)\n  \n  # Sélectionner la porte avec la chèvre (pas un prix) et\n  # qui n'est pas celle choisie (choix1)\n  option <- c(choix1, prix)\n  \n  if(choix1 != prix){\n    \n    # Si une porte valide\n    monty <- c(1:3)[-option]\n    \n  }else{\n    \n    # Si deux portes valides, en choisir une aléatoirement.\n    monty <- sample(c(1:3)[-option], size = 1)\n    \n  }\n  \n  # Décision : Reste à choix1\n  choix.rester <- choix1\n  \n  # Décision : Changer de choix changer\n  # Changer = ne pas prendre la porte initial, ni la porte ouverte\n  choix.changer <- c(1:3)[-c(choix1, monty)]\n  \n  # Enregistrement des gains\n  gain.rester  <- gain.rester  + \n                   (tirage[choix.rester]  == \"Voiture\")\n  gain.changer <- gain.changer + \n                   (tirage[choix.changer] == \"Voiture\")\n}\n\ncbind(gain.rester, gain.changer) / nreps\n>      gain.rester gain.changer\n> [1,]       0.316        0.684# Simulation du problème de Monty Hall\nportes <- c(\"Voiture\", rep(\"Chèvre\", 99))\nn <- length(portes)\nnreps <- 1000\nset.seed(7896)\n\n# Valeur initial des gains\ngain.rester <- 0\ngain.changer <- 0\n\nfor(i in 1:nreps){\n  # Arrangement initial des portes\n  tirage <- sample(portes)\n  \n  # Trouver le prix\n  prix <- which(tirage %in% \"Voiture\")\n  \n  # Choix aléatoire\n  choix1 <- sample(n, size = 1)\n  \n  # Sélectionner la porte avec la chèvre (pas un prix) et\n  # qui n'est pas celle choisie (choix1)\n  option <- c(choix1, prix)\n  \n  # Les autres portes\n  monty <- sample(c(1:n)[-option], size = n - 2)\n  \n  # Décision : Reste à choix1\n  choix.rester <- choix1\n  \n  # Décision : Changer de choix changer\n  # Changer = ne pas prendre la porte initial, ni la porte ouverte\n  choix.changer <- c(1:n)[-c(choix1, monty)]\n  \n  # Enregistrement des gains\n  gain.rester <- gain.rester + \n                  (tirage[choix.rester] == \"Voiture\")\n  gain.changer <- gain.changer + \n                  (tirage[choix.changer] == \"Voiture\")\n}\n\ncbind(gain.rester,gain.changer)  / nreps\n>      gain.rester gain.changer\n> [1,]       0.009        0.991"},{"path":"simuler.html","id":"le-bootstrap","chapter":" 12 Simuler","heading":"12.2 Le bootstrap","text":"Le bootstrap (dont il n’y pas d’excellente traduction en français) est une des techniques de rééchantillonnage astucieuses permettant des inférences statistiques (Efron & Tibshriani, 1979). Il fait partie de la famille des simulations Monte-Carlo, car il se base sur la réitération multiple d’une statistique à partir d’un jeu de données. Sans entrer dans les détails, il existe plusieurs types de techniques de bootstrap, qui font elles-mêmes parties d’une plus grande famille, les simulations stochastiques. Y est inclus les simulations de Monte-Carlo (dont le bootstrap fait parti) et les méthodes numériques bayésiennes.Cet ouvrage insiste sur le bootstrap non paramétrique, impliquant que les distributions sous-jacentes aux échantillons ne soient pas spécifiées, bien qu’il existe du bootstrap paramétrique. Ce premier type de bootstrap est certainement la plus utile pour l’expérimentateur.Dans le bootstrap, l’échantillon initial est considéré comme une pseudopopulation. Il ne nécessite pas d’autre information que celle fournie par l’échantillon. Il permet d’obtenir une distribution d’échantillonnage et tous ses bénéfices. Alors que dans les chapitres Inférer et Analyser, il faut préciser et connaître quelle distribution d’échantillonnage correspond à quelle statistique (p. ex., le score-\\(z\\) d’une unité à la distribution normale; la moyenne d’un échantillon à la distribution \\(t\\) si la variance est inconnue), aucune de ces connaissances n’est nécessaire. L’étendue d’application du bootstrap est immense, surtout pour les concepts statistiques qui offriront plus de défis, ce qui sera vu dans des chapitres ultérieurs.Comme abordé dans le chapitre Inférer, la distribution d’échantillonnage permet :d’estimer l’indice désiré;d’estimer l’indice désiré;d’estimer son erreur standard;d’estimer son erreur standard;d’estimer ses intervalles de confiance;d’estimer ses intervalles de confiance;de réaliser un test d’hypothèse;de réaliser un test d’hypothèse;tout cela en ignorant les distributions et postulats sous-jacents qui empêchent ou limitent leurs usages. Le bootstrap n’que deux hypothèses fondamentales : l’échantillon reflète la population et chaque unité est indépendante et identiquement distribuée. Autrement dit, chaque unité provienne bel et bien d’une même boîte (population, voir Inférer) et le tirage d’une unité n’influence pas celle d’une autre.Le fondement du bootstrap repose sur les étapes suivantes :Sélectionner avec remise les unités d’un échantillon;Sélectionner avec remise les unités d’un échantillon;Calculer et enregistrer l’indice statistique désiré auprès de ce nouvel échantillon;Calculer et enregistrer l’indice statistique désiré auprès de ce nouvel échantillon;Réitérer les deux premières étapes un nombre élevé de fois.Réitérer les deux premières étapes un nombre élevé de fois.Ce processus hautement facilité par l’excellente performance des ordinateurs d’aujourd’hui, se réalise très facilement et rapidement. L’exemple suivant se base sur le rééchantillonnage (1000 fois) de la moyenne à partir d’une variable aléatoire tirée d’une distribution uniforme avec un minimum de 5 et d’une maximum de 15.La fonction sample(, replace = TRUE) rééchantillonne avec remplacement les identifiants des unités. Par simplicité, il est possible d’éliminer la ligne nouveau.X = X[id] en utilisant mean(X[id]) ou plus simplement mean(sample(X, replace = TRUE)). Cette utilisation se limite seulement au cas d’un vecteur de données. S’il s’agit d’un jeu de données avec plus d’une variable, alors mean(X[id, ]) est utilisée. Noter l’usage de la , entre crochets. De cette façon, toutes les variables des unités identifiées par id sont extraites (voir Référer à une variable dans un jeu de données).À partir des informations obtenues, des inférences statistiques sont possibles. Toutes les estimations sont présentées comme un histogramme tel que l’illustre la Figure 12.2. Le code se retrouve ci-dessous. Quelques formules pour améliorer la présentation se retrouvent dans la syntaxe. L’utilisation simple de hist() pourra convenir.\nFigure 12.2: Historgamme des estimations des échantillons\nLa distribution de la Figure 12.2 se désigne comme la distribution d’échantillionnage, comme c’est le cas dans le chapitre sur les inférences Inférer. Elle agit comme distribution de la population de l’indice. Elle permet d’avoir une estimation plus robuste de l’erreur standard, qui se trouve à être l’écart type des indices rééchantillonnés. Elle est également utilisée pour construire des intervalles de confiance et permet ainsi de faire des tests d’hypothèse comme : l’intervalle de confiance à \\((1-\\alpha) \\times 100\\) % contient-elle la valeur 0? Comme il est fait avec une hypothèse nulle traditionnelle.Le cas illustré est trivial au sens où, par le théorème central limite, la distribution des moyennes est déjà connue. Si l’exemple était plutôt sur la médiane, il faudrait obligatoirement procéder par bootstrap pour calculer son erreur type et ses intervalles de confiance, car aucune formule exacte ne permet sa dérivation (il existe bien des approximations cela dit). Le bootstrap est alors des plus appropriés pour calculer l’erreur type et en tirer des intervalles de confiances, voire même en faire des tests d’hypothèses. Plusieurs statistiques auront recours au bootstrap pour dériver ces informations, la plus utilisée étant le coefficient de détermination (voir le chapitre sur la régression) ou la médiation pour tester les effets indirects.À partir des informations obtenues auprès du rééchantillonnage, il est possible d’obtenir les éléments désirés. Comme le cas est trivial, les indices statistiques seront très similaires. Cela confirmera au lecteur qu’aucun principe ésotérique ne s’est déroulé devant ses yeux en plus de confirmer que les statistiques attendues se produisent effectivement.La moyenne et l’erreur standard sont très près de la moyenne de l’échantillon et celle de la population (qui est de 10) et de l’erreur type attendue. Il est possible de créer des intervalles de confiances avec la fonction quantile qui prend en argument un vecteur de données et les probabilités désirées. Dans le cas de 95% pour une erreur de type de 5%, soit \\(\\alpha=.05\\), il s’agit de \\(.05/2 = .025\\) et \\(1-.05/2= .975\\), laissant au total 5% aux extrémités.Pour réaliser le test d’hypothèse nulle, il suffit de constater si l’intervalle de confiance contient ou non 0. Dans le cas où l’intervalle contient 0, le test n’est pas significatif; l’hypotèse nulle n’est pas rejetée; il est vraisemblable que l’absence d’effet soit vraie. S’il ne contient pas 0, l’hypothèse nulle est rejetée, le test est significatif et il est vraisemblable qu’il y ait un effet. Dans cet exemple, la moyenne est clairement différente de 0, car elle ne contient pas cette valeur.","code":"\nset.seed(158)\n# Nombre d'unités\nn <- 30\n\n# Le nombre de rééchantillonnage\nnreps <- 10000\n\n# Création de la variable\nX <- runif(n = n, min = 5, max = 15)\n\n# Création d'une variable vide pour l'enregistrement\nmoyenne.X <- numeric()\n\n# L'utilisation d'une boucle permet de réitérer les étapes\nfor (i in 1:nreps){\n  # Rééchantillonnage avec remise\n  id <- sample(n, replace = TRUE)\n  \n  # Nouvel échantillon\n  nouveau.X <- X[id]\n  \n  # Calculer et enregistrer la moyenne\n  moyenne.X[i] <- mean(nouveau.X)\n}\nhist(moyenne.X,              # Données\n     ylab = \" Frequence\",    # Changer l'axe y\n     main = \"\",              # Retirer le titre\n     breaks = 50,            # Nombre de colonnes\n     xlim = c(7,13)          # Définir l'étendu de l'axe x\n)# Voici la moyenne et l'erreur standard originales\nmean(X) ; sd(X)/sqrt(n)\n> [1] 10.2\n> [1] 0.485\n\n# La moyenne et l'erreur standard à partir des rééchantillons\nmean(moyenne.X)  ; sd(moyenne.X)\n> [1] 10.2\n> [1] 0.481# Erreur de type I\nalpha <- .05\n\n# Valeurs critiques\ncrit <- c(alpha/2, (1-alpha/2))\ntv <- qt(crit, df = n - 1)\n\n# Intervalles basés sur les indices de l'échantillon\nmean(X) + tv * sd(X)/sqrt(n)\n> [1]  9.2 11.2\n\n# Intervalles basés sur les indices du rééchantillonnage\nmean(moyenne.X) + tv * sd(moyenne.X)\n> [1]  9.22 11.18\n\n# Intervalles sur le rééchantillonnage\nquantile(moyenne.X, crit)\n>  2.5% 97.5% \n>  9.26 11.13"},{"path":"simuler.html","id":"meilleures-pratiques","chapter":" 12 Simuler","heading":"12.2.1 Meilleures pratiques","text":"R n’est pas particulièrement efficace pour réaliser des boucles. Il existe une famille de fonctions apply qui permet d’appliquer une fonction sur une série de vecteurs (comme une liste, un jeu de données ou une matrice). L’appel à l’aide ?apply donne beaucoup d’informations à ce sujet.La fonction principale est apply(). Elle nécessite le jeu de données (X) sur lequel appliqué la fonction (FUN) et un argument (MARGIN) pour identifier la dimension selon laquelle il faut appliquer la fonction, comme MARGIN = 1 produit l’opération par lignes; MARGIN = 2 produit l’opération par colonnes.12","code":"# Il faut un vecteur de données `vec`, un nombre de répétitions `nreps`\n# et une fonction `theta` à calculer sur le vecteur\nbootstrap <- function(vec, nreps, theta){\n  btsp.vec <- matrix(sample(vec, nreps * length(vec), replace = TRUE),\n                     nrow = nreps)\n  btsp.res <- apply(X = btsp.vec, MARGIN = 1, FUN = theta)\n  return(btsp.res)\n}\n\n# En utilisant les données de l'exemple précédent\nbtsp <- bootstrap(vec = X, nreps = nreps, theta = mean)\n\n# Pour fin de comparaison :\n# Voici la moyenne et l'erreur standard de ce bootstrap\nmean(btsp)  ; sd(btsp)\n> [1] 10.2\n> [1] 0.484\n\n# Voici la moyenne et l'erreur standard à partir du premier bootstrap\nmean(moyenne.X)  ; sd(moyenne.X)\n> [1] 10.2\n> [1] 0.481\n\n# Et voici la moyenne et l'erreur standard originales\nmean(X) ; sd(X)/sqrt(n)\n> [1] 10.2\n> [1] 0.485"},{"path":"simuler.html","id":"les-packages-1","chapter":" 12 Simuler","heading":"12.2.2 Les packages","text":"Il existe plusieurs packages pour réaliser du bootstrap dans R. Il y bootstrap (Tibshirani & Leisch, 2019) et boot (Canty & Ripley, 2021). Ajouter à cela que plusieurs fonctions R possèdent des options de bootstrap intégrées (qu’il faut commander dans les arguments). Plusieurs analyses statistiques recourant aux bootstraps sont déjà implantées. L’aperçu donné dans ce chapitre devrait convaincre le lecteur que, s’il besoin de bootstrap, il est assez rudimentaire de l’implanter soi-même.","code":""},{"path":"exercice-analyse.html","id":"exercice-analyse","chapter":"Exercices","heading":"Exercices","text":"Avec le jeu de données mtcars, réaliser une analyse descriptive complète. Ne conservez que la moyenne, l’écart type, l’asymétrie et l’aplatissement.Avec le jeu de données mtcars, réaliser une analyse descriptive complète. Ne conservez que la moyenne, l’écart type, l’asymétrie et l’aplatissement.Avec le jeu de données CO2, faire une table de contingence entre Treatment et Type.Avec le jeu de données CO2, faire une table de contingence entre Treatment et Type.Produire les valeurs-\\(t\\) critiques pour \\(dl = 1,2,3 ,... ,30\\) et \\(\\alpha=.05\\) unilatérale.Produire les valeurs-\\(t\\) critiques pour \\(dl = 1,2,3 ,... ,30\\) et \\(\\alpha=.05\\) unilatérale.Comparer la puissance de la distribution-\\(t\\) avec 20 degrés de liberté par rapport à une distribution normale centrée réduite avec une \\(\\alpha = .05\\) bilatérale. L’hypothèse alternative est distribuée normalement et fixée à une moyenne de 2 et l’écart type est de 1.Comparer la puissance de la distribution-\\(t\\) avec 20 degrés de liberté par rapport à une distribution normale centrée réduite avec une \\(\\alpha = .05\\) bilatérale. L’hypothèse alternative est distribuée normalement et fixée à une moyenne de 2 et l’écart type est de 1.Calculer la puissance d’une corrélation de \\(\\rho = .30\\) avec 80 participants et un \\(\\alpha = .05\\) bilatérale. Rappel : une corrélation peut se standardiser avec la tangente hyperbolique inverse, soit atanh(), et en multipliant par l’erreur type, \\(\\sqrt{n-3}\\).13 (Question difficile)Calculer la puissance d’une corrélation de \\(\\rho = .30\\) avec 80 participants et un \\(\\alpha = .05\\) bilatérale. Rappel : une corrélation peut se standardiser avec la tangente hyperbolique inverse, soit atanh(), et en multipliant par l’erreur type, \\(\\sqrt{n-3}\\).13 (Question difficile)Avec le jeu de données ToothGrowth, réaliser un test-\\(t\\) afin de comparer les supp par rapport à la longueur des dents (len).Avec le jeu de données ToothGrowth, réaliser un test-\\(t\\) afin de comparer les supp par rapport à la longueur des dents (len).Avec le jeu de données sleep, faire un test-\\(t\\) permettant de comparer les deux temps de mesure nommés group par rapport à la variable dépendante extra.Avec le jeu de données sleep, faire un test-\\(t\\) permettant de comparer les deux temps de mesure nommés group par rapport à la variable dépendante extra.Réaliser une simulation. Calculer la probabilité de gagner au jeu du roche-papier-ciseau pour chacune des options.Réaliser une simulation. Calculer la probabilité de gagner au jeu du roche-papier-ciseau pour chacune des options.Réaliser une simulation. Trouver la valeur critique (c.vrit) pour un \\(\\alpha = .025\\) unilatérale d’une distribution normale centrée sur \\(0\\) et un écart type de \\(1/\\sqrt{n}\\). Le scénario : tirer aléatoirement un échantillon de \\(n=30\\) participants à partir d’une distribution normale de moyenne \\(.5\\) et un écart type de 1. Calculer la moyenne de cet échantillon. Pour chaque scénario, additionner chaque occasion où cette moyenne est plus élevée que la valeur critique. Répéter ce scénario 1000 fois. Calculer la probabilité (en pourcentage) d’occurrence selon laquelle la moyenne de l’échantillon est plus élevée que la valeur critique.Réaliser une simulation. Trouver la valeur critique (c.vrit) pour un \\(\\alpha = .025\\) unilatérale d’une distribution normale centrée sur \\(0\\) et un écart type de \\(1/\\sqrt{n}\\). Le scénario : tirer aléatoirement un échantillon de \\(n=30\\) participants à partir d’une distribution normale de moyenne \\(.5\\) et un écart type de 1. Calculer la moyenne de cet échantillon. Pour chaque scénario, additionner chaque occasion où cette moyenne est plus élevée que la valeur critique. Répéter ce scénario 1000 fois. Calculer la probabilité (en pourcentage) d’occurrence selon laquelle la moyenne de l’échantillon est plus élevée que la valeur critique.Réaliser un bootstrap la corrélation entre sleep_total (temps de sommeil total) et bodywt (poids du corps) dans le jeu de données msleep (du package ggplot2). Produire la moyenne et l’écart type des échantillons bootstrapées ainsi que l’intervalle de confiance à 95%.Réaliser un bootstrap la corrélation entre sleep_total (temps de sommeil total) et bodywt (poids du corps) dans le jeu de données msleep (du package ggplot2). Produire la moyenne et l’écart type des échantillons bootstrapées ainsi que l’intervalle de confiance à 95%.","code":""},{"path":"comparer.html","id":"comparer","chapter":" 13 Comparer","heading":" 13 Comparer","text":"Les expérimentateurs souhaitent svouent tester plus que deux moyennes. L’analyse de variance, souvent appelée ANOVA (ANalysis VAriance), permet de comparer une variable continue sur plusieurs groupes ou traitements. Il s’agit d’une extension du test-\\(t\\) indépendant qui compare une variable continue auprès de deux groupes. À plus de deux groupes, l’analyse de variance entre en jeu.Attention! Cette section se limite au cas où les groupes sont de tailles identiques (même nombre de participants par groupe, le symbole \\(n_k\\) pour référer à cette quantité) afin d’en simplifier la présentation.","code":""},{"path":"comparer.html","id":"la-logique-de-lanalyse-de-variance","chapter":" 13 Comparer","heading":"13.1 La logique de l’analyse de variance","text":"Comme il été présenté dans la section sur le test-\\(t\\) indépendant, le modèle sous-jacent à l’analyse de variance à un facteur est une extension de ce modèle pour \\(k\\) groupes. Chaque groupe est associé à une différence de moyennes \\(\\mu_i\\), pour \\(= 1,2, ...k\\) par rapport à la moyenne groupe référent, \\(\\mu_0\\). Les variables \\(x_i\\) définissent l’appartenance au groupe \\(\\) par la valeur 1 et 0 pour les autres groupes (codage factice ou dummy coding).\\[y = \\mu_0 + \\mu_1x_1 + \\mu_2x_2+ ... +\\mu_kx_k + \\epsilon\\]\nL’hypothèse nulle est la suivante.\n\\[\\sigma^2_1=\\sigma^2_2=...=\\sigma^2_k\\]\nL’analyse de variance est un test omnibus (global) qui ne teste pas où est la différence, mais bien s’il y au moins une différence entre les groupes. L’hypothèse nulle peut s’avérer fausse de plusieurs façons. Il peut y avoir une ou plusieurs inégalités pour rejeter l’hypothèse nulle.La logique sous-jacente est basée sur l’idée selon laquelle les moyennes des groupes proviennent d’une même population. Elle compare les hypothèses suivantes : l’hypothèse nulle : les données (les moyennes) proviennent d’une même population; et l’hypothèse opposée : les données ne proviennent pas d’une même population.Elle suggère ainsi deux types de variances (ou carré moyen, CM, dans ce contexte) : le CMI, carré moyen intergroupe, lorsque l’hypothèse nulle est vraie (la variabilité des moyennes); et le CMR, carré moyen résiduel, lorsque l’hypothèse nulle n’est ni vraie, ni fausse (la variabilité des données).Pour calculer le CMI, la variance des moyennes des groupes est\n\\[s^2_{\\bar{x}}=\\sum_{=1}^k\\frac{(\\bar{x_i}-\\overline{\\overline{x}})^2}{k-1}\\]où \\(\\overline{\\overline{x}}\\) est la grande moyenne (la moyenne de toutes les unités). Cette statistique se retrouve sur la plan des distributions d’échantillonnage (comment les moyennes se distribuent). Il faut ainsi multiplier cette valeur par \\(n_k\\), le nombre d’unités par groupes, pour obtenir une estimation de la variance de la population. Ainsi, \\(\\text{CMI} = s^2_{\\bar{x}}n_k\\).Il y plusieurs méthodes pour calculer le CMR, comme le CMR est envisagé comme la moyenne des variances de groupes, il est pratique d’en référer le calcul à sa définition.\\[\n\\text{CMR} = \\bar{s^2} = \\frac{1}{k}\\sum_{=k}^k\\sigma_k^2\n\\]\nLorsque l’hypothèse nulle est vraie, les variances des groupes sont traitées comme le résultat d’une étude sur une même population, comme si \\(k\\) petites études avaient été réalisées. L’estimation du CMI est de l’ordre d’une distribution d’échantillonnage connue par le théorème central limite définissant le comportement des moyennes lorsqu’elles proviennent d’une même population (c’est ce que pose comme hypothèse l’analyse de variance). Toutefois, la valeur du CMI est relative au CMR, la quantité de bruit dans les données.L’analyse de variance pose alors la question : la variance attribuable aux différentes moyennes des groupes est-elle supérieure à la variance résiduelle? Pour tester cette question, une possibilité est de tester le ratio \\(\\frac{\\text{CMI}}{\\text{CMR}}\\), ce qui donne une valeur-\\(F\\). Le ratio de deux variances suit une distribution-\\(F_{dl_1, dl_2}\\) avec deux degrés de liberté différents qui lui sont associés. Autrement dit, la distribution-\\(F\\) est la distribution d’échantillonnage du ratio deux variances.\\[F_{dl_1,dl_2} = \\frac{n_ks^2_{\\bar{x}}}{\\bar{s^2}} = \\frac{\\text{CMI}}{\\text{CMR}}\\]\noù \\(dl_1 = k-1\\), et \\(dl_2 = k(n_k-1)\\). L’interprétation du ratio est ainsi :si \\(\\text{CMI}>\\text{CMR}\\), alors \\(\\text{CMI}/\\text{CMR}>1\\);si \\(\\text{CMI}=\\text{CMR}\\), alors \\(\\text{CMI}/\\text{CMR}=1\\);si \\(\\text{CMI}<\\text{CMR}\\), alors \\(\\text{CMI}/\\text{CMR}<1\\).Plus les moyennes sont variables, plus la valeur du CMI est élevée. Plus les unités sont variables, plus le CMR est élevé. S’il existe au moins une différence entre les groupes, le ratio est en faveur du CMI. En fait, plus \\(F\\) est élevée, toutes autres choses étant égales (les degrés de libertés, p. ex.), plus la probabilité de rejeter l’hypothèse nulle est grande. Ainsi, lorsque, la valeur-\\(F\\) est obtenue, il est possible de calculer une valeur-\\(p\\), et le test d’hypothèse se déroule tel qu’auparavant.","code":""},{"path":"comparer.html","id":"la-création-de-données","chapter":" 13 Comparer","heading":"13.2 La création de données","text":"Dans un jeu de données commun, une variable désigne les groupes et une autre leur score sur une certaine mesure. La syntaxe suivante montre trois façons pour créer une variable facteur.Il est aussi possible de remplacer les arguments, 1:k par des chaînes de caractères (des catégories au lieu de nombres).Une bonne pratique dans le contexte des comparaisons de moyenne est de déclarer les variables catégorielles comme facteur avec .factor().Pour créer des valeurs à ces catégories, une stratégie simple est de créer des valeurs pour chacun des groupes et de les combiner.","code":"nk <- 5 # nombre d'unités par groupe\nk <- 4  # nombre de groupes\n# En ordre\ngroupe1 <- rep(1:k, each = nk)\n# En alternance\ngroupe2 <- rep(1:k, times = nk)\n# Aléatoire\nset.seed(765)\ngroupe3 <- sample(x = 1:k, size = (k * nk), replace = TRUE)\ncbind(groupe1, groupe2, groupe3)\n>       groupe1 groupe2 groupe3\n>  [1,]       1       1       4\n>  [2,]       1       2       2\n>  [3,]       1       3       4\n>  [4,]       1       4       3\n>  [5,]       1       1       3\n>  [6,]       2       2       4\n>  [7,]       2       3       3\n>  [8,]       2       4       1\n>  [9,]       2       1       4\n> [10,]       2       2       3\n> [11,]       3       3       3\n> [12,]       3       4       2\n> [13,]       3       1       3\n> [14,]       3       2       3\n> [15,]       3       3       2\n> [16,]       4       4       4\n> [17,]       4       1       4\n> [18,]       4       2       2\n> [19,]       4       3       1\n> [20,]       4       4       1categorie <- c(\"char\", \"chat\", \"cheval\", \"chevalier\", \"chien\")\nnk <- 2\ngroupe <- as.factor(rep(categorie, each = nk)) # Déclarer comme facteur\ngroupe\n>  [1] char      char      chat      chat      cheval   \n>  [6] cheval    chevalier chevalier chien     chien    \n> Levels: char chat cheval chevalier chien# Pour la reproductibilité\nset.seed(2602)\nchar      <- rnorm(n = nk, mean = 15, sd = 4) \nchat      <- rnorm(n = nk, mean = 20, sd = 4) # Différences ici\ncheval    <- rnorm(n = nk, mean = 10, sd = 4) # et ici\nchevalier <- rnorm(n = nk, mean = 15, sd = 4)\nchien     <- rnorm(n = nk, mean = 15, sd = 4) \n# Combinés\nscore <- round(c(char, chat, cheval, chevalier, chien))\n# Conserver toutes les informations en un jeu de données\ndonnees <- data.frame(groupe, score)\nhead(donnees)\n>   groupe score\n> 1   char    14\n> 2   char    12\n> 3   chat    17\n> 4   chat    21\n> 5 cheval    14\n> 6 cheval    10"},{"path":"comparer.html","id":"codage-factice","chapter":" 13 Comparer","heading":"13.3 Codage factice","text":"La plupart du temps, les variables de regroupement, les variables identifiant l’appartenance aux groupes, sont construites avec une variable de type facteur, c’est-à-dire une colonne avec différentes valeurs ou libellés. C’est d’ailleurs ce qui été fait dans l’exemple précédent. Cette méthode d’identification de groupement implique une programmation plus intensive, surtout pour la création de valeurs. En termes de programmation, il est plus élégant de recourir à une fonction de codage factice (dummy coding). Cela permettra de représenter fidèlement le modèle sous-jacent. Étrangement, il n’y pas de fonction de base avec R pour du codage factice. Une fonction maison permettra d’automatiser la réassignation des groupes (en une seule variable) sur plusieurs variables désignant leur appartenance.La plupart du temps, les variables de regroupement, les variables identifiant l’appartenance aux groupes, sont construites avec une variable de type facteur, c’est-à-dire une colonne avec différentes valeurs ou libellés. C’est d’ailleurs ce qui été fait dans l’exemple précédent. Cette méthode d’identification de groupement implique une programmation plus intensive, surtout pour la création de valeurs. En termes de programmation, il est plus élégant de recourir à une fonction de codage factice. Cela permettra de représenter fidèlement le modèle sous-jacent. Étrangement, il n’y pas de fonction de base avec R pour du codage factice. Une fonction maison permettra d’automatiser la réassignation des groupes (en une seule variable) sur plusieurs variables désignant leur appartenance.La fonction maison retourne un codage factice pour les \\(k\\) groupes. Attention, le codage factice est fait en ordre alphabétique. Comme il est redondant d’avoir \\(k\\) groupes (identifier \\(k\\) groupes nécessite \\(k-1\\) variables), un groupe référent est désigné. Ce dernier aura 0 sur tous les scores. Pour retirer un groupe, l’utilisation des crochets et une valeur négative associés à la colonne du groupe référent feront l’affaire, p. ex. dummy.coding(x)[,-k] déclare le \\(k\\)e groupe comme le groupe référent.L’expression X %*% mu est une multiplication d’algèbre matricielle qui multiplie la matrice \\(n \\times p\\) de codage factice \\(X\\) à une matrice \\(p\\times 1\\) de moyennes \\(\\mu\\). L’opération multiple les \\(p\\) éléments d’une ligne de \\(X\\) à la colonne \\(p\\) correspondante de \\(\\mu\\). En algèbre matricielle Le résultat est une matrice \\(n \\times 1\\) qui contient les différences de moyennes pour chaque unité. Dans la même ligne de syntaxe, la moyenne de la population (groupe référent), \\(\\mu_0\\) est ajoutée et la variation individuelle, \\(\\epsilon\\).Les scores produits sont identiques. Dans cet exemple par contre, l’origine des différences entre groupes est plus évidente, spécialement en comparant le modèle sous-jacent à l’analyse de variance. La source d’erreur est visible ainsi que les différences de moyennes. La force de cette deuxième méthode est qu’elle peut facilement être automatisée pour créer des jeux de données, alors que la première est plus compliquée. La seconde méthode nécessite six arguments (\\(n_k\\), \\(\\mu_0\\), \\(\\mu_{1:k}\\), et \\(\\sigma_{e}\\) en plus de catégories et définir le groupe référent), la première aurait de la difficulté à spécifier tous les arrangements de différence de moyennes automatiquement, quoiqu’elles sont plus personnalisables.","code":"\ndummy.coding <- function(x){\n  # Retourne un codage factice de x\n  # avec les facteurs en ordre alphabétique\n  W <- sapply(unique(x), \n              USE.NAMES = TRUE,\n              FUN = function(v) {x == v}) * 1 \n  return(W)\n}# Pour la reproductibilité\nset.seed(2602)\n\n# Facteur de groupe\nk <- length(categorie)\nnk <- 2\ngroupe <- rep(categorie, each = nk)\n\n# Groupement\nX <- dummy.coding(groupe)[,-5] # groupe 4 \"chien\" comme référent\n\n# Spécifications des paramètres\nmu0 <- 15\nmu <- c(0, 5, -5, 0)\ne <- rnorm(n = k * nk, sd = 4)\n\n# Création des scores\nscore <- round(mu0 + X %*% mu + e)\n\n# Comparaison des valeurs\ncbind(donnees, score)\n>       groupe score score\n> 1       char    14    14\n> 2       char    12    12\n> 3       chat    17    17\n> 4       chat    21    21\n> 5     cheval    14    14\n> 6     cheval    10    10\n> 7  chevalier    12    12\n> 8  chevalier    17    17\n> 9      chien    21    21\n> 10     chien    18    18"},{"path":"comparer.html","id":"analyse","chapter":" 13 Comparer","heading":"13.4 Analyse","text":"À toute fin pratique, un jeu de données est recréé avec les catégories et paramètres précédents, mais avec \\(n_k=20\\) unités par groupe. La fonction R de base est aov(). Elle prend comme argument une formule, de forme VD ~ VI (variable dépendante prédite par variable indépendante) et un jeu de données duquel prendre les variables. Il existe également une fonction anova(), une fonction un peu plus complexe que aov(). Pour obtenir toute l’information désirée de la sortie de la fonction, il faut demander un sommaire de la sortie avec summary().Les résultats sont identiques, les seules différences étant dues à l’arrondissement. Comme la valeur-\\(p\\) est de 5.145^{-9}, ce qui est extrêmement plus petit que l’usuel .05 (ou un autre taux d’erreur de type fixé à l’avance), l’hypothèse nulle est rejetée, il y vraisemblablement une différence entre les groupes, ce qui est déjà connu. La Figure 13.1 illustre très bien la rareté d’un tel jeu de données sous l’hypothèse nulle.\nFigure 13.1: Valeur-\\(F\\) de la comparaison des moyennes\n","code":"# Anova de base\nres <- aov(score ~ groupe, data = donnees)\nsummary(res)\n>             Df Sum Sq Mean Sq F value  Pr(>F)    \n> groupe       4    834   208.4      14 5.1e-09 ***\n> Residuals   95   1414    14.9                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Fonction maison\ngr <- table(donnees$groupe)\nk  <- length(gr)          # Nombre de groupes\nnk <- dim(donnees)[1] / k # Nombre de participants par groupe\n\n# Moyennes et variance par (`by()`) groupes\nmoyenne  <- by(donnees$score, donnees$groupe, mean)\nvariance <- by(donnees$score, donnees$groupe, var)\n\n# Statistiques\nCMI <- nk * var(moyenne)\nCMR <- mean(variance)\ndl1 <- k - 1\ndl2 <- k * (nk - 1)\nvf <- CMI / CMR \nvp <- (1 - pf(vf, df1 = dl1, df2 = dl2)) \n\n# Création du tableau\nresultats <- matrix(0,2,5) # Tableau vide\n\n# Ajouter noms\ncolnames(resultats)  <- c(\"dl\", \"SS\", \"CM\", \"F\", \"p\")\nrow.names(resultats) <- c(\"groupe\", \"residu\")\n\n# Ajouter valeurs\nresultats[1,] <- c(dl1, CMI * dl1, CMI, vf, vp)\nresultats[2,] <- c(dl2, CMR * dl2, CMR, 0, 0)\nresultats\n>        dl   SS    CM  F        p\n> groupe  4  834 208.4 14 5.15e-09\n> residu 95 1414  14.9  0 0.00e+00"},{"path":"comparer.html","id":"rapporter-lanova","chapter":" 13 Comparer","heading":"13.5 Rapporter l’ANOVA","text":"Voici comment commander une anova avec R. Il faut utilise la formule, VD ~ VI où la VD est la variable dépendante qui est d’échelle continue, la ou les VI, les variables indépendantes, contiennent les variables de groupement, mais aussi des variables continues, si besoin est.Voici comment rapporter l’ANOVA dans un article scientifique.Une ANOVA est réalisée pour comparer les groupes sur un certain score. Les résutats montrent une différence significative (pour un \\(\\alpha = .05\\)), \\(F(4, 95) = 13.998, p < .001\\).","code":"res <- aov(score ~ groupe, data = donnees)\nsummary(res)\n>             Df Sum Sq Mean Sq F value  Pr(>F)    \n> groupe       4    834   208.4      14 5.1e-09 ***\n> Residuals   95   1414    14.9                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"comparer.html","id":"la-sortie-de-aov","chapter":" 13 Comparer","heading":"13.5.1 La sortie de aov()","text":"Une petite digression pour parler de la sortie de aov(). Normalement, R produit [des listes][Créer des listes] comme sortie, particulièrement en ce qui trait aux analyses statistiques. Le cas de aov() est différent : le sommaire est une liste vide.Pour avoir accès directement au tableau de résultats du sommaire pour le manipuler, il faut ajouter [[1]] à la suite du code, ce qui permet d’atteindre les résultats. Les manipulations usuelles d’une liste sont maintenant réalisables.Le symbole [[1]] indique que seulement le premier élément de la liste est désiré. Il s’agit d’une formule renforcée des crochets pour ne choisir qu’un seul élément de la liste. Et dans cette liste, retrouve la liste contenant des résultats. En d’autres termes, la sortie de aov() est une liste. Il y bien deux niveaux de liste dans cette sortie, la première rendant plus difficile à la seconde. Une petit idiosyncrasie de R.","code":"sommaire <- summary(res)\n# Extraire les éléments de la sortie `sommaire`\nnames(sommaire)\n> NULLsommaire <- summary(res)[[1]]\nnames(sommaire)\n> [1] \"Df\"      \"Sum Sq\"  \"Mean Sq\" \"F value\" \"Pr(>F)\""},{"path":"comparer.html","id":"les-tests-posthoc","chapter":" 13 Comparer","heading":"13.6 Les tests posthoc","text":"Une fois que le test général montre une différence à laquelle l’expérimentateur juge pertinent de s’y pencher14, ce dernier désire généralement connaître d’où origine cette différence globale entre les groupes. Autrement, il cherche à identifier spécifiquement les groupes qui diffère des autres.La première étape est évidement de rouler l’ANOVA globale, voir la section précédente Rapporter l’ANOVA. En deuxième étape, une façon simple de demander les comparaisons posthoc est d’utiliser la fonction TukeyHSD(), soit la différence significative honnête, ou plus simplement la méthode de Tukey, en y introduisant comme argument la sortie de l’ANOVA.Dans cette sortie, chaque paire de groupes est indiquer à gauche, les différences et les intervalles de confiance sont produites. La dernière colonnes (p adj) contient la valeur-\\(p\\) indiquant si la paire est significativement différente ou non. Par exemple, la première ligne porte sur la comparaison chat-char et ces deux groupes sont vraisemblablement différents avec une valeur-\\(p\\) de 0.004. Il est possible de définir différents arguments, comme l’ordre des paires avec ordered = TRUE qui ordonne les moyennes des groupes de la plus petite à la plus grandes, ou encore le degré de confiance conf.level = 0.95 par défaut. Enfin, il est possible de sélectionner spécifiquement la variable de groupement avec l’argument .La fonction TukeyHSD() est aussi lié à une figure qu’il est possible de commander avec plot() en y mettant comme argument la sortie de TukeyHSD().\nFigure 13.2: Illustration du test de comparaisons multiples de Tukey\nMalheureusement la Figure 13.2 n’est pas parfaite, car elle n’affiche pas toutes les libellées de chaque ligne. Toutefois, chaque ligne de la Figure 13.2 correspond à une ligne de la figure. Avec le package ggplot2(voir [Visualier]), il est possible de remédier à la situation.D’abord, il faut importer le tidyverse. Ensuite, la prochaine étape consiste à extraire les résultats de TukeyHSD(), de les transformer en jeu de données avec .data.frame() et de rendre le nom des lignes manipulables avec add_rownames_to_column(). Comme pour toutes les figures, il faut indiquer l’abscisse et l’ordonnée, ici le nom des lignes (x) et la différence entre les groupes(y), respectivement. Ensuite, comme la figure avec les barres d’erreurs, il faut associer dans geom_errorbar les intervalles de confiance lwr et upr à ymin et ymax. Enfin, il est possible d’ajouter la moyenne sous forme de point (geom_point() et de tourner la figure avec coord_flip, ce qui permet de répliquer la figure originale.\nFigure 13.3: Illustration corrigée du test de comparaisons multiples de Tukey\nLa Figure 13.3 montre une illustration des comparaisons de Tukey plus adéquate.Une autre technique permet d’obtenir une table de comparaisons. Il s’agit de pairwise.t.test(). Cette fonction prend en argument deux vecteurs, la variable continue et la variable de groupement, et un troisième argument, soit la correction à appliquer avec l’argument p.adj.Cette technique à l’avantage d’être prête pour la plupart corrections, par exemple p.adj = \"bonferroni\" ou p.adj = \"holm\".","code":"TukeyHSD(res)\n>   Tukey multiple comparisons of means\n>     95% family-wise confidence level\n> \n> Fit: aov(formula = score ~ groupe, data = donnees)\n> \n> $groupe\n>                   diff     lwr   upr p adj\n> chat-char         4.45   1.057  7.84 0.004\n> cheval-char      -4.55  -7.943 -1.16 0.003\n> chevalier-char   -1.05  -4.443  2.34 0.910\n> chien-char       -1.00  -4.393  2.39 0.924\n> cheval-chat      -9.00 -12.393 -5.61 0.000\n> chevalier-chat   -5.50  -8.893 -2.11 0.000\n> chien-chat       -5.45  -8.843 -2.06 0.000\n> chevalier-cheval  3.50   0.107  6.89 0.040\n> chien-cheval      3.55   0.157  6.94 0.036\n> chien-chevalier   0.05  -3.343  3.44 1.000\nplot(TukeyHSD(res))\nTukeyHSD(res)[[1]] %>%   # Extraire les résultats\n  as.data.frame() %>%    # Transformer en jeu de données\n  rownames_to_column() %>%     # Ajouter les noms des lignes\n  ggplot(mapping = aes(x = rowname, \n                       y = diff)) + \n  geom_errorbar(aes(ymin = lwr,\n                    ymax = upr),\n                width = .25) + \n  geom_point() + \n  coord_flip()pairwise.t.test(donnees$score, donnees$groupe, p.adj = \"none\")\n> \n>   Pairwise comparisons using t tests with pooled SD \n> \n> data:  donnees$score and donnees$groupe \n> \n>           char  chat  cheval chevalier\n> chat      4e-04 -     -      -        \n> cheval    3e-04 6e-11 -      -        \n> chevalier 0.392 2e-05 0.005  -        \n> chien     0.415 2e-05 0.005  0.967    \n> \n> P value adjustment method: none"},{"path":"prédire.html","id":"prédire","chapter":" 14 Prédire","heading":" 14 Prédire","text":"Un objectif des expérimentateurs est généralement de développer des modèles afin de faire des prédictions à partir d’un échantillon. Les statistiques sont des outils idéaux pour développer ces modèles. La régression est l’une des analyses fondamentales pour réaliser cet objectif et constitue en quelques sortes les fondements de biens des méthodes récentes (comme l’apprentissage machine).L’objectif de la régression est de décrire la relation entre un variable dépendante et un ensemble de variables indépendantes. Un aperçu est donnée à la section sur l’association linéaire dans le chapitre Analyser. Ce présent chapitre débute avec la notion de covariance et l’étend jusqu’à celle de régression. Des techniques rudimentaires de création de données sont présentées. Par la suite, les mathématiques et la programmation sous-jacente au modèle linéaire sont illustrées.","code":""},{"path":"prédire.html","id":"retour-sur-lassociation-linéaire","chapter":" 14 Prédire","heading":"14.1 Retour sur l’association linéaire","text":"Une première méthode de mesure d’association est la covariance qui est représentée par l’équation (14.1).\\[\\begin{equation}\ns_{xy}=\\frac{1}{n-1}\\sum_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\tag{14.1}\n\\end{equation}\\]L’équation (14.1) représente la somme des produits des écarts à la moyenne de deux variables.Comment généraliser cette équation à un ensemble de plus de deux variables?Pour débuter, les variables sont mises sur un même pied d’égalité. Plutôt que de recourir à des lettres différentes (\\(x\\) et \\(y\\)), il faut toutes les considérées comme des \\(x_{,j}\\), où l’indice \\(\\) identifie le \\(\\)e participant, comme pour l’équation (14.1), et l’indice \\(j\\) indique la \\(j\\)e variable parmi \\(p\\). Le calcul pour chacune des paires de variables \\(j\\) et \\(k\\) est réalisé pour les \\(p\\) variables; l’équation de la covariance devient ainsi l’équation (14.2).\\[\\begin{equation}\ns_{x_j,x_k}=\\frac{1}{n-1}\\sum_{=1}^n(x_{,j}-\\bar{x_j})(x_{,k}-\\bar{x_k})\n\\tag{14.2}\n\\end{equation}\\]Si les variables sont centrées, l’équation (14.2) devient, pour faciliter l’intuition, l’équation (14.3), soit la somme des produits entre deux variables.\\[\\begin{equation}\ns_{x_j,x_k}=\\frac{1}{n-1}\\sum_{=1}^n(x_{,j})(x_{,k})\n\\tag{14.3}\n\\end{equation}\\]Avantageusement, lorsque \\(j=k\\), les équations (14.2) et (14.3) calculent la variance de la variable correspondante. En syntaxe R, ces équations s’écrivent dans une fonction comme la suivante. Pour rappel, la fonction cov() dans laquelle une matrice de données est passée comme argument fournit la matrice de covariance.Dans le code R ci-dessus, les fonctions ncol() et nrow() extraient le nombre de dimensions de la base de données, soit le nombre de variables et le nombre d’unités. La fonction scale() permet de centrer les variables de X et de les retourner dans Xc. Par défaut, l’argument pour centrer est center = TRUE, ce pourquoi l’argument n’est pas explicité, mais la fonction ne standardise pas les valeurs, car scale = FALSE. Une variable vide S est créé pour enregistrer les résultats. La boucle, quant à elle, calcule l’addition de l’équation (14.3) pour enfin diviser la somme par \\(n-1\\). Le résultat est S, la matrice de covariance.","code":"\ncovariance1 <- function(X){ \n  # X est un jeu de données\n  Xc <- scale(X, scale = FALSE)  # Centrées les variables\n  p  <- ncol(X)                  # Nombre de variables \n  n  <- nrow(X)                  # Nombre de sujets\n  S  <- matrix(0, p, p)          # Matrice vide pour enregistrer\n  \n  for(j in 1:p) {\n    for(k in 1:p) {\n      S[j, k] <- sum(Xc[ ,j] * Xc[ ,k])\n    }\n  }\n  \n  S <- S / (n-1)\n  \n  # S est la matrice de covariance\n  return(covariance = S)\n}"},{"path":"prédire.html","id":"illustration-de-la-covariance","chapter":" 14 Prédire","heading":"14.1.1 Illustration de la covariance","text":"Il est relativement aisé d’exprimer graphiquement la covariance bivariée. L’équation (14.3) rappelle l’aire d’un rectangle. Pour chaque paire \\((x_i,y_i)\\), un rectangle peut être dessiner à partir du centre \\((0, 0)\\) jusqu’au \\((x_i,y_i)\\). La Figure 14.1 montre quatre exemples de ces rectangles. Lorsque la moyenne d’une variable est soustraite, les données deviennent centrées sur le point \\((0, 0)\\). L’expression \\(xy\\) ou \\(x_ix_j\\) rappelle le calcul de l’aire d’un rectangle. C’est effectivement ce qui se produit pour la covariance. L’équation calcule l’aire du rectangle formé par les points \\((0,0)\\) et \\((x_i,y_i)\\). En fait, l’équation (14.1) calcule le rectangle moyen dont la somme des produits est divisée par le nombre de rectangles moins 1, soit \\((n-1)\\).\nFigure 14.1: Illustration de la covariance\n\nFigure 14.2: Illustration des produits (rectangles) pour différentes valeurs de covariance\nLes Figures 14.1 et 14.2 permettent d’inférer quelques propriétés de la covariance.Comme la quantité de surface blanche (ou de noire) dépend de la taille de la figure, la covariance est directement proportionnelle aux échelles à l’abscisse et l’ordonnée.Comme la quantité de surface blanche (ou de noire) dépend de la taille de la figure, la covariance est directement proportionnelle aux échelles à l’abscisse et l’ordonnée.La covariance augmente lorsque les points s’approchent d’une ligne à pente ascendante et diminue lorsque les points s’approchent d’une ligne à pente descendante.La covariance augmente lorsque les points s’approchent d’une ligne à pente ascendante et diminue lorsque les points s’approchent d’une ligne à pente descendante.Comme les associations non linéaires peuvent créer des amalgames de rectangles positifs et négatifs, elles conduisent à des covariances imprévisibles et peu pertinentes.Comme les associations non linéaires peuvent créer des amalgames de rectangles positifs et négatifs, elles conduisent à des covariances imprévisibles et peu pertinentes.La covariance (et la corrélation) est sensible aux valeurs aberrantes. Un point éloigné de la masse créera une aire rectangulaire bien plus grande que les autres points. À lui seul, il peut créer une quantité substantielle positive ou négative de surface blanche (ou noire) dans la figure.La covariance (et la corrélation) est sensible aux valeurs aberrantes. Un point éloigné de la masse créera une aire rectangulaire bien plus grande que les autres points. À lui seul, il peut créer une quantité substantielle positive ou négative de surface blanche (ou noire) dans la figure.Si une variable est multipliée par elle-même, il s’agit de l’aire d’un carré, ce qui équivaut au calcul d’une variance. Par extension, si le produit de deux variables se rapproche davantage d’un carré que d’un rectangle (en moyenne), alors les deux variables sont fortement liées.Si une variable est multipliée par elle-même, il s’agit de l’aire d’un carré, ce qui équivaut au calcul d’une variance. Par extension, si le produit de deux variables se rapproche davantage d’un carré que d’un rectangle (en moyenne), alors les deux variables sont fortement liées.Le paramètre de la corrélation de la population peut être conceptualisé comme un carré déformé en rectangle à cause de l’erreur de mesure des axes.Le paramètre de la corrélation de la population peut être conceptualisé comme un carré déformé en rectangle à cause de l’erreur de mesure des axes.","code":""},{"path":"prédire.html","id":"la-covariance-en-termes-dalgèbre-matricielle","chapter":" 14 Prédire","heading":"14.1.2 La covariance en termes d’algèbre matricielle","text":"L’équation de la covariance peut aussi se calculer en termes d’algèbre matricielle. En plus d’accélérer le calcul des résultats, il simplifie énormément les mathématiques sous-jacente (pourvu que l’utilisateur connaisse l’algèbre matricielle).Une matrice est un ensemble de variables représenté sous une seule variable mathématique. Dans les équations mathématiques, une matrice est désignée par une lettre majuscule en gras: \\(x\\) devient \\(\\mathbf{X}\\) et \\(\\sigma\\) devient \\(\\mathbf{\\Sigma}\\). Dans une matrice, chaque colonne est une variable, chaque ligne correspond à un sujet différent mesuré sur toutes les variables. Une matrice est définie en partie par son nombre de lignes (\\(n\\), nombre d’unités) et son nombre de colonnes (\\(p\\), nombre de variables), dont voici une illustration à l’équation (14.4).\\[\\begin{equation}\n\\mathbf{X} = \\left(\\begin{array}{cccc}\nx_{1,1} & x_{1,2} & ...&x_{1,p}\\\\\nx_{2,1} & x_{2,2} & ...&x_{2,p}\\\\\n... & ...& ...& ... \\\\\nx_{n,1} & x_{n,2} & ... &x_{n,p}\\\\\n\\end{array}\\right)\n\\tag{14.4}\n\\end{equation}\\]En syntaxe R, il ne s’agit rien de plus que de concaténer des variables (mesurant les mêmes individus) ensembles par des colonnes, comme il est fait avec des jeux de données data.frame(). Un jeu de données est, à peu de chose près, une matrice (voir Concaténer). Pour créer une matrice X à partir des variables x, y, et z avec R, par exemple, la ligne X = cbind(x, y, z) joindra les trois variables ensemble. La fonction cbind() et une fonction pour concaténer des colonnes. Il existe aussi rbind() pour les lignes.Pour réaliser le calcul de la covariance, il faut multiplier la matrice des données centrées (les variables concaténées) par elle-même puis diviser par \\(n-1\\). Le symbole \\(X\\) représente la concaténation des variables. Par simplicité, l’équation utilise des variables centrées.\\[\\begin{equation}\n\\mathbf{S} = (n-1)^{-1}\\mathbf{X}^\\prime \\mathbf{X}\n\\tag{14.5}\n\\end{equation}\\]Le symbole \\(\\mathbf{S}\\) représente la matrice de variance-covariance. La diagonale de cette matrice représente les variances des données et les éléments hors diagonales sont les covariances, soulignant le lien entre la variance et la covariance. Le symbole \\(\\prime\\) (prime) correspond à l’opération de transposer une matrice, soit d’échanger les lignes par ces colonnes. Cette procédure est nécessaire pour produire la multiplication d’une matrice par elle-même. Noter que l’expression \\((n-1)^{-1}=\\frac{1}{n-1}\\).\\[\\begin{equation}\n\\begin{aligned}\n\\mathbf{S} & = (n-1)^{-1}\n\\left(\\begin{array}{cccc}\nx_{1,1} & x_{2,1} & ... & x_{n,1} \\\\\nx_{1,2} & x_{2,2} & ... & x_{n,2} \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{cc}\nx_{1,1} & x_{1,2} \\\\\nx_{2,1} & x_{2,2}\\\\\n... & ...  \\\\\nx_{n,1} & x_{n,2} \\\\\n\\end{array}\\right) \\\\\n& = (n-1)^{-1}\n\\left(\\begin{array}{cc}\n\\sum_{=1}^n(x_{,1})(x_{,1}) & \\sum_{=1}^n(x_{,1})(x_{,2})\\\\\n\\sum_{=1}^n(x_{,2})(x_{,1}) & \\sum_{=1}^n(x_{,2})(x_{,2})\n\\end{array}\\right)\n\\end{aligned}\n\\tag{14.6}\n\\end{equation}\\]L’équation (14.6) illustre l’équation (14.5) qui sont toutes les deux équivalentes à (14.3). En termes de syntaxe R, elles peuvent être traduites comme suit.La fonction t() opère la transpose (représentée par \\(\\prime\\)) et le symbole %*% signifie le produit matriciel des variables. Il faut bien distinguer * et %*%. L’usuel symbole de multiplication * de R opère une multiplication cellule par cellule (avec recyclage) plutôt que l’opération matricielle (multiplication des colonnes de la première matrice avec les lignes de la seconde matrice).L’utilisation de l’algèbre matricielle est plus simple et efficace. Elle nécessite cinq lignes de code, élimine deux boucles, prend moins de temps à calculer, en plus de produire toutes les variances et covariances.Une matrice de covariance possède plusieurs propriétés importantes. Elle est toujours carrée soit \\(p \\times p\\) pour \\(p\\) variables et contient \\(p^2\\) éléments. Parmi ces éléments, les \\(p\\) éléments de la diagonale sont des variances, ce pourquoi elle est parfois appelée matrice de variance-covariance. Les éléments triangulaires inférieurs hors diagonale sont un parfait reflet des éléments supérieurs, p.ex. \\(\\sigma_{1,2} = \\sigma_{2,1}\\). Il y ainsi \\(\\frac{p(p-1)}{2}\\) covariances uniques dans une matrice et \\(p(p+1)/2\\) éléments uniques (variances et covariances). En plus, de ces caractéristiques, la matrice doit être positive semi-définie, ce qui est un terme mathématique impliquant, pour les fins de ce chapitre, que les variances ne peuvent être nulle15. La Figure (14.7) illustre la matrice de covariance \\(\\mathbf{\\Sigma}\\) de la population.\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\n\\begin{array}{cccc}\n\\sigma_{1,1}^2 & \\sigma_{1,2} & ... &  \\sigma_{1,p}\\\\\n\\sigma_{2,1} & \\sigma_{2,2}^2 & ... &  \\sigma_{2,p}\\\\\n...& ...& ... &  \\sigma_{3,p}\\\\\n\\sigma_{p,1} & \\sigma_{p,2} & ... &  \\sigma_{p,p}^2\\\\\n\\end{array}\n\\right)\n\\tag{14.7}\n\\end{equation}\\]","code":"\ncovariance2 = function(X){\n  # X est une jeu de données ou matrice de n sujets par p variables\n  n <- nrow(X)\n  Xc <- scale(X, scale = FALSE)\n  \n  # L'algèbre matriciel permet de calculer le\n  # produit d'une colonne avec les autres colonnes\n  cov.X <- t(Xc) %*% Xc / (n - 1)\n  return(cov.X)\n}"},{"path":"prédire.html","id":"la-matrice-de-corrélation-en-termes-dalgèbre-matricielle","chapter":" 14 Prédire","heading":"14.1.3 La matrice de corrélation en termes d’algèbre matricielle","text":"Une matrice de corrélation est une matrice de covariance dont les variables sont standardisées. Cela implique de transformer la matrice afin que toute la diagonale soit à l’unité, c’est-à-dire que toutes les variances soient égalent à 1. Cela permet d’avoir une interprétation standardisée des corrélations, car celles-ci sont indépendantes des métriques originales. Les autres éléments conceptuels de la matrice de covariance s’appliquent pour la matrice de corrélation.Pour transformer la matrice de covariance en matrice de corrélation, trois techniques sont possibles.La première est de standardiser \\(\\mathbf{X}\\) préalablement au calcul de la covariance.La deuxième méthode est de standardiser la matrice de covariance en utilisant l’algèbre matricielle, où \\(\\mathbf{S}\\) est la matrice de covariance. Pour ce faire, pré et post multiplie \\(\\mathbf{S}\\) par \\(\\mathbf{D}\\), comme l’équation (14.8).\\[\\begin{equation}\n\\mathbf{R}=\\mathbf{D_{S}}\\mathbf{S}\\mathbf{D_{S}}\n\\tag{14.8}\n\\end{equation}\\]Pour obtenir \\(\\mathbf{D_S}\\), il faut :extraire de la matrice les variances de la diagonale (avec la fonction mathématique \\(\\text{diag}\\));extraire de la matrice les variances de la diagonale (avec la fonction mathématique \\(\\text{diag}\\));faire la racine carrée pour obtenir des écarts types;faire la racine carrée pour obtenir des écarts types;recréer une matrice carrée avec la fonction mathématique \\(\\text{diag}\\), et;recréer une matrice carrée avec la fonction mathématique \\(\\text{diag}\\), et;finalement inverser la matrice.finalement inverser la matrice.Autrement dit, \\(\\mathbf{D_S}\\) correspond à l’équation (14.9).\\[\\begin{equation}\n\\mathbf{D}_{\\mathbf{S}} = (\\text{diag}(\\sqrt{\\text{diag}(\\mathbf{S})})^{-1})\n\\tag{14.9}\n\\end{equation}\\]Le calcul complet de l’équation (14.8) en ajoutant (14.9) est représenté par l’équation (14.10).\\[\\begin{equation}\n\\mathbf{R} = (\\text{diag}(\\sqrt{\\text{diag}(\\mathbf{S})})^{-1}) \\mathbf{S} (\\text{diag}(\\sqrt{\\text{diag}(\\mathbf{S})})^{-1})\n\\tag{14.10}\n\\end{equation}\\]En code R, l’équation (14.10) se traduit ainsi.Lorsqu’une matrice est passée comme argument à diag(), elle extrait les éléments de la diagonale pour en faire un vecteur. Si un vecteur est passé en argument, alors diag() retourne une matrice avec les éléments du vecteur en diagonale. La fonction solve() calcule l’inverse d’une matrice, comme l’utilisateur pourrait attendre de \\(\\mathbf{X}^{-1}\\). Il s’agit certainement de l’aspect le plus biscornu de R. Enfin, l’opérateur %*% est le produit matriciel.La troisième méthode est d’utiliser la fonction de base cov2cor() pour transformer la matrice de covariance en matrice de corrélation, ce qui est plus simple et plus rapide que la deuxième option, mais qui cache ce qui se réalise vraiment, soit l’équation (14.10).","code":"\nXz <- scale(X)\nn <- nrow(x)\ncor.X <- t(Xz) %*% Xz / (n - 1)\nR = solve(diag(sqrt(diag(S)))) %*% S %*% solve(diag(sqrt(diag(S))))"},{"path":"prédire.html","id":"la-régression","chapter":" 14 Prédire","heading":"14.2 La régression","text":"Jusqu’à présent, des rudiments de la covariance et de la corrélation ont été présentés dans le d’introduire graduellement l’algèbre matricielle. L’objectif étant atteint, le regard porte maintenant sur la régression en tant que moyen de prédire une variable \\(y\\) à partir d’un ensemble de variables \\(\\mathbf{X}\\).Quelle est la différence entre les analyses de covariance et corrélation comparativement à l’analyse de régression? Bien que la logique sous-jacente soit très similaire, il faut maintenant déterminer une variable différente de l’autre, c’est-à-dire distinguer une variable dépendante, des variables indépendantes. Les variables indépendantes prédisent la variable dépendante un peu comme dans un modèle déterministe, il faut décider de la cause (variables indépendantes) et l’effet (variable dépendante), ce dernier étant généré par les premiers.Plutôt que de traiter chaque paire de variable comme c’est le cas avec l’analyse de corrélation, la régression étudie la relation entre la variable dépendante et toutes les variables indépendantes, et ce, simultanément. Ainsi, les variables indépendantes sont contrôlées entre elles pour évaluer leur effet sur la variable dépendante et un modèle de prédiction plus juste est obtenu (autant que faire ce peut avec un modèle statistique).","code":""},{"path":"prédire.html","id":"le-modèle-de-régression-simple","chapter":" 14 Prédire","heading":"14.2.1 Le modèle de régression simple","text":"L’équation pour un modèle de régression simple se résume au cas bivarié, soit la prédiction de \\(y\\) par une seule variable \\(x\\).\\[\\begin{equation}\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\tag{14.11}\n\\end{equation}\\]Dans ce modèle, \\(y\\) est la variable prédite, \\(x\\) est le seul prédicteur, \\(\\beta_0\\) est l’ordonnée à l’origine, \\(\\beta_1\\) est le coefficient de régression et \\(\\epsilon\\), l’erreur dans la variable \\(y\\) indépendante (non corrélée, ni fonctionnellement liée) de \\(x\\).Il vaut la peine de se pencher un peu plus sur ces paramètres. L’ordonnée \\(\\beta_0\\) est la moyenne de \\(y\\) lorsque \\(x=0\\), autrement dit, contrôlée par la variable indépendante. Si \\(x\\) est centrée, alors \\(\\beta_0\\) égale la moyenne de \\(y\\); si \\(x\\) et \\(y\\) sont centrées, alors \\(\\beta_0=0\\). La pente \\(\\beta_1\\) est le coefficient de régression, le degré selon lequel \\(x\\) prédit \\(y\\). Ce paramètre dépend de la corrélation entre ces deux variables. En fait, si \\(x\\) et \\(y\\) sont standardisées, alors \\(\\beta_1\\) correspond à la corrélation entre les deux16. Le paramètre \\(\\epsilon\\) est l’erreur du modèle. Par définition, l’erreur est normalement distribuée et sa moyenne est de \\(0\\). Seule l’erreur standard (l’écart type de l’erreur) change.La Figure 14.3 illustre ces paramètres. L’ordonnée \\(\\beta_0\\) est représentée en pointillé du centre \\((0,0)\\) jusqu’à la valeur de \\(y\\) lorsque \\(x = 0\\). La pente \\(\\beta_1\\) correspond à l’inclinaison de la pente (droite oblique pointillé). La droite oblique pointillée représente la prédiction de \\(y\\), soit \\(\\hat{y}\\) par \\(x\\). qui correspond à l’équation (14.11), mais en excluant \\(\\epsilon\\).\\[\\begin{equation}\n\\hat{y_i} = \\beta_0 + \\beta_1 x_i\n\\tag{14.12}\n\\end{equation}\\]L’équation (14.12) exprime toute la ligne pointillée, soit la meilleure prédiction possible. Celle-ci n’est pas parfaite et contient des erreurs, des résidus, autrement dit, ce qui n’est pas prédit par \\(x\\). Les courtes lignes verticales grises représentent les résidus des participants, soit \\(\\epsilon_i\\), la part de \\(y\\) qui n’est pas prédite par \\(x\\) pour la participant \\(\\). Les résidus se distribuent normalement avec une moyenne de \\(0\\) et un écart type de \\(\\sigma_{\\epsilon}\\). Plus \\(\\sigma_{\\epsilon}\\) est petit (tend vers 0), plus les points seront près de la droite.Une façon de connaître la qualité de la droite est le coefficient de détermination. Ce coefficient correspond à l’équation (14.13).\\[\\begin{equation}\nR^2 = 1-\\frac{\\sigma^2_{\\epsilon}}{\\sigma^2_y}\n\\tag{14.13}\n\\end{equation}\\]Le ratio \\(\\frac{\\sigma^2_{\\epsilon}}{\\sigma^2_y}\\) représente la proportion de variance de \\(y\\) non expliquée par les variables indépendantes. Cette proportion s’étend de 0 à 1. La différence \\(1-\\frac{\\sigma^2_{\\epsilon}}{\\sigma^2_y}\\) donne l’inverse de cette proportion… la variance expliquée! Allant de 0 à 1, le coefficient de détermination indique la qualité de la prédiction, 1 étant une relation parfaite.Pour aller un peu plus loin, dans un modèle bivarié, le coefficient de détermination correspond au coefficient de corrélation au carré, \\(r^2\\), la force du lien entre \\(x\\) et \\(y\\). La valeur \\(1-r^2\\) correspond donc la variance des résidus standardisés, \\(\\sigma^2_{\\epsilon}\\).\nFigure 14.3: Diagramme de dispersion\n","code":""},{"path":"prédire.html","id":"lanalyse-de-régression","chapter":" 14 Prédire","heading":"14.2.2 L’analyse de régression","text":"Lorsque le jeu de données est obtenu, il est temps de procéder à l’analyse de régression. Essentiellement, l’analyse de régression produit à peu près ceci en algèbre matricielle.Pour aider la compréhension, voici une explication avec le modèle linéaire simple assumant des variables centrées. Le modèle correspond à\\[\ny = \\beta x\n\\]où l’erreur, \\(\\epsilon\\) n’est pas explicitée. Il faut isoler \\(\\beta\\) afin de l’estimer, soit l’opération suivante,\\[\n\\beta = \\mathbb{E}(\\frac{y}{x})\n\\]où le symbole \\(\\mathbb{E}\\) signifie l’espérance (la moyenne). En multipliant par \\(\\frac{x}{x}\\) de chaque côté de l’équation, cela produit l’équation suivante.\\[\\begin{equation}\n\\beta = \\mathbb{E}(\\frac{xy}{xx}) = \\frac{\\sigma_{xy}}{\\sigma^2_x}\n\\tag{14.14}\n\\end{equation}\\]Dans l’équation (14.14), le numérateur rappelle la covariance et au dénominateur la variance de \\(x\\). Comment généraliser pour \\(k\\) variables? En algèbre matricielle et dans la mesure où les variables contenues dans \\(\\mathbf{X}\\) sont centrées, cela revient au même que de calculer l’équation (14.15).\\[\\begin{equation}\n\\hat{\\mathbf{B}} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} y\n\\tag{14.15}\n\\end{equation}\\]Comme pour l’équation (14.14), la composante \\((X^{\\prime} X)^{-1}\\) agit en dénominateur (par l’exposant \\(-1\\)) et correspond à la matrice de variance-covariance des variables de \\(X\\) ensemble, alors que \\(X^{\\prime} y\\) agit comme le numérateur, soit la covariance entre les variables de \\(X\\) avec \\(y\\).Pour l’erreur standard, il s’agit de calculer l’équation (14.16).\\[\\begin{equation}\n\\text{var}(\\hat{B}) = \\sigma^2 \\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\n\\tag{14.16}\n\\end{equation}\\]La racine carrée de ce résultat donne l’erreur standard (standard error) ou erreur type.Le ratio \\(\\frac{B}{\\text{se}_B} \\sim t_{n-p-1}\\), soit le quotient d’un estimateur par son erreur type, suit une distribution-\\(t\\) avec \\(n-p-1\\) degrés de liberté.Quelques détails sont importants à considérer pour la programmation. Afin d’ajouter l’intercepte (pour estimer \\(\\beta_0\\)), la solution la plus simple est d’ajouter un vecteur d’unité (un vecteur ne contenant que des \\(1\\)) à la matrice \\(\\mathbf{X}\\). En programmation R, l’inversion de matrice se fait par la fonction solve() et non pas avec un signe d’exposant. En syntaxe R, la régression s’écrit comme ceci.Le modèle linéaire peut aussi contenir des variables nominales dans la mesure où celle-ci sont transformées en données factices (dummy coding). En fait, une analyse de variance n’est au fond qu’une régression dans laquelle les variables nominales sont transformées en données factices, puis utilisées en variable indépendante. Les \\(\\beta\\) de la régression correspondent aux moyennes \\(\\mu\\), si le participant appartient (1) à tel ou tel autre groupe ou (0) autrement. Bien que les logiciels produisent souvent des sorties statistiques différentes en fonction de l’analyse demandée, les mathématiques sous-jacentes sont les mêmes.","code":"\nregression <- function(y, X){\n  # Taille d'échantillon \n  n <- nrow(X)\n  \n  # Centrer la matrice\n  X <- scale(X, scale = FALSE)\n\n  \n  # Joindre un intercepte\n  X <- cbind(intercept = 1, X)\n  X <- as.matrix(X)\n  \n  # Nombre de variables\n  p <- ncol(X)\n  \n  # Calculer les coefficients de régression\n  B <- solve(t(X) %*% X) %*% t(X) %*% y\n  \n  #Calculer la variance résiduelle\n  var.e <- sum((y - X %*% B)^2) / (n-p)\n  \n  # Calculer les erreurs standards des beta\n  se.B <- sqrt(c(var.e) * diag(solve(t(X) %*% X)))\n  \n  # Valeurs-t\n  vt <- B / se.B\n  \n  # Valeurs-p\n  vp <- pt(abs(vt), df = n - p - 1, lower.tail = FALSE) * 2\n  \n  # Création d'un tableau de sortie\n  resultats <- data.frame(Estimate = B, \n                          Std.Error = se.B, \n                          t.value = vt, \n                          p.value = vp) \n  return(resultats)\n}  "},{"path":"prédire.html","id":"la-création-de-données-1","chapter":" 14 Prédire","heading":"14.2.3 La création de données","text":"Une façon simple et efficace de créer des données à ce stade est la package MASS dont un aperçu été donné dans le chapitre Analyser.La matrice de covariance, \\(\\mathbf{\\Sigma}\\), pour \\(p=3\\), s’écrit comme l’équation (14.17).\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\n\\begin{array}{ccc}\n\\sigma_{1,1}^2 & \\sigma_{1,2} & \\sigma_{1,3}\\\\\n\\sigma_{2,1} & \\sigma_{2,2}^2 & \\sigma_{2,3}\\\\\n\\sigma_{3,1} & \\sigma_{3,2} & \\sigma_{2,3}^2\n\\end{array}\n\\right)\n\\tag{14.17}\n\\end{equation}\\]Il convient d’écrire \\(\\mathbf{\\Sigma}\\) (sigma majuscule) et \\(\\sigma\\) (sigma minuscule) plutôt que \\(\\mathbf{S}\\), car il s’agit de la matrice de covariance de la population. Le résultat de S = cov(donnees) est empirique et la notation \\(\\mathbf{S}\\) est plus appropriée. Comme il y \\(p=3\\) variables dans la syntaxe, il faudra préalablement spécifier \\(3*4/2 = 6\\) arguments : \\(p = 3\\) variances \\(\\sigma_{1,1},\\sigma_{2,2},\\sigma_{3,3}\\) et \\(\\frac{p(p-1)}{2}=3(2)/2 = 3\\) covariances \\(\\sigma_{1,2},\\sigma_{1,3},\\sigma_{2,3}\\).Une autre façon de créer des données en fonction d’un modèle linéaire plutôt qu’à partir de la matrice de corrélation (comme avec MASS) est de reprendre l’équation (14.11) et de spécifier les paramètres libre. D’abord, il faut remplacer les paramètres du modèle par des valeurs, \\(\\beta_0\\) et \\(\\beta_1\\), pour ensuite créer deux variables aléatoires de taille \\(n\\) (la taille d’échantillon), une première pour \\(x\\) et une seconde pour \\(\\epsilon\\). Les hypothèses sous-jacentes aux modèles linéaires assument que l’erreur (\\(\\epsilon\\)) est distribuée normalement (avec implicitement une moyenne de 0), la fonction rnorm() pourra jouer le rôle. Pour \\(x\\), il n’y pas de distribution à respecter, mais une distribution normale fait très bien l’affaire. Voici un exemple de code R. En spécifiant une taille d’échantillon très grande n = 10000, l’erreur échantillonnalle est considérablement réduite.Si l’utilisateur souhaite ajouter une autre variable, il lui suffit d’ajouter un \\(\\beta\\) supplémentaire et de créer une autre variable aléatoire.Cette méthode de création de données possède toutefois des limites. Principalement, elle ne spécifie pas les propriétés statistiques désirables, comme la corrélation entre les variables. Quelle est la corrélation entre xet y dans l’exemple précédent? Il est bien sûr possible de déterminer ces valeurs pour la population posteriori. Il faut résoudre l’équation (14.18).\\[\\begin{equation}\n\\rho_{x,y} = \\beta_1 \\frac{\\sigma_x}{\\sigma_y}\n\\tag{14.18}\n\\end{equation}\\]Certaines valeurs sont déjà connues, car préspécifiées par l’utilisateur, \\(\\beta_1 = 1\\) et \\(\\sigma_x = 1\\). Qu’en est-il de \\(\\sigma_y\\)? L’utilisateur n’pas spécifié la valeur de la variance de \\(y\\), il plutôt choisi la valeur de la variance de l’erreur résiduelle, \\(\\sigma^2_{\\epsilon}\\). La loi de la somme des variances permet de calculer cette valeur. Pour le lecteur intéressé, les réponses sont \\(\\sigma^2_y = \\beta_1^2\\sigma^2_x+\\sigma^2_{\\epsilon} = 10\\) et donc \\(\\rho_{x,y} = \\frac{\\beta_1 \\sigma_x}{\\sigma_y} = 0.316\\) (les détails sont présentés dans le chapitre Créer).La limite liée à la méthode de création de données est maintenant flagrante. En plus de ne pas connaître la corrélation entre les variables, la variance de \\(y\\) n’est pas connue priori. La stratégie de spécification est ainsi de choisir des valeurs et d’espérer qu’elles soient conformes aux attentes. Pire, s’il y avait plusieurs variables indépendantes, elles seraient toutes non corrélées entre elles, alors que l’utilisateur pourrait vouloir autrement, mais cette première technique ne le permet pas.Pour l’utilisateur qui crée son propre jeu de données, ces caractéristiques sont souvent plus essentielles que de spécifier à l’avance la variance résiduelle. Pour résoudre cette situation, la solution est de spécifier un modèle standardisé, puis de le déstandardiser (ajouté des moyennes et des variances posteriori).La philosophie de modélisation de cet ouvrage repose sur l’idée selon laquelle, un modèle doit être standardisé au départ puis déstandardisé. Cette logique ne se prêtera pas à tous les contextes. Pour certains la difficulté sera immense; pour d’autres, cela ne respectera pas les objectifs. En partant d’un modèle standardisé toutefois, la matrice de corrélation est connue à l’avance et la variance est spécifiée directement par l’utilisateur. Les tailles d’effets attendues sont également assurées. Il suffit de dériver la variance résiduelle du modèle plutôt que de la spécifier.En assumant un modèle linéaire,\\[\\begin{equation}\ny = \\beta_0 + \\beta_1 x_1 + ... +\\beta_k x_k + \\epsilon\n\\tag{14.19}\n\\end{equation}\\]où l’équation (14.19) correspond à la généralisation de l’équation (14.11) pour \\(k\\) variables indépendantes, il est possible d’isoler \\(\\epsilon\\). La variance se calcule alors comme l’équation (14.20), pour le cas générale.\\[\\begin{equation}\n\\sigma^2_{\\epsilon} = \\sigma^2_y - \\mathbf{B}^{\\prime}\\mathbf{R}\\mathbf{B}\n\\tag{14.20}\n\\end{equation}\\]où \\(\\mathbf{R}\\) est la matrice de corrélation et \\(\\mathbf{B}\\) est le vecteur contenant tous les \\(\\beta\\) standardisés. Pour assurer un scénario standardisé \\(\\sigma^2_y = 1\\). La seule condition sous-jacente à l’équation (14.20) est de s’assurer que \\(\\sigma^2_{\\epsilon} > 0\\), c’est-à-dire en vérifiant que \\(\\mathbf{B}^{\\prime}\\mathbf{R}\\mathbf{B} < \\sigma^2_y\\), autrement la variance est négative, ce qui est impossible. En termes de syntaxe R, l’équation (14.20) correspond à ceci.L’avantage de cette technique est () de pouvoir spécifier les corrélations entre les variables indépendantes avec la matrice \\(\\mathbf{R}\\); (b) de déterminer à l’avance la variance de \\(y\\) et (c) que le vecteur \\(\\mathbf{B}\\) contient les \\(\\beta\\) standardisés qui sont dans ce contexte les corrélations partielles qui relient chacune des variables indépendantes à la variable dépendante (des tailles d’effet) en contrôlant pour chacune d’elles.L’utilisateur crée par la suite les données en spécifiant le vecteur \\(\\mathbf{B}\\) et en créant une variable basée sur la matrice de corrélation. Voici un exemple pour \\(k=3\\) variables centrées suivant une distribution normale multivariée avec la matrice de corrélation \\(\\mathbf{R}\\).\\[\n\\mathbf{R} = \\left(\n\\begin{array}{ccc}\n1 & .2 & .3\\\\\n.2 & 1 & .1\\\\\n.3 & .1 & 1\n\\end{array}\n\\right)\n\\]\nUne fois les données de \\(\\mathbf{X}\\) créées, avec la fonction MASS::mvrnorm(), comme il été fait précédemment, il suffit de multiplier \\(\\mathbf{X}\\) avec \\(\\mathbf{B}\\) et d’ajouter la variable aléatoire \\(\\epsilon\\) avec la variance appropriée pour obtenir la variable dépendante \\(y\\).Maintenant, il est possible de déstandardisé X et y en additionnant des moyennes ou multipliant par des écarts types à chaque variable.","code":"\n# Création de la matrice de covariance pour p = 3\np <- 3   # Nombre de variables\nSigma <- matrix(c(s11, s12, s13,\n                  s12, s22, s23,\n                  s13, s23, s33), \n                nrow = p, ncol = p)\n\n# Création des données \ndonnees <- data.frame(MASS::mvrnorm(n = n, \n                                    mu = rep(0, p), \n                                    Sigma = Sigma))\nn <- 10000 # Taille d'échantillon\n# Les betas\nbeta0 <- 5\nbeta1 <- 1\n\n# Deux variables aléatoires tirées de distributions normales.\n# Les moyennes sont nulles et \n# les écarts types sont spécifiés\nx <- rnorm(n = n, sd = 1)\ne <- rnorm(n = n, sd = 3)\n\n# Création de la variable dépendante\ny <- beta0 + beta1 * x + e\n# Calculer la variance de epsilon\nvar_e <-  var_y - t(B) %*% R %*% Bset.seed(42)  # Pour reproductibilité\nn <- 1000      # Taille d'échantillon\nk <- 3         # Nombre de variables indépendantes\n\n# Matrice de corrélation\nR <- matrix(c(1, .2, .3,\n             .2, 1, .1,\n             .3, .1, 1), k, k)\n# Moyennes\nmu <- rep(0, k)\n\n# Choix des betas standardisés\nB <- c(beta1 = .2, beta2 = -.5, beta3 = .3)\n\n# Variance de epsilon\nvar_e <- 1 - t(B) %*% R %*% B\n\n# Créations des variables aléatoires\nX <- MASS::mvrnorm(n = n, mu = mu, Sigma = R)\ne <- rnorm(n = n, sd = sqrt(var_e))\n\n# Création de la variable dépendante\ny <- X %*% B + e\n\n# Création du jeu de données\njd <- data.frame(y = y, X = X)\n\n# Quelques vérifications\n# Les données\nhead(jd)\n>        y    X.1    X.2     X.3\n> 1  0.635 -0.956 -2.567  0.3239\n> 2 -0.264  0.672 -0.172  0.5200\n> 3  0.341  0.885 -1.369 -0.6387\n> 4 -0.602  0.778 -1.104 -1.2678\n> 5  0.401  0.360  0.286 -1.4336\n> 6 -1.076 -0.210  0.621 -0.0399\n\n# La matrice de corrélation entre les variables indépendantes\n# Très près des valeurs choisies à la troisième décimale\ncor(X)\n>       [,1]  [,2]  [,3]\n> [1,] 1.000 0.188 0.279\n> [2,] 0.188 1.000 0.136\n> [3,] 0.279 0.136 1.000\n\n# La variance de y (encore une fois très près)\nvar(y)\n>      [,1]\n> [1,] 1.03"},{"path":"prédire.html","id":"les-conditions-dapplication-de-la-régression","chapter":" 14 Prédire","heading":"14.2.4 Les conditions d’application de la régression","text":"La régression possède quatre hypothèses sous-jacentes :La vraie relation est linéaire;La vraie relation est linéaire;Les observations sont indépendantes.Les observations sont indépendantes.Les résidus sont normalement distribués;Les résidus sont normalement distribués;La variance résiduelle est homoscédastique.La variance résiduelle est homoscédastique.Les deux premiers points sont davantage méthodologiques que statistiques bien que leurs conséquences soient réelles. La relation entre les variables doit être linéaire. La régression tient compte des relations en ligne droite, si la relation entre deux variables suit une courbe, elle ne pourra être adéquatement analysée. La Figure 14.4 montre trois relations possibles entre deux variables. Bien que la relation soit très forte, peu importe la forme de la relation (les données suivent un parton très évident), seule celle au centre (relation linéaire) donne un résultat indiquant un lien fort. Il existe des techniques de transformation de données lorsqu’elles sont théoriquement justifiées. La relation quantitative entre l’âge et la quantité de rapport sexuel est un exemple (caricatural) de relation non linéaire : elle débute à l’adolescence, atteint son apogée à l’âge de jeune adulte, puis décroît progressivement. La seconde hypothèse est que les unités d’observation doivent être indépendantes. Techniquement, chaque unité devrait avoir une chance égale et indépendante d’être choisie. La régression est robuste à ce genre de violation, mais il faut toujours conserver cette idée en tête lorsque le devis d’étude est conceptualisé et lors des analyses. Les élèves dans une même salle de classe ne sont techniquement pas indépendants puisqu’ils sont tous corrélés. Ils ont le même enseignant, les mêmes pairs, les mêmes locaux, etc. Ce sont des variables qui peuvent toutes à leur façon avoir des effets sur les comportements des élèves. Dans ce cas, recourir à des analyses multiniveaux sera la seule possibilité pour tenir compte de cette violation et bien représenter les modèles. Un autre exemple est la corrélation entre différents temps de mesure sur une même unité (qui est corrélée avec elle-même). Dans ce cas, ce sera des analyses pour des devis temporels (les analyses multiveaux peuvent également tenir compte du temps).Les deux autres considérations, qui sont elles d’ordre statistique, concernent les résidus (l’écart entre la prédiction et les valeurs réelles de \\(y\\)). Les distributions des variables n’ont pas à être normales; elles peuvent suivre différentes distributions de probabilités. Par contre, l’erreur résiduelle, elle, doit être normalement distribuées. Il s’agit d’un postulat de l’estimation des moindres carrés. La dernière hypothèse concerne la variance résiduelle homoscédastique, c’est-à-dire que l’écart entre les résidus et les valeurs prédites restent constantes, peu importe le niveau sur la droite de régression. Si ce n’est pas le cas pour l’une ou l’autre, c’est qu’une variable théorique important est vraisemblablement négligée ou qu’une des relations n’est pas linéaire entre les variables.\nFigure 14.4: Différentes formes de relation\n","code":""},{"path":"prédire.html","id":"lanalyse-de-régression-avec-r","chapter":" 14 Prédire","heading":"14.2.5 L’analyse de régression avec R","text":"R de base offre la fonction lm() pour linear model (modèle linéaire) afin de réaliser une régression. Pour réaliser l’analyse, deux éléments sont primordiaux : le jeu de données et le modèle. Le jeu de données est en soi assez évident. Le modèle linéaire est quant à lui représenté par l’équation (14.19).Pour écrire le modèle en syntaxe R, il faut remplacer les \\(x\\) par le nom des variables dans le jeu de données, utiliser le ~ (tilde) pour délimiter les variables dépendantes à gauche des variables indépendantes à droite. Les variables indépendantes sont délimitées, comme dans l’équation (14.19), par des signes d’addition +. Il est aussi de définir des effets d’interaction (modération) avec le signe de multiplication * (la section Modérer approfondie davantage ce sujet). Les symboles - (soustraction) et / (division) ne fonctionnent pas. L’intercepte (\\(\\beta_0\\)) est ajouté par défaut. La fonction n’exige pas de mettre la formule entre guillemets17.Pour ajouter une variable, il suffit de VD ~ VI1 + VI2; pour ajouter un effet d’interaction, il est possible de faire VD ~ VI1 * VI2. Pour ajouter une variable nominale (catégorielle), il suffit d’ajouter la variable comme n’importe quelles autres x, mais en s’assurant bien qu’elle soit désignée comme un facteur dans le jeu de données. Si ce n’était pas le cas, la fonction .factor() corrige la situation.La fonction lm() en elle-même n’imprime que peut de résultats. Pour obtenir l’information complète, il faut requérir le sommaire avec la fonction summary(). Le sommaire des résultats contient toute l’information qu’un expérimentateur peut désirer. Il y les coefficients de régression Estimate, leur erreur standard Std. Error, leur valeur-\\(t\\) t value et leur valeur-\\(p\\) Pr(>|t|). Au-dessous de la sortie imprimée, il y également le coefficient de détermination (\\(R^2\\), R-squared), les degrés de liberté et la valeur-\\(p\\) associé au modèle.Tous les éléments peuvent être extraits avec summary(res.lm)$... en remplaçant ... par les éléments désirés. Par exemple, le coefficient de détermination peut être extrait indépendamment avec la syntaxe summary(res.lm)$r.squared.Les résultats de lm() sont comparables avec la fonction maison regression() expliquée auparavant à la section [L’analyse de régression].","code":"# Créer un jeu de données à partir \n# des variables de la syntaxe précédente\nres.lm <- lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n\n# Les résultats\nres.lm\n> \n> Call:\n> lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n> \n> Coefficients:\n> (Intercept)          X.1          X.2          X.3  \n>     -0.0171       0.2139      -0.5520       0.3095\n\n# Sommaire des résultats\nsummary(res.lm)\n> \n> Call:\n> lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n> \n> Residuals:\n>     Min      1Q  Median      3Q     Max \n> -2.5227 -0.5546 -0.0119  0.5513  2.5762 \n> \n> Coefficients:\n>             Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)  -0.0171     0.0253   -0.68      0.5    \n> X.1           0.2139     0.0266    8.03  2.7e-15 ***\n> X.2          -0.5520     0.0258  -21.42  < 2e-16 ***\n> X.3           0.3095     0.0263   11.76  < 2e-16 ***\n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 0.8 on 996 degrees of freedom\n> Multiple R-squared:  0.383,   Adjusted R-squared:  0.381 \n> F-statistic:  206 on 3 and 996 DF,  p-value: <2e-16regression(y = jd$y, X = jd[ ,2:4])\n>           Estimate Std.Error t.value  p.value\n> intercept   -0.018    0.0253  -0.713 4.76e-01\n> X.1          0.214    0.0266   8.034 2.66e-15\n> X.2         -0.552    0.0258 -21.417 5.52e-84\n> X.3          0.310    0.0263  11.757 5.63e-30"},{"path":"prédire.html","id":"rapporter-la-régression","chapter":" 14 Prédire","heading":"14.2.6 Rapporter la régression","text":"Suite à l’analyse qui se commande, comme mentionné ci-haut, avec la syntaxe suivante.Un article scientifique rapporte les résultats à peu près comme ceci.Le modèle tester obtient un coefficient de détermination de \\(R^2(996) = 0.383, p < .001\\). Les trois prédicteurs sont liés significativement à la variable dépendante, respectivement \\(X_1: \\beta_1 = 0.214\\), \\(p = < .001\\), \\(X_2: \\beta_2 = 0.214\\), \\(p = < .001\\), \\(X_3: \\beta_3 = 0.31\\), \\(p = < .001\\).Évidemment, comme l’exemple est artificiel, il y peu de chose à dire sans devoir fabriquer de toutes pièces des interprétations alambiquées bien que cela s’avère bénéfique pour la carrière de certains.Pour vérifier la qualité des résultats, il faut vérifier la distribution des résidus. Pour ce faire, il faut extraire les résidus et les valeurs prédites. Pour la création de graphiques, il est plus simple d’ajouter ces scores au jeu de données. Les fonctions resid() et predict() extraient les résidus et les prédictions en y insérant comme argument le sommaire de la fonction lm() obtenu avec les données.Une fois ces valeurs extraites, le package ggplot2 permet de réaliser rapidement des graphiques (voir Visualiser), comme le diagramme de dispersion à la Figure 14.5 ou l’histogramme des résidus à la Figure 14.6. Dans les meilleures situations, les résidus sont distribués normalement dans l’histogramme et aucune tendance n’est discernable dans le diagramme de dispersion. Si ce n’est pas le cas, il faut étudier davantage la situation, par exemple, une relation non linéaire imprévue. Les Figures 14.5 et 14.6 ne signalent aucun problème, ce qui est attendu considérant la création des données employées.\nFigure 14.5: Relation entre prédicitons et résidus\n\nFigure 14.6: Histogramme des résidus\n","code":"# La régression\nres.lm <- lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n\n# Les résultats\nres.lm\n> \n> Call:\n> lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n> \n> Coefficients:\n> (Intercept)          X.1          X.2          X.3  \n>     -0.0171       0.2139      -0.5520       0.3095\n\n# Sommaire des résultats\nsummary(res.lm)\n> \n> Call:\n> lm(formula = y ~ X.1 + X.2 + X.3, data = jd)\n> \n> Residuals:\n>     Min      1Q  Median      3Q     Max \n> -2.5227 -0.5546 -0.0119  0.5513  2.5762 \n> \n> Coefficients:\n>             Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)  -0.0171     0.0253   -0.68      0.5    \n> X.1           0.2139     0.0266    8.03  2.7e-15 ***\n> X.2          -0.5520     0.0258  -21.42  < 2e-16 ***\n> X.3           0.3095     0.0263   11.76  < 2e-16 ***\n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 0.8 on 996 degrees of freedom\n> Multiple R-squared:  0.383,   Adjusted R-squared:  0.381 \n> F-statistic:  206 on 3 and 996 DF,  p-value: <2e-16\n# Ajouter les résidus et scores prédits à la base de données\n# avec la fonction `resid()`\njd$residu <- resid(res.lm)\njd$predit <- predict(res.lm)\n# Diagramme de dispersion prédits par résidus\njd %>% \n  ggplot(mapping = aes(x = predit, y = residu)) + \n  geom_point() \n# Histogramme des résidus\njd %>% \n  ggplot(mapping = aes(x = residu)) + \n  geom_histogram()"},{"path":"prédire.html","id":"la-matrice-de-corrélation-partielle-et-semi-partielle","chapter":" 14 Prédire","heading":"14.3 La matrice de corrélation partielle et semi partielle","text":"En plus de la matrice de corrélation, deux autres types de corrélation peuvent intéressés l’expérimentateur : les corrélations partielles et les corrélations semi partielles. La présentation est faite en terme d’algèbre matricielle, ce qui facilite substantiellement les calculs et la programmation.","code":""},{"path":"prédire.html","id":"les-corrélations-partielles","chapter":" 14 Prédire","heading":"14.3.1 Les corrélations partielles","text":"La corrélation partielle mesure le degré d’association symétrique entre deux variables en contrôlant pour toutes les autres variables. La contribution des autres variables est retirée sur les deux variables cibles. Conceptuellement, pour une paire de variables, il s’agit de retirer l’effet d’un ensemble de variables contrôles sur la paire de variables, puis de corréler leurs résidus. L’équation (14.21) montre le calcul de la matrice de corrélation partielle.\\[\\begin{equation}\n\\mathbf{R}_{\\text{partielle}} = -\\mathbf{D}_{\\mathbf{S}^{-1}}\\mathbf{S}^{-1}\\mathbf{D}_{\\mathbf{S}^{-1}}\n\\tag{14.21}\n\\end{equation}\\]La formule pour calculer \\(\\mathbf{D}_{\\mathbf{S}^{-1}}\\) est la même que l’équation (14.9), mais où \\(\\mathbf{S}\\) est remplacée par \\(\\mathbf{S}^{-1}\\). En code R, l’équation (14.21) devient la syntaxe suivante.Par souci esthétique, la diagonale de \\(\\mathbf{R}_{\\text{partielle}}\\) est souvent remplacée par l’unité, comme diag(Rp) = 1.","code":"\n# La matrice de corrélation partielle\nRp <- -cov2cor(solve(S))"},{"path":"prédire.html","id":"les-corrélations-semi-partielles","chapter":" 14 Prédire","heading":"14.3.2 Les corrélations semi partielles","text":"La corrélation semi partielle mesure le degré d’association asymétrique entre une variable indépendante et dépendante en contrôlant pour toutes les autres. Conceptuellement, il s’agit de retirer l’effet d’un ensemble de variables contrôles sur la variable dépendante, puis d’utiliser la variable indépendante comme nouveau prédicteur. Autrement dit, il s’agit de la contribution ajoutée d’une variable sur une autre.Le calcul de la matrice de corrélation semi partielle part de la matrice de corrélation partielle et applique un dénominateur pondérant les contributions des autres variables dans la matrice (Kim, 2015). La formule est représentée par l’équation (14.22).\\[\\begin{equation}\n\\frac{\\mathbf{R}_{\\text{partielle}}}{{\\sqrt{\\text{diag}(\\mathbf{S})}\\sqrt{|\\mathbf{S}^{-1}-(\\frac{((\\mathbf{S}^{-1})^2)^{\\prime}}{\\text{diag}(\\mathbf{S}^{-1})}})^{\\prime}|}}\n\\tag{14.22}\n\\end{equation}\\]Voici l’équation en code R.","code":"\n# La matrice de corrélation semi partielle\niS <- solve(S)\nRsp <- -cov2cor(iS) / \n         sqrt(diag(S)) /\n          sqrt(abs(diag(iS) - t(t(iS^2) / diag(iS))))"},{"path":"prédire.html","id":"une-comparaison-entre-partielle-et-semi-partielle","chapter":" 14 Prédire","heading":"14.3.3 Une comparaison entre partielle et semi partielle","text":"La section suivante développe un exemple afin de comparer la corrélation partielle et la corrélation semi partielle. L’équation (14.23) présente une matrice de corrélation hypothétique.\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\n\\begin{array}{ccc}\n1 & .2 & 0\\\\\n.2 & 1 & .8\\\\\n0 & .8 & 1\n\\end{array}\n\\right)\n\\tag{14.23}\n\\end{equation}\\]Le code suivant calcule la matrice de corrélation partielle et semi partielle en fonction des équations (14.21) et (14.22).Les matrices Rp et Rsp se lisent comme suit : la ligne (variable indépendante) prédit la colonne (variable dépendante). Cette distinction n’est pas importante pour la matrice Rp (corrélations partielles), car les variables sont symétriques, mais est très importante pour la matrice Rsp (corrélations semi partielles), car la relation est asymétrique. Par exemple, la corrélation semi partielle de \\(x\\) prédit \\(y\\) est de 0.333, mais l’inverse est de 0.2.À partir de ces résultats, plusieurs constats sont possibles.Pour une même paire de variable, une même corrélation partielle et semi partielle sont de même signe et de magnitude comparable.Pour une même paire de variable, la corrélation partielle est toujours plus grande ou égale que la corrélation semi partielle.La matrice de corrélation partielle est symétrique alors que la matrice de corrélation semi partielle ne l’est pas. Cela s’explique du fait que la corrélation semi partielle attribue un rôle (indépendant et dépendant) pour une paire de variable. La contribution des autres variables est retirée de la variable dépendante, puis c’est l’ajout de la variable indépendante qui est d’intérêt. Par exemple, l’effet de la variable x prédite par y en contrôlant par z est de 0.2. Le lien fixé auparavant à \\(.2\\) entre n’est pas touché par z, car cette dernière est indépendante de x. Toutefois, si inverse la relation, alors le lien entre y prédit par x est différent, car z est lié à y (\\(.8\\))Une dernière observation : les explications basées sur les diagrammes de Venn pour distinguer les corrélations partielles et semi partielles portent plus souvent à confusion (à long terme) qu’elles n’apportent d’éclaircissement (à court terme), bien qu’elles se retrouvent abondamment dans les manuels.\nFigure 14.7: Diagramme représentant l’agencement des variables\nDans la Figure 14.7, tirée de l’exemple avec Sigma ci-haut, la zone \\(\\) illustre la covariance entre \\(x\\) et \\(y\\) au carré18, soit \\(\\sigma_{xy}^2 = \\), et de façon équivalente, \\(\\sigma^2_{yz} = .8^2\\) et \\(\\sigma^2_{xz} = 0\\). Chaque cercle possède une aire totale de \\(1\\), par exemple, l’aire du cercle en bas à gauche est \\(+x=.2^2+.96 =1\\). Par simplicité, les autres aires sont précalculées. Les ouvrages indiquent souvent que la corrélation partielle au carré entre \\(x\\) vers \\(y\\) est égale à \\(/(+y)=.2^2/(.2^2+.32)=.111\\) dont la racine carré donne \\(.333\\), comme prévu. L’inverse, la corrélation partielle au carré entre \\(y\\) vers \\(x\\), devrait être \\(/(+x)\\), mais cela donne \\(/(+x)=.2^2/(.2^2+.96)=.04\\). La racine carré donne \\(.2\\)… la corrélation semi partielle!? L’équation calcule plutôt la corrélation semi partielle et non la partielle. En plus, des zones comme la corrélation semi partielles entre \\(x\\) vers \\(z\\), qui est de -0.267 et au carré donne 0.071, ne sont étrangement pas illustrées. Où est la zone d’aire correspondante? Le pire est certainement que les ouvrages utilisent des agencements de variables plus compliquées, p. ex., un modèle simple avec deux variables non-corrélées, qui cachent les potentielles ambiguïtés.Qu’est-ce qui explique ces divergences? Il revient au fait que les corrélations partielles et semi partielles se basent sur l’inverse de la matrice de covariance, \\(\\mathbf{\\Sigma}^{-1}\\) ou solve(Sigma), la matrice de précision. Elles se retrouve dans un espace différent de celle de la matrice de covariance qui, elle, est bien illustrée dans le diagramme de Venn.Il faut toujours contre-vérifier.","code":"# Créer une matrice de covariance\n# avec des libellées\nSigma <- matrix(c( 1, .2,  0,\n                  .2,  1, .8,\n                   0, .8,  1), \n                nrow = 3, \n                dimnames = list(col <- c(\"x\", \"y\", \"z\"), \n                                row <- col))\n\n# Calculer la matrice de corrélation partielle\nRp <- -cov2cor(solve(Sigma))\ndiag(Rp) <- 1\n\n# Calculer la matrice de corrélation semi-partielle\niS <- solve(Sigma)\nRsp <- Rp / sqrt(diag(Sigma)) /\n          sqrt(abs(diag(iS) - t(t(iS^2) / diag(iS))))\ndiag(Rsp) <- 1\n\n# Sortie\nRp\n>        x     y      z\n> x  1.000 0.333 -0.272\n> y  0.333 1.000  0.816\n> z -0.272 0.816  1.000\nRsp\n>        x     y      z\n> x  1.000 0.333 -0.267\n> y  0.200 1.000  0.800\n> z -0.163 0.816  1.000"},{"path":"créer.html","id":"créer","chapter":" 15 Créer","heading":" 15 Créer","text":"Dans le chapitre sur la régression, les variables indépendantes sont créées simultanément à partir d’une matrice de corrélation, puis des coefficients de régressions sont utilisés pour créer la variable dépendante. Dans un modèle plus élaboré, plutôt que d’ignorer ces corrélations, les liens entre les variables prédictrices peuvent être explicitement modélisés. Cela revient à décrire un modèle de façon à ce qu’une première variable cause la seconde; la première et la seconde causent la troisième et ainsi de suite.La modélisation en système d’équations devient primordiale dans les prochaines sections, notamment pour les analyses de trajectoires, de médiation, de modération, et plus. C’est le fondement de la modélisation par équations structurelles (SEM).Dans ce chapitre19, des techniques de modélisation avancées sont présentées. Elles s’appliquent à des modèles de trajectoires récursifs, c’est-à-dire que la causalité s’enchaîne dans une direction, de la première variable vers la dernière20.","code":""},{"path":"créer.html","id":"la-loi-de-la-somme-des-variances","chapter":" 15 Créer","heading":"15.1 La loi de la somme des variances","text":"La loi de la somme des variances explique comment additionner la variance de variables aléatoires, quelle que soit leur distribution (Casella & Berger, 2002). Le cas le plus simple est celui de deux variables aléatoires, comme \\(x_1\\) et \\(x_2\\), et la façon dont elles s’additionnent pour former une troisième variable, \\(y\\). Elles donnent un modèle simple représenté par l’équation (15.1).\\[\\begin{equation}\ny=x_1+x_2\n\\tag{15.1}\n\\end{equation}\\]La variance de leur somme prend la forme suivante\n\\[\\begin{equation}\n\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2\n\\tag{15.2}\n\\end{equation}\\]où \\(\\sigma^2\\) est le symbole habituel de la variance. La variance de la somme (ou de la différence) de deux variables indépendantes (non-corrélées) aléatoires est leur somme. En fait, la somme de \\(p\\) variables indépendantes est la somme de leurs variances. En syntaxe R, ces équations ressemblent à ceci.Dans cet exemple, les variances de x1 et de x3 sont égales à 2 et 3 respectivement. Si ces deux variables sont additionnées pour créer y, l’équation (15.2) implique que la variance est de \\(\\sigma^2_{x_{1}} + \\sigma^2_{x_2} = 2 + 3= 5\\)Le résultat concorde.En pratique, il est rare que les variables prédictrices soient non corrélées. Ici, aucune corrélation n’été précisée entre x1 et x2. En développant le cas général où les variables sont corrélées, si la covariance entre \\(x_1\\) et \\(x_2\\) est représentée par \\(\\sigma_{x_1,x_2}\\), alors la loi de la somme des variances devient ceci.\\[\\begin{equation}\n\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2+2\\sigma_{x_1,x_2}\n\\tag{15.3}\n\\end{equation}\\]L’équation (15.2) est un cas particulier lorsque \\(\\sigma_{x_1 x_2}=0\\). En syntaxe R, pour \\(\\rho = .5\\), cela revient à programmer la syntaxe suivante.Pour rappel, la corrélation s’obtient avec \\(\\rho_{x_1,x_2}=\\sigma_{x_1,x_2}/(\\sigma_{x_1} \\sigma_{x_2})\\) et inversement la covariance est obtenue par \\(\\rho_{x_1,x_2} \\sigma_{x_1} \\sigma_{x_2}=\\sigma_{x_1,x_2}\\). En calculant d’abord la covariance\\[\\sigma_{x_1,x_2} = \\rho_{x_1,x_2} \\sigma_{x_1} \\sigma_{x_2} = 0.5 \\times 1.414 \\times 1.732 = 1.225\\],confirmée par R,l’équation (15.3) pour la variance de y mène à\\[\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2+2\\sigma_{x_1,x_2}= 2+3 + 2 \\times 1.225  = 7.449\\]\nce qui est très près du résultat simulé.La loi des sommes des variances est directement reliée au théorème de Pythagore tel qu’illustré par l’équation (15.2). Deux variables non corrélées peuvent être envisagées comme formant un triangle rectangle. L’équation (15.3) est la règle du cosinus, ou la généralisation du théorème de Pythagore pour les triangles non rectangulaires, de sorte que la corrélation représente géométriquement un angle (Rodgers & Nicewander, 1988).La Figure 15.1 montre un triangle rectangle dont l’hypoténuse représente l’écart type de la somme de deux variables indépendantes. Lorsque l’angle est à 90\\(^\\circ\\), la corrélation est nulle. Il suffit de mettre les droites au carré pour retrouver l’équation (15.2). Lorsque l’angle rétréci, plus les deux lignes \\(\\sigma\\) se rapprochent et projettent sur une plus longue distance la somme des variances. Lorsqu’elles sont collées l’une sur l’autre, la corrélation est parfaite.\nFigure 15.1: Représentation des variables sous forme de triangles\nUne autre façon mathématiquement plus simple de calculer la loi de la somme des variances est de concevoir la variance de la somme de \\(p\\) variables comme la grande somme de leur matrice de covariance. La grande somme est une fonction mathématique informelle qui fait référence à la somme de tous les éléments d’une matrice. Soit \\(\\mathbf{\\Sigma}\\), la matrice de variance-covariance de deux variables aléatoires \\(x_1\\) et \\(x_2\\) (les éléments diagonaux sont des variances), alors\\[\\begin{equation}\n\\sigma_y^2 = \\text{grande somme}(\\mathbf{\\Sigma}) = \\text{grande somme}\\left(\\begin{array}{cc}\n\\sigma_{x_1}^2 & \\sigma_{x_1,x_2} \\\\\n\\sigma_{x_1,x_2} & \\sigma_{x_2}^2\n\\end{array}\\right)\n\\tag{15.4}\n\\end{equation}\\]ce qui, en faisant la somme, conduit à l’équation (15.3). Encore une fois, si \\(\\sigma_{x_1,x_2}=0\\), alors l’équation (15.4) est égale à l’équation (15.2). Cette formulation l’avantage de montrer l’origine des deux covariances dans l’équation (15.3). La syntaxe est dès plus rudimentaire en code R.Même si la notation matricielle peut sembler peu attrayante au départ, elle devient beaucoup plus intéressante lorsque le nombre de variables, \\(p\\), augmente, car il y \\(p(p-1)/2\\) éléments hors diagonale à ajouter aux \\(p\\) variances. Elle deviendra même indispensable à la fin de ce chapitre.Pour un exemple à \\(p=3\\), l’équation (15.4) devient ceci.\\[\\begin{equation}\n\\sigma_y^2=\\text{grande somme}(\\mathbf{\\Sigma})=\\text{grande somme}\n\\left(\\begin{array}{ccc}\n\\sigma_{x_1}^2&\\sigma_{x_1,x_2}&\\sigma_{x_1 x_3} \\\\\n\\sigma_{x_1,x_2}&\\sigma_{x_2}^2&\\sigma_{x_2 x_3}\\\\\n\\sigma_{x_1 x_3}&\\sigma_{x_2 x_3}&\\sigma_{x_3}^2\n\\end{array}\n\\right)\n\\tag{15.5}\n\\end{equation}\\]Sous la forme linéaire, l’équation (15.5) devient\\[\\begin{equation}\n\\sigma_y^2=\\sigma_{x_1}^2+\\sigma_{x_2}^2+\\sigma_{x_3}^2+2\\sigma_{x_1,x_2}+2\\sigma_{x_1 x_3}+2\\sigma_{x_2 x_3}\n\\tag{15.6}\n\\end{equation}\\]et augmente à mesure que \\(p\\) s’accroît.Une opération matricielle équivalente à la grande somme est\\[\\begin{equation}\n\\mathbf{1}^{\\prime} \\mathbf{\\Sigma} \\mathbf{1}\n\\tag{15.7}\n\\end{equation}\\]où \\(1\\) est un vecteur de longueur \\(p\\) contenant seulement des 1. Voici en syntaxe R.Cette opération produit la somme de tous les éléments de \\(\\mathbf{\\Sigma}\\) (Sigma). Cela sera utile pour dériver un cas plus général dans la section suivante.","code":"\nset.seed(1155)            # reproductibilité\nn <- 100000               # taille élevée pour la précision\ns.x1 <- sqrt(2)           # variance de 2 \ns.x2 <- sqrt(3)           # variance de 3\n\n# Création des variables\nx1 <- rnorm(n = n, sd = s.x1) \nx2 <- rnorm(n = n, sd = s.x2)     y <- x1 + x2\nvar(y)\n> [1] 5\n\n# Vérifier par\ns.x1^2 + s.x2^2\n> [1] 5# Création de variables corrélées\nrho <- .5                          # corrélation de .5\ns.x1x2 <- rho * s.x1 * s.x2        # covariance de x et y\nS <- matrix(c(s.x1^2, s.x1x2,      # matrice de covariance\n             s.x1x2, s.x2^2), ncol =  2, nrow = 2)\n\n# Création de variables\nX <- MASS::mvrnorm(n = n, Sigma = S, mu = c(0, 0), empirical = TRUE)\nx1 <- X[,1] ; x2 <- X[,2]\n\n# La somme de deux variables corrélées\ny <- x1 + x2\nvar(y)\n> [1] 7.45\n\n# Vérifier par\ns.x1^2 + s.x2^2 + 2 * s.x1x2\n> [1] 7.45s.x1x2\n> [1] 1.22sum(S)\n> [1] 7.45Un <- c(1, 1)       # Création du vecteur 1\nt(Un) %*% S %*% Un  # Grande somme\n>      [,1]\n> [1,] 7.45"},{"path":"créer.html","id":"lajout-des-constantes-déchelle-beta","chapter":" 15 Créer","heading":"15.1.1 L’ajout des constantes d’échelle \\(\\beta\\)","text":"Les équations (15.3), (15.4), (15.5) et (15.6) sont des cas particuliers d’une loi plus générale. Elles ne fonctionnent pas si des constantes d’échelle (scaling constant) sont ajoutées (qui seront plus tard des coefficients de régression) ou pour calculer la différence (un type d’échelle également).La situation où la variable \\(y\\) correspond au produit d’une constante \\(\\beta\\) et de la variable \\(x\\) comme :\\[\\begin{equation}\ny=\\beta x\n\\tag{15.8}\n\\end{equation}\\]un modèle linéaire qui ne comporte pas d’erreur (\\(\\epsilon\\) est ignoré pour l’instant).Il peut être utile de considérer \\(\\beta\\) comme, éventuellement, le degré de relation entre deux variances, mais aussi comme un pur modificateur de l’écart type (également une constante d’échelle). De la même manière, une variable aléatoire avec une moyenne de \\(0\\) et un écart type de \\(1\\), \\(x\\sim \\mathcal{N}(0,1)\\), multipliée par la valeur \\(\\beta\\), un facteur d’échelle arbitraire, devient distribuée comme \\(\\beta x \\sim \\mathcal{N}(0,\\beta)\\). Ainsi, \\(\\beta\\) modifié (ou mis à l’échelle) l’écart type de la distribution. Un cas connexe et fréquemment rencontré est lorsque les données sont standardisées en tant que score-\\(z\\) ou non standardisées (divisées ou multipliées respectivement par \\(\\sigma\\)). La contribution est un écart-type, donc \\(\\beta\\) doit être élevé au carré pour donner la variance de \\(y\\), soit \\(\\beta^2\\). Cela mène à l’équation (15.9) qui permet le calcul de la variance de \\(y\\).\\[\\begin{equation}\n\\sigma_y^2=\\beta^2 \\sigma_x^2\n\\tag{15.9}\n\\end{equation}\\]En termes de syntaxe R, cela se traduit (toujours avec le même exemple).L’équation (15.9) donne \\(\\sigma_y^2=\\beta^2 \\sigma_x^2=4^2 \\times 2= 32\\), ce qui est identique.L’étape suivante consiste à considérer un modèle avec deux variables et deux constantes d’échelles, ce qui rappelle de plus en plus la régression. Le modèle linéaire prend la forme suivante\\[\\begin{equation}\ny=\\beta_1 x_1+\\beta_2 x_2\n\\tag{15.10}\n\\end{equation}\\]qui est le même modèle que l’équation (15.1) où les constantes d’échelle \\(\\beta_i\\) sont ajoutées. Pour considérer les constantes d’échelle, la loi de la somme des variances basée sur l’équation (15.2) devient pour deux variables indépendantes\\[\n\\sigma_y^2=\\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma_{x_2}^2\n\\]\n\net quand ils covarient, comme l’équation (15.3), cela devient\\[\\begin{equation}\n\\sigma_y^2=\\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma_{x_2}^2+2\\beta_1 \\beta_2 \\sigma_{x_1,x_2}\n\\tag{15.11}\n\\end{equation}\\]Dans ces équations, les \\(\\beta_i\\) mettent à l’échelle la variance et la covariance. Ces équations se vérifient avec R (toujours avec le même exemple).Le résultat est celui attendu par l’équation (15.12).\\[\n\\begin{aligned}\n\\sigma_y^2 &= \\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma_{x_2}^2+2\\beta_1 \\beta_2 \\sigma_{x_1,x_2} \\\\ & =\n4^2 \\times 2 + 5^2 \\times 3 + 2 \\times 4 \\times 5 \\times 1.225 \\\\ & = 155.99\n\\end{aligned}\n\\]Comme précédemment, la formule en termes d’algèbre matricielle est plus simple et plus élégante, soit\\[\\begin{equation}\n\\sigma_y^2 = \\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B}\n\\tag{15.12}\n\\end{equation}\\]où \\(\\mathbf{B}\\) (\\(\\beta\\) majuscule) est un vecteur contenant tous les coefficients de régression \\(\\beta_i\\) de longueur \\(p\\) prédisant \\(y\\) et s’appliquant à un nombre quelconque de prédicteurs \\(x\\). Le premier \\(\\prime\\) est le symbole de transposition, un opérateur qui pivote la matrice sur sa diagonale et, dans le cas d’un vecteur, retourne les colonnes en lignes et vice-versa (voir son implication dans l’équation (15.13) par exemple). Dans l’équation (15.12), les deux vecteurs \\(\\mathbf{B}\\) revient à élever au carré \\(\\beta_i\\). Si tous les éléments de \\(\\mathbf{B}\\) égalent 1, alors l’équation (15.12) est identique à la définition d’une grande somme, voir l’équation (15.7). En syntaxe R, l’équation (15.12) donne :un code plus général et qui s’appliquera à toutes les situations (peu importe la valeur de \\(p\\), le nombre de variables). La fonction t() correspond à la transposeition, soit le symbole \\(^\\prime\\) de l’équation (15.12)21.Il est intéressant de noter que l’équation (15.12) est la même que les coefficients de détermination, \\(R^2\\) (Cohen et al., 2003), lorsque toutes les variables et les coefficients de régression sont standardisés.Dans le modèle linéaire simple décrit dans l’équation (15.10), l’équation (15.11) devient\\[\\begin{equation}\n\\sigma_y^2 =\n\\left(\\begin{array}{cc}\n\\beta_1 & \\beta_2\n\\end{array}\\right)\n\\left( \\begin{array}{cc}\n\\sigma^2_{x_1} & \\sigma_{x_1,x_2} \\\\\n\\sigma_{x_2,x_1} & \\sigma^2_{x_2}\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\end{array} \\right)\n\\tag{15.13}\n\\end{equation}\\]ce qui est équivalent à l’équation (15.11). À ce stade, le lecteur peut avoir l’intuition que les équations (15.1) à (15.6) n’étaient qu’un cas particulier où tous les \\(\\beta=1\\).","code":"beta <- 4\n# La variance de x1 est toujours de 2\ny <- beta * x1\nvar(y)\n> [1] 32\n\n# Vérifier par\nbeta^2 * s.x1^2\n> [1] 32# En suivant, l'exemple précédent\nbeta1 <- 4\nbeta2 <- 5\ny <- beta1 * x1 + beta2 * x2\nvar(y) \n> [1] 156\n\n# Vérifier par\nbeta1^2 * s.x1^2 + beta2^2 * s.x2^2 + \n  2 * beta1 * beta2 * s.x1x2 \n> [1] 156# Joint beta1 et beta2 dans un vecteur\nbeta <- c(beta1, beta2)\n\n# S est la matrice de covariance calculée auparavant.\nt(beta) %*% S %*% beta \n>      [,1]\n> [1,]  156"},{"path":"créer.html","id":"la-différence-de-variances","chapter":" 15 Créer","heading":"15.1.2 La différence de variances","text":"La variance de la différence de deux variables revient à affirmer que le \\(\\beta\\) des variables soustraites est de signe inverse (s’il est négatif, il devient positif ou s’il est positif, il devient négatif), c’est-à-dire \\(-\\beta\\), ce qui pour le modèle conduit à\\[\ny=\\beta_1 x_1-\\beta_2 x_2\n\\]\n\nou, de manière équivalente\\[\ny=(\\beta_1) x_1+(-\\beta_2) x_2\n\\]\n\ndonnent\\[\\begin{equation}\n\\begin{aligned}\n\\sigma_y^2 &=\n\\left(\\begin{array}{cc}\n\\beta_1 & -\\beta_2\n\\end{array}\\right)\n\\left( \\begin{array}{cc}\n\\sigma^2_{x_1} & \\sigma_{x_1,x_2} \\\\\n\\sigma_{x_2,x_1} & \\sigma^2_{x_2}\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} \\beta_1 \\\\ -\\beta_2 \\end{array} \\right) \\\\[15pt]\n& = \\beta_1^2 \\sigma_{x_1}^2+(-\\beta_2)^2 \\sigma^2_{x_2} + 2(\\beta_1)(-\\beta_2)\\sigma_{x_1,x_2} \\\\[15pt]\n& = \\beta_1^2 \\sigma_{x_1}^2+\\beta_2^2 \\sigma^2_{x_2} - 2\\beta_1\\beta_2\\sigma_{x_1,x_2}\n\\end{aligned}\n\\tag{15.14}\n\\end{equation}\\]Comme prévu, la variance de la différence de deux variables aléatoires est la somme de leurs variances en soustrayant deux fois leur échelle de covariance par \\(\\beta\\). Cela se vérifie avec R.","code":"y = beta1 * x1 - beta2 * x2\nvar(y)\n> [1] 58\n\n# Vérifier par\nbeta = c(beta1, -beta2)   # Signe négatif, très important!\nbeta %*% S %*% beta       # Forme matricielle\n>      [,1]\n> [1,]   58"},{"path":"créer.html","id":"les-implications-pour-la-modélisation","chapter":" 15 Créer","heading":"15.2 Les implications pour la modélisation","text":"La loi de la somme des variances de nombreuses implications dans la modélisation, en particulier si les données sont façonnées selon certaines caractéristiques souhaitables, comme dans les modèles linéaires. La régression linéaire est une approche permettant de modéliser des effets additifs (variables indépendantes, \\(x_i\\)) pour prédire une variable dépendante (\\(y\\)). La linéarité fait référence à la propriété d’une fonction d’être compatible avec l’addition et la mise à l’échelle. En tant que tel, il existe une relation directe entre les équations ci-dessus et le modèle linéaire :\\[\\begin{equation}\ny=\\beta_1 x_1 + ... +\\beta_p x_p+\\epsilon\n\\tag{15.15}\n\\end{equation}\\]en omettant la constante \\(\\beta_0\\), qui ne joue aucun rôle dans la variance de la variable dépendante et en ajoutant l’erreur, une variable indépendante particulière qui est supposée ne pas être liée à \\(x\\), avoir une moyenne de \\(0\\), un écart-type \\(\\sigma_\\epsilon\\). L’équation (15.15) est la forme générale du modèle linéaire de l’équation (15.1) et (15.9) dans laquelle la variance de \\(y\\) est une fonction des variances-covariances des \\(x\\) pondérées par \\(\\beta\\).","code":""},{"path":"créer.html","id":"le-calcul-de-la-variance-de-lerreur","chapter":" 15 Créer","heading":"15.2.1 Le calcul de la variance de l’erreur","text":"Le modèle linéaire le plus simple, soit à deux variables est représentée par l’équation suivante\\[\\begin{equation}\ny=x+\\epsilon\n\\tag{15.16}\n\\end{equation}\\]où \\(\\epsilon\\) est le terme d’erreur résiduel. C’est le minimum pour construire un modèle bivarié. Il s’agit de la même somme bivariée que l’équation (15.1), mais où \\(x_2\\) est remplacé par \\(\\epsilon\\) et est défini comme indépendant de \\(x_1\\) (non corrélé). Sur la base de ce modèle, l’équation (15.2) conduit à l’équation suivante.\\[\n\\sigma_y^2=\\sigma_x^2+\\sigma_\\epsilon^2\n\\]\n\nDans la plupart des cas de modélisation de données, les variances \\(\\sigma_x^2\\) et \\(\\sigma_y^2\\) sont généralement spécifiées à l’avance plutôt que \\(\\sigma_\\epsilon^2\\). Le calcul priori de la variance de l’erreur est plus pertinent que d’ajuster les paramètres d’intérêt. En réarrangeant l’équation (15.16) pour isoler \\(\\epsilon\\),\\[\n\\epsilon=y-x\n\\]\n\net en termes de variance\\[\n\\sigma_\\epsilon^2=\\sigma_y^2+\\sigma_x^2-2\\sigma_{x,y}\n\\]\n\noù l’équation (15.14) pour la forme en algèbre matricielle.\\[\n\\sigma_{\\epsilon}^2 =\n\\left(\\begin{array}{cc}\n1 & -1\n\\end{array}\\right)\n\\left( \\begin{array}{cc}\n\\sigma^2_{y} & \\sigma_{x,y} \\\\\n\\sigma_{x,y} & \\sigma^2_{x}\n\\end{array}\n\\right)\n\\left(\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right) =\n\\sigma^2_y + \\sigma^2_x - 2\\sigma_{x,y}\n\\]\n\n","code":""},{"path":"créer.html","id":"la-variance-des-erreurs-avec-beta","chapter":" 15 Créer","heading":"15.2.2 La variance des erreurs avec \\(\\beta\\)","text":"Une préoccupation lors de la modélisation des données est de préserver les propriétés souhaitées du modèle dans les jeux de données comme la variance de l’erreur, les paramètres de régression, les covariances, etc. Après la variance du terme d’erreur, le dernier élément à considérer est les coefficients de régression, \\(\\beta\\), autrement dit, le degré de la relation entre les variables indépendantes et dépendantes. À partir de l’équation (15.16), ajoute la pente et l’erreur :\\[\ny=\\beta x+\\epsilon\n\\]\n\nLa loi de la somme des variances pour ce modèle devient\\[\\begin{equation}\n\\sigma_y^2=\\beta^2 \\sigma_x^2+\\sigma_\\epsilon^2\n\\tag{15.17}\n\\end{equation}\\]Pour rappel, l’erreur n’est pas corrélée à la variable indépendante \\(x_i\\). Comme précédemment, la forme d’algèbre matricielle est plus simple et plus élégante :\\[\\begin{equation}\n\\sigma_y^2 = \\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B} + \\sigma_\\epsilon^2\n\\tag{15.18}\n\\end{equation}\\]où \\(\\beta\\) est un vecteur contenant tous les coefficients de régression \\(\\beta\\) prédisant \\(y\\) et s’applique à un nombre quelconque de prédicteurs \\(x\\). En profitant de l’indépendance du terme d’erreur, l’équation (15.18) est réarrangée comme :\\[\\begin{equation}\n\\sigma_\\epsilon^2=\\sigma_y^2 -\\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B}\n\\tag{15.19}\n\\end{equation}\\]ce qui donne la variance du terme d’erreur. Sous une forme linéaire (pour une seule variable indépendante), il est possible de réarranger l’équation (15.17) pour isoler \\(\\sigma_\\epsilon\\).\\[\n\\sigma_\\epsilon^2=\\sigma_y^2-\\beta^2 \\sigma_x^2\n\\]\n\n","code":""},{"path":"créer.html","id":"le-scénario-standardisé","chapter":" 15 Créer","heading":"15.2.3 Le scénario standardisé","text":"Pour conclure cette section, le scénario standardisé, c’est-à-dire lorsque les variances des variables sont égales à \\(1\\). Plus précisément, la variance de chaque variable est fixe ; seules les erreurs résiduelles doivent être ajustées pour maintenir ces propriétés. Dans ce scénario, la matrice de variance-covariance, \\(\\Sigma\\), est une matrice de corrélation, qui définit les variances des erreurs résiduelles. Pour calculer la variance des résidus, l’équation (15.19) est utilisée en remplaçant la variance des variables par \\(1\\) comme suit\\[\\begin{equation}\n\\sigma_\\epsilon^2=1 - \\mathbf{B}^{\\prime} \\mathbf{\\Sigma} \\mathbf{B}\n\\tag{15.20}\n\\end{equation}\\]ou, pour un seul prédicteur.\\[\\begin{equation}\n\\sigma_\\epsilon^2=1-\\beta^2 \\sigma_x^2\n\\tag{15.21}\n\\end{equation}\\]Comme mentionner pour l’équation (15.12), ces deux dernières équations rappellent le coefficient de détermination. La valeur \\(1\\) correspond au potentiel explicatif d’une variable, \\(\\sigma_\\epsilon^2\\) est la variance résiduelle et, par conséquent, \\(1-\\sigma_\\epsilon^2\\) est la variance expliquée par le modèle.","code":""},{"path":"créer.html","id":"le-scénario-non-standardisé","chapter":" 15 Créer","heading":"15.2.4 Le scénario non standardisé","text":"Dans la pratique, les scénarios sont rarement standardisés. Les variables n’ont pas toutes des variances de \\(1\\) et des moyennes de \\(0\\). Pour ajouter une touche de naturelle aux jeux de données, il est possible, une fois que le système d’équations est complètement obtenu, d’ajouter des variances différentes en multipliant une variable par l’écart-type souhaité et d’additionner une moyenne. Cela modifiera la matrice de covariance et les coefficients de régression, mais la force relative des liens et la matrice de corrélation resteront identiques. En d’autres termes, la déstandardisation est l’inverse d’un score \\(z\\). Les données créées jusqu’ici sont des scores \\(z\\) et pour les déstandardisés, il faut procéder ainsi \\(x = s(z + \\bar{x})\\), où \\(s\\) est l’écart-type et \\(\\bar{x}\\) est la moyenne de la variable déstandardisée.","code":""},{"path":"créer.html","id":"la-création-de-modèle-récursif","chapter":" 15 Créer","heading":"15.3 La création de modèle récursif","text":"Jusqu’à présent, seule la création d’une variable est présentée. Le défi pour créer un système d’équations est d’obtenir la matrice de covariance des variables précédentes pour chaque variable subséquente.Puisque le sujet peut devenir compliqué rapidement, cette section est basée sur un exemple à trois variables avec un scénario standardisé (variables centrées et réduites). Le cas général sera développé par la suite.","code":""},{"path":"créer.html","id":"le-cas-spécifique","chapter":" 15 Créer","heading":"15.3.1 Le cas spécifique","text":"Le défi commence lorsqu’il y trois ou plus variables à générer. Le cas à trois variables est tout de même abordable. La causalité est unidirectionnelle, c’est-à-dire d’une variable vers une autre, sans retour en arrière. Cela se nomme un modèle récursif, impliquant du même coup l’existence de modèles non-récursifs. Les modèles non-récursifs ont la caractéristique d’avoir une ou des boucles de causalité bidirectionnelle, comme \\(x \\leftrightarrow y\\), alors que les modèles récursif n’ont que des causalité directionnel comme \\(x \\rightarrow y\\). Les modèles non-récursifs dépassent la portée de cet chapitre.Pour un modèle à trois variables, une seule configuration récursive complète est possible. Elle est présentée à la Figure 15.2.\nFigure 15.2: Modèle à trois variables\nLa Figure 15.2 montre un modèle à trois variables. La première variable \\(x_1\\) prédit \\(x_2\\) et \\(x_3\\), en plus \\(x_2\\) prédit \\(x_3\\). Ce modèle peut aussi se représenter en matrice \\(\\mathbf{B}\\) dans laquelle se retrouvent les coefficients de régression qui relient les variables. Dans ce cas-ci, \\(\\beta_{3,1}\\) signifie que la variable 1 prédit la 3 à un degré \\(\\beta_{3,1}\\); \\(\\beta_{3,2}\\) signifie que la variable 2 prédit la 3 à un degré \\(\\beta_{3,2}\\). Le premier indice correspond à l’effet (variable dépendante) et le deuxième à la cause (variable indépendante).\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{ccc}\n0 & 0 & 0 \\\\\n\\beta_{2,1} & 0 & 0 \\\\\n\\beta_{3,1} & \\beta_{3,2} & 0 \\\\\n\\end{array}\n\\right)\n\\tag{15.22}\n\\end{equation}\\]Afin de bâtir un exemple complet avec R, la syntaxe ci-dessous montre les paramètres arbitraires pour la création de données à parti du modèle de la figure 15.2.La première variable \\(x_1\\) est exogène, c’est-à-dire qu’elle n’est prédite par aucune autre variable, et n’obtient aucune information d’aucune autre variable. Cela se perçoit notamment dans la matrice \\(\\mathbf{B}\\) avec la première ligne ne contenant que des 0. Pour créer x1, il suffit de connaître sa variance (1 dans un scénario standardisé). Il faudra en supplément un vecteur de variances (ici, toutes fixées à 1 par le scénario standardisé).À l’aide de la loi de la somme des variances, la création de \\(x_2\\) est assez simple puisqu’il n’y qu’un seul prédicteur.\\[\nx_2 = \\beta_{2,1}x_1+\\epsilon_{x_2}\n\\]En suivant l’équation (15.21), il est possible de calculer la variance résiduelle, toujours en assumant que \\(\\sigma^2_{x_1} = \\sigma^2_{x_2}=1\\).\\[\n\\sigma^2_{\\epsilon_{x_2}} = 1-\\beta_{2,1}^2\\sigma^2_{x_1}=1-\\beta_{21}^2\n\\]En code R, il est possible de procéder ainsi.Maintenant, il reste à créer la variable \\(x_3\\). Celle-ci est générée à partir de \\(x_1\\) et \\(x_2\\) selon l’équation suivante.\\[\nx_3 = \\beta_{3,1}x_1 + \\beta_{3,2}x_2 + \\epsilon_{x_3}\n\\]La variance résiduelle suit l’équation (15.20). En équation linéaire, elle s’écrit comme suit.\\[\n\\begin{aligned}\n\\sigma^2_{\\epsilon_{x_3}} &  = \\sigma^2_{x_3} -( \\beta_{3,1}^2\\sigma^2_{x_1} + \\beta_{3,2}^2\\sigma^2_{x_2}  +  2\\beta_{3,1}\\beta_{3,2}\\sigma_{x_1,x_2}) \\\\\n& = 1-(\\beta_{3,1}^2+\\beta_{3,2}^2  +  2\\beta_{3,1}\\beta_{3,2}\\sigma_{x_1,x_2})\n\\end{aligned}\n\\]Elle occasionne toutefois un défi, car la covariance entre \\(x_1\\) et \\(x_2\\) n’est pas explicitement connue. Dans le cas d’une variable prédite exclusivement par une autre variable qui elle est exogène (sans prédicteur), leur covariance est égale au coefficient de régression, soit \\(\\beta_{21}\\). Ce cas ne survient que dans cette situation précise. Il sera impératif de dégager une solution générale pour des modèles ayant plus de trois variables, la complexité du calcul de la covariance augmentant avec la croissance de \\(p\\). En R, \\(x_3\\) se génère ainsi.Il est possible de vérifier les caractéristiques des trois modèles.Comme il s’agit d’un scénario standardisé, la matrice de corrélation est similaire à la matrice de covariance. Notamment, les variances sont près de \\(1\\). Aussi, les résultats confirment que la covariance entre \\(x_1\\) et \\(x_2\\) est bel et bien le coefficient de régression, mais surtout, comme il était mentionné, qu’il s’agit du seul cas où cela est vrai. Les régressions effectuées par lm() retrouvent également les \\(\\beta\\) choisis pour l’exemple.","code":"\n# Pour la reproductibilité\nset.seed(1448)            \nn <- 100000\n\n# Paramètres arbitraires\nbeta21 <- .2 \nbeta31 <- .4\nbeta32 <- -.5\n# Création de la première variable\nx1 <- rnorm(n = n, sd = 1) # Variance de 1\n# Variance résiduelle\ne_x2 <- 1 - beta21^2\n\n# Création de la variable\nx2 = beta21 * x1 + rnorm(n = n,  sd = sqrt(e_x2)) # Variance de 1\n# Variance résiduelle\ne_x3 <- 1 - (beta31^2 + beta32^2 + \n               2 * beta31 * beta32 * beta21)\n\n# Création de la variable\nx3 = beta31 * x1 + beta32 * \n  x2 + rnorm(n = n, sd = sqrt(e_x3))# Création du jeu de données\nX <- data.frame(x1 = x1,\n                x2 = x2,\n                x3 = x3)\n\n# Comme il s'agit d'un scénario standardisé, la matrice de\n# corrélation est similaire à la matrice de covariance.\ncov(X)\n>       x1     x2     x3\n> x1 1.002  0.202  0.303\n> x2 0.202  1.004 -0.418\n> x3 0.303 -0.418  0.997\ncor(X)\n>       x1     x2     x3\n> x1 1.000  0.201  0.303\n> x2 0.201  1.000 -0.417\n> x3 0.303 -0.417  1.000\n\n# Retrouver beta21\nlm(x2 ~ x1, data = X)\n> \n> Call:\n> lm(formula = x2 ~ x1, data = X)\n> \n> Coefficients:\n> (Intercept)           x1  \n>     0.00148      0.20113\n\n# Retrouver beta31 et beta32\nlm(x3 ~ x1 + x2, data = X)\n> \n> Call:\n> lm(formula = x3 ~ x1 + x2, data = X)\n> \n> Coefficients:\n> (Intercept)           x1           x2  \n>     0.00578      0.40254     -0.49663"},{"path":"créer.html","id":"le-cas-général","chapter":" 15 Créer","heading":"15.3.2 Le cas général","text":"Un cas général permet d’obtenir la matrice de covariance \\(\\mathbf{\\Sigma}\\) à partir de la matrice \\(\\mathbf{B}\\) et d’un vecteur de variance \\(\\text{diag}(\\mathbf{\\Sigma})\\). Par la suite, il est possible de créer des variables en série comme la section précédente, ou bien de revenir à ce qui se faisait dans les chapitres précédents, c’est-à-dire d’utiliser MASS:mvrnorm() pour générer des données.Pour obtenir la matrice de covariance, il est nécessaire d’avoir une matrice de coefficients de régression\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{cccc}\n0 & 0 & 0 & 0\\\\\n\\beta_{2,1} & 0 & 0 & 0\\\\\n...& ...  & 0 & 0 \\\\\n\\beta_{p,1} & ... & \\beta_{p,p-1} & 0\n\\end{array}\n\\right)\n\\tag{15.23}\n\\end{equation}\\]et d’un vecteur de variances\\[\\begin{equation}\n\\text{diag}(\\mathbf{\\Sigma}) = \\left(\\sigma^2_{x_1},...,\\sigma^2_{x_p} \\right)\n\\end{equation}\\]Ces matrices sont construites de façon générale. La matrice \\(\\mathbf{B}\\) est de dimension \\(p \\times p\\) avec des valeurs nulles comme triangle supérieur incluant la diagonale. Les coefficients de régresison se trouvent dans le triangle extérieur. Le vecteur de variance contient \\(p\\) valeurs qui représentent les variances de variables. Elles sont toutes à l’unité quand le scénario est standardisé.L’idée sous-jacente est qu’il est possible de calculer les covariances de la variable \\(\\) à partir de la matrice de covariance des variables précédente \\(1:(-1)\\) soit \\(\\mathbf{\\Sigma}_{1:(-1),1:(-1)}\\) et des coefficients de régression associés \\(\\mathbf{B}_{, 1:(-1)}\\) en procédant en série pour toutes variables de \\(x_2\\) (\\(=2\\)) (prédite par au moins une variable, toutes sauf la première) jusqu’à la dernière variable \\(x_p\\) (\\(=p\\)). Le calcul est décrit à l’équation (15.24), malheureusement pour certain, elle recourt à l’algèbre matricielle, mais demeure toujours beaucoup plus simple et générale que si elle était présentée en algèbre linéaire.\\[\\begin{equation}\n\\begin{aligned}\n\\mathbf{\\Sigma}_{,1:(-1)} = \\mathbf{B}_{+1,1:}\\mathbf{\\Sigma}_{1:,1:}\\\\\n\\mathbf{\\Sigma}_{1:(-1),} = \\mathbf{B}_{+1,1:}\\mathbf{\\Sigma}_{1:,1:}\n\\end{aligned}\n\\tag{15.24}\n\\end{equation}\\]Il faut à chaque étape s’assurer de faire le remplacement des valeurs obtenues sur chaque côté de la diagonale, ce pourquoi deux équations sont reproduites à l’équaiton (15.24) avec des indices différents pour \\(\\mathbf{\\Sigma}\\).Voici, pour l’exemple précédent, les étapes de calcul de l’équation (15.24) décrites une à une. D’abord, il faut construire une matrice, \\(\\mathbf{\\Sigma}\\) (S en code R) avec comme diagonale les variances. Puis, calculer l’équation (15.24). Ensuite, il faut remplacer le résultat obtenu des deux côtés de la diagonale de façon à ce que S demeure symétrique. Ces étapes sont répétées pour \\(=2,...,p\\) (dans cet exemple, \\(p=3\\)).Avant de procéder, une courte digression sur une façon de réaliser en moins de lignes, mais avec une syntaxe plus complexe, la réassignation des valeurs dans S. Le code est présenté dans la syntaxe ci-dessous. Il appert qu’il n’est pas aussi intéressant à programmer pour sauver deux lignes.Trêve de digression, une fois la matrice de covariance S calculée, la fonction MASS::mvrnorm() peut être utilisée pour créer des données. Les résultats sont presque identiques pour les régressions.Comme la formule est générale et qu’elle implique plusieurs itérations, il est envisageable de programmer ces calculs avec une boucle dans une fonction maison. La fonction maison beta2cov() permet, à partir d’une matrice de coefficient de régression et d’un vecteur de variance, d’obtenir la matrice de covariance.Évidemment, comme il est possible de passer d’une matrice de coefficient de régression à une matrice de covariance, l’inverse est envisageable. L’équation (15.24) est réarrangée pour isoler \\(\\mathbf{B}\\) de l’équation, ce qui donne l’équation (15.25).\\[\\begin{equation}\n\\mathbf{B}_{+1, 1:} = \\mathbf{\\Sigma}^{-1}_{1:,1:}\\mathbf{\\Sigma}_{1+,1:}\n\\tag{15.25}\n\\end{equation}\\]L’équation (15.25) se transforme (relativement) aisément en fonction maison. Par rapport à beta2cov(), la boucle n’inclut pas la \\(p\\)e variable, mais bien la première.La fonction maison cov2beta() est maintenant testée sur S pour évaluer si elle retourne bien la matrice B originale.Ce qui est le cas.Une seule contrainte s’impose lors de la du calcul de la covariance à partir des coefficients de régression. Il s’agit de s’assurer que la matrice de covariance demeure positive semi-définie à chaque étape. Cela se manifeste notamment lorsque les coefficients de régression pour une variable dépendante sont si élevés que le coefficient de détermination surpasse la variance de la variable en question. Autrement dit, la variance de la variable n’est pas assez élevée pour le potentiel explicatif. Mathématiquement, le problème revient à \\(1-R^2_X < 0\\) ou plus généralement que \\(\\sigma^2_y-\\mathbf{B^\\prime\\Sigma B}<0\\). Dans ces cas, la variance résiduelle négative, ce qui est impossible. Cette situation correspond à une variance expliquée de plus que 100%, autrement dit, la somme des parts d’informations (variables prédictrices plus les résidus) est plus grande que l’information totale. L’une des corrections apporté sont soit de réduire les coefficients de régression \\(\\mathbf{B}\\) ou bien d’augmenter la variance de \\(\\sigma^2_y\\) de sorte que \\(\\sigma^2_y-\\mathbf{B^\\prime\\Sigma B}>=0\\) soit toujours vrai à chaque étape.Une dernière note, comme ces équations et syntaxes procèdent de \\(=2,...,p\\), l’ordre des variables est primordiale et altérer leur ordre des conséquences substantielles sur les résultats. Lorsque la matrice \\(\\mathbf{B}\\) est créées, il faut être sûr de l’ordre déterministe des variables, c’est-à-dire, quelle variable cause quelles autres variables, comme illustre à la Figure 15.2 par exemple. Changer ou retirer ne serait-ce qu’une variable change les coefficients de régression : les régressions ne sont plus identiques, la matrice de coefficients de régression ne sera pas retrouvée. Il ne faut pas être surpris donc, si les résultats changent dans cette situation. Nonobstant, changer ou retirer une variable peut être pertinent dans certains contextes, surtout pour l’étude de la misspécification (en anglais) des modèles, c’est-à-dire lorsqu’un modèle erroné est utilisé plutôt que le vrai modèle, ce qui entraîne notamment des biais. Les études à ce sujet emprunteront une méthode statistique similaire.","code":"# Matrice B\n# Rappel : beta21 <- .2; beta31 <- .4; beta32 <- -.5\nB <- matrix(c(  0,     0,    0,\n             beta21,   0,    0,\n             beta31, beta32, 0), \n           ncol  = 3, nrow = 3, byrow = TRUE)\n\n# Vecteur de variances\nV = c(1, 1, 1)\n\n# Montrer la matrice B\nB\n>      [,1] [,2] [,3]\n> [1,]  0.0  0.0    0\n> [2,]  0.2  0.0    0\n> [3,]  0.4 -0.5    0# Créer une matrice de covariance préliminaire\nS <- diag(V)\n\n# Aucune covariance n'est inscrite dans S\nS\n>      [,1] [,2] [,3]\n> [1,]    1    0    0\n> [2,]    0    1    0\n> [3,]    0    0    1\n\n# Première étape\ni <- 2\n\n# Calcul de la covariance entre x1 et x2\nB[i, 1:(i-1)] %*% S[1:(i-1),1:(i-1)]\n>      [,1]\n> [1,]  0.2\n\n# Calcul de la covariance entre x1 et x2 assignée à COV\nCOV <- B[i, 1:(i-1)] %*% S[1:(i-1),1:(i-1)]\nS[i, 1:(i-1)] <- COV\n\n# Il faut remplacer ce résultat de chaque côté de la diagonale\nS[1:(i-1),i] <- COV\n\n# La matrice est mise à jour pour i = 2\nS\n>      [,1] [,2] [,3]\n> [1,]  1.0  0.2    0\n> [2,]  0.2  1.0    0\n> [3,]  0.0  0.0    1\n\n# Pour la seconde étape, l'équation est reprise pour i = 3\ni <- 3\n\n# Calcul de la covariance entre x1 et x2 assignée à COV\nCOV <- B[i, 1:(i-1)] %*% S[1:(i-1), 1:(i-1)]\nS[i, 1:(i-1)] = COV\nS[1:(i-1), i] = COV\n\n# La matrice est mise à jour pour i = 3\nS\n>      [,1]  [,2]  [,3]\n> [1,]  1.0  0.20  0.30\n> [2,]  0.2  1.00 -0.42\n> [3,]  0.3 -0.42  1.00\n\n# Elle est approximativement identique \n# aux données de l'exemple précédent\ncov(X)\n>       x1     x2     x3\n> x1 1.002  0.202  0.303\n> x2 0.202  1.004 -0.418\n> x3 0.303 -0.418  0.997\n# Une note pour indiquer que les valeurs à remplacer pourrait\n# être fait en une seule ligne de syntaxe en bénéficiant du \n# recyclage vectoriel de R, mais la solution n'est ni simple, \n# ni élégante.\nremplacer = cbind(c(rep(i, i-1), 1:(i-1)), c(1:(i-1), rep(i, i-1)))\nS[remplacer]# La même que l'exemple précédent\nset.seed(1448) \n\n# Création de données\nX <- MASS::mvrnorm(n = n, mu = c(0, 0, 0), Sigma = S)\n\n# Configurer en tableau de données (data.frame)\nX <- as.data.frame(X)\ncolnames(X) = c(\"x1\",\"x2\",\"x3\")\n\n# Retrouver beta21\nlm(x2 ~ x1, data = X)\n> \n> Call:\n> lm(formula = x2 ~ x1, data = X)\n> \n> Coefficients:\n> (Intercept)           x1  \n>    -0.00219      0.20191\n\n# Retrouver beta31 et beta32\nlm(x3 ~ x1 + x2, data = X)\n> \n> Call:\n> lm(formula = x3 ~ x1 + x2, data = X)\n> \n> Coefficients:\n> (Intercept)           x1           x2  \n>    -0.00579      0.40295     -0.49899\n# De Beta vers covariance (beta 2 covariance)\nbeta2cov <- function(B, V = NULL){\n  \n  p <- dim(B)[1]    # Nombre de variables\n  \n  if(is.null(V)){   # Si V est nulle, alors V est une\n    S <- diag(p)    # matrice diagonale d'identité,\n  }else{            # autrement il s'agit d'une matrice\n    S <- diag(V)    # avec les variances en diagonale\n  }  \n  \n  # Boucle de calcul pour la covariance\n  # de la variable i (i = 2:p)\n  for(i in 2:p){\n    \n    COV <- B[(i), (1:(i-1))] %*% S[1:(i-1), 1:(i-1)]\n    S[i, 1:(i-1)] <- COV\n    S[1:(i-1), i] <- COV\n    \n  }\n  \n  return(S)\n  \n}\n# De la covariance à Beta (cov2beta)\ncov2beta <- function(S){\n  \n  p <- ncol(S)            # Nombre de variable\n  BETA <- matrix(0, p, p) # Matrice vide\n  \n  # Boucle de calcul pour la covariance \n  # de la variable i (i = 1:(p-1))\n  for(i in 1:(p-1)){\n    \n    BETA[i+1, 1:i] <- solve(S[1:i, 1:i], S[1+i, 1:i])\n    \n  }\n  \n  return(BETA)\n  \n}cov2beta(S)\n>      [,1] [,2] [,3]\n> [1,]  0.0  0.0    0\n> [2,]  0.2  0.0    0\n> [3,]  0.4 -0.5    0"},{"path":"médier.html","id":"médier","chapter":" 16 Médier","heading":" 16 Médier","text":"Outre les liens directs entre deux variables (ce qu’une régression permet de découvrir), il existe également des liens indirects, une relation sous-jacente entre une variable indépendante et une variable dépendante expliquée par l’inclusion d’une troisième variable. L’attrait d’une telle analyse est patent : les expérimentateurs s’intéressent souvent à expliquer les mécanismes biologiques, psychologiques, cognitifs, etc., qui sous-tendent la relation entre deux variables.L’analyse de médiation permet de découvrir et tester des liens indirect. Elle est une analyse statistique de plus en plus populaire parmi les expérimentateurs, peu importe leur discipline, puisqu’elle quantifie le degré selon lequel une variable participe à la transmission du changement d’une cause vers son effet. L’analyse de médiation peut contribuer à mieux comprendre la relation entre une variable indépendante et une variable dépendante lorsque ces variables n’ont pas de lien direct évident.L’analyse de médiation est un sous-ensemble de l’analyse de trajectoire dans lequel le statisticien s’intéresse à la relation entre la variable indépendante \\(x\\) sur la variable dépendante \\(y\\) par l’intermédiaire de la variable médiatrice \\(m\\). Elle s’inscrit dans un système d’équations. L’analyse de médiation se base sur les liens indirects qui existent dans ce système d’équations. Ces liens indirects sont ces relations intermédiaires qui intéressent le statisticien.Il est plus aisé de concevoir ce qu’est un lien indirect en le comparant aux liens directs. Un lien direct, c’est la relation entre une variable indépendante et une dépendante, comme le coefficient de régression, par exemple. Le lien indirect, c’est la relation qui existe entre une variable indépendante et une dépendante à travers une ou plusieurs autres variables.Pour les fins de ce chapitre22, l’analyse à un seul médiateur est présentée (aussi nommé l’analyse de médiation simple). Il existe de nombreuses extensions (parallèle, sérielle, modérée, etc.) dont les fondements reposent ultimement sur la même logique que celle présentée.","code":""},{"path":"médier.html","id":"analyse-de-médiation-simple","chapter":" 16 Médier","heading":"16.1 Analyse de médiation simple","text":"La Figure 16.1 présente le diagramme de trajectoire correspondant au modèle de médiation simple (un seul médiateur) dans le panneau supérieur.\nFigure 16.1: Modèle de médiation simple\nLe cadran supérieur devrait être familier aux lecteurs, car il été abordé dans le chapitre Créer, dans une orientation légèrement différente. La Figure 16.1 peut également être représentée avec une matrice de coefficients de régression \\(\\mathbf{B}\\) dans laquelle se retrouvent les coefficients de régression qui relient les variables.\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{ccc}\n0 & 0 & 0 \\\\\n\\beta_{2,1} & 0 & 0 \\\\\n\\beta_{3,1} & \\beta_{3,2} & 0 \\\\\n\\end{array}\n\\right)\n\\tag{16.1}\n\\end{equation}\\]Dans ce cas-ci, \\(\\beta_{2,1}\\) signifie que la variable \\(x\\) prédit le médiateur \\(m\\), \\(\\beta_{3,2}\\) signifie que le médiateur prédit la variable \\(y\\) et \\(\\beta_{3,1}\\) signifie que la variable \\(x\\) prédit la variable \\(y\\). En analyse de médiation, ces effets sont nommés des effets directs, comme l’effet direct de \\(x\\) sur \\(y\\), ou l’effet direct de \\(m\\) sur \\(y\\). Pour identifier un effet médiateur, le statisticien cherche l’effet indirect de \\(x\\) sur \\(y\\), c’est-à-dire l’effet de \\(x\\) passant par \\(m\\) et allant à \\(y\\). L’effet indirect est le produit des deux effets directs concernés et correspond à l’équation (16.2).\\[\\begin{equation}\n\\beta_{2,1}\\times\\beta_{3,2}\n\\tag{16.2}\n\\end{equation}\\]Ce résultat est dérivé notamment des travaux de Wright (1934) sur la méthode de coefficients de trajectoires (path coefficients) qui est un moyen flexible de relier les coefficients de régression entre les variables d’un système d’équations.Lorsqu’il n’y pas de \\(m\\), la relation existante entre \\(x\\) et \\(y\\) est nommée l’effet total représentée par \\(\\sigma_{x,y}\\) et est illustrée dans le cadran inférieur de la Figure 16.1. Ce lien correspond au coefficient de régression entre \\(x\\) et \\(y\\). Dans ce cas spécifique à deux variables, il s’agit également de la covariance entre \\(x\\) et \\(y\\). L’effet total peut être séparé en deux autres effets dont il fait la somme : l’effet direct de \\(x\\) sur \\(y\\) (\\(\\beta_{3,1}\\)) et l’effet indirect \\(\\beta_{2,1}\\beta_{3,2}\\).\\[\\begin{equation}\n\\sigma_{3,1} = \\beta_{3,1} + \\beta_{2,1} \\beta_{3,2}\n\\tag{16.3}\n\\end{equation}\\]Un avantage de l’équation (16.3) est qu’elle met en relation l’interdépendance des quatre composantes pour dériver un effet total, direct ou indirect. Si trois des mesures sont connues, la quatrième est dérivable avec un peu de réaménagement algébrique. Un autre avantage sur la plan statistique est également de montrer le lien intime qui existe entre la matrice de covariance et les coefficients de régression.","code":""},{"path":"médier.html","id":"tester-leffet-indirect","chapter":" 16 Médier","heading":"16.2 Tester l’effet indirect","text":"Qu’en est-il du test d’hypothèse de l’effet indirect? Il existe trois méthodes principales pour tester si l’effet indirect est significatif. Il s’agit de la méthode d’étape causale (causal step method), la plus populaire étant celle de Baron et Kenny (Baron & Kenny, 1986), la méthode delta multivarié (multivariate delta method) (Rao, 2002) duquel le test de Sobel (Sobel, 1982) est un cas particulier et (c) les méthodes de bootstrap (Efron & Tibshriani, 1979). Chacune de ces méthodes sera détaillée ci-dessous.","code":""},{"path":"médier.html","id":"la-méthode-détape-causale","chapter":" 16 Médier","heading":"16.2.1 La méthode d’étape causale","text":"La méthode d’étape causale aussi connue sous le nom de test de Baron-Kenny est un test séquentiel d’hypothèse afin de vérifier l’existence du lien indirect. Ce test est présenté à des fins historiques uniquement (certains expérimentateurs l’exigent et l’utilisent encore!). Par contre, dans la littérature méthodologique, il n’est plus recommandé, étant rejeté en faveur d’autres méthodes plus adéquates tant sur le plan statistique que conceptuel. La méthode provenant des années 80 lorsque les ordinateurs personnels n’étaient pas encore dans toutes les chaumières certainement du mérite pour l’époque, mais n’est plus nécessaire aujourd’hui. En plus, c’est un bon exercice d’extraction de résultats avec R.Pour réaliser le test en bonne et due forme, trois tests d’hypothèse sont réalisés en séries.Existe-t-il une relation entre \\(x\\) et \\(y\\) pour l’effet total? Autrement dit, avant de procéder à l’analyse du lien indirect, y -t-il un lien entre les variables indépendantes et dépendantes?Existe-t-il une relation entre \\(x\\) et \\(y\\) pour l’effet total? Autrement dit, avant de procéder à l’analyse du lien indirect, y -t-il un lien entre les variables indépendantes et dépendantes?Existe-t-il une relation directe entre \\(x\\) et \\(m\\)? Y -t-il un lien entre le variable indépendante et le médiateur?Existe-t-il une relation directe entre \\(x\\) et \\(m\\)? Y -t-il un lien entre le variable indépendante et le médiateur?Existe-t-il une relation directe entre \\(m\\) et \\(y\\)? Y -t-il un lien entre le médiateur et la variable dépendante?Existe-t-il une relation directe entre \\(m\\) et \\(y\\)? Y -t-il un lien entre le médiateur et la variable dépendante?Les étapes 2 et 3 visent à vérifier si le médiateur bel et bien un rôle à jouer entre la variable indépendante et dépendante. Le rejet de l’une ou l’autre de ces trois hypothèses mènerait certainement à un statisticien à douter d’une relation entre les variables. Comment pourrait-il y avoir un lien indirect, si l’un de ces liens n’était pas soutenu par les données.En guise de quatrième test, les expérimentateurs suivant cette tradition testent si la médiation est complète ou partielle. La médiation complète signifie que l’entièreté du lien total entre \\(x\\) et \\(y\\) est maintenant attribuable à l’ajout de \\(m\\). Ce résultat s’observe lorsque le lien direct entre \\(x\\) et \\(y\\) (lorsque \\(m\\) est inclus pour prédire \\(y\\)) n’est pas significatif en comparaison à la première étape où le lien total, lui, était significatif. Si le lien direct entre \\(x\\) et \\(y\\) est toujours significatif, même après avoir ajouté le \\(m\\) dans la prédiction de \\(y\\), alors la médiation est partielle.La syntaxe montre comment la méthode d’étape causale pourrait être programmée dans R. Il y trois régressions (lm()), une pour chaque test d’hypothèse et quatre étapes sous forme de conditionnel (les trois hypothèses plus le type de médiation).Si une des conditions () n’est pas respectée, le test retourne l’hypothèse nulle. À chacune des étapes, la valeur-\\(p\\) du coefficient testé est extraite et comparée à l’erreur de type fixée à l’avance (\\(\\alpha\\)). Le test doit être significatif pour procéder à l’étape suivante. À la toute fin, l’hypothèse nulle est rejetée et le type de médiation (complète ou partielle) est rapporté.Pour chaque régression, il faut extraire la valeur-\\(p\\) de l’estimateur concerné. La valeur-\\(p\\) se trouve dans le sommaire (summary()) du résultat de la régression (etape) dans la liste coefficients. Dans cette liste, il faut identifier la ligne (\"estimateur\") à la colonne \"Pr(>|t|)\" qui correspond aux valeurs-\\(p\\). Au final, l’extraction se commande summary(etape)$coefficients[\"estimateur\", \"Pr(>|t|)\"]. Pour bien fonctionner, les variables du jeu de données doivent se nommer x, m et y.Plusieurs raisons suggèrent de ne pas utiliser la méthode d’étape causale. D’abord, comme une série de tests d’hypothèse est réalisée, l’erreur de type est différente de celle fixée. Il y trois hypothèses nulles à rejeter, chacune ayant un seuil \\(\\alpha\\). La vraie erreur de type est égale la probabilité de rejeter toutes ces hypothèses nulles accidentellement. Cela correspond à \\(\\alpha^3\\). Avec \\(\\alpha=.05\\), cela signifie que le taux est de \\(\\alpha^3=.05^3= 1.25\\times 10^{-4}\\), ce qui est bien plus stricte que l’erreur de type fixée. Cela entraîne une perte de puissance, c’est-à-dire de trouver des effets indirects lorsqu’ils sont vrais.Une seconde raison est que l’absence d’effet total entre la variable indépendante et dépendante n’est pas une hypothèse obligatoire. Autrement dit, la première étape, tester si \\(x\\) est lié à \\(y\\) sans tenir de compte de \\(m\\), n’est pas recommander. Il peut exister des effets indirects théoriquement valides sans effets totaux. De plus, les deux autres hypothèses (étapes) ne sont pas obligatoire non plus (quoi qu’elles sont un peu plus dures à justifier) d’ailleurs. Il est tout à fait possible d’avoir des effets indirects significatifs dont les effets directs qui le compose sont non-significatifs23.","code":"\nBK <- function(donnees, alpha = .05){\n  # alpha est l'erreur de type I\n  \n  # Créer l'hypothèse nulle\n  decision = FALSE\n  interpretation = \"L'effet indirect est non significatif\"\n  \n  # Première régression\n  # Est-ce que l'effet total est significatif?\n  etape1 <- lm(formula = y ~ x, data = donnees)\n  pC <- summary(etape1)$coefficients[\"x\", \"Pr(>|t|)\"]\n  \n  if (pC <= alpha){\n    # Si le coefficient rho_31 est significatif, alors\n    # Deuxième régression\n    # Est-ce que l'effet direct de x vers m est significatif?\n    etape2 <- lm(formula = m ~ x, data = donnees)\n    pA <- summary(etape2)$coefficients[\"x\", \"Pr(>|t|)\"]\n    \n    if (pA <= alpha){\n      # Si le coefficient beta_21 est significatif, alors\n      # Troisième régression\n      # Est-ce que l'effet direct de m vers y est significatif?\n      etape3 <- lm(formula = y ~ x + m, data = donnees)\n      pB <- summary(etape3)$coefficients[\"m\", \"Pr(>|t|)\"]\n      \n      # Enregistrer le résultats, si l'effet de médiation a lieu\n      decision <- (pB <= alpha)\n      \n      if (decision){\n        # S'il y médiation, est-elle partielle ou complète?\n        # Est-ce que Beta_31 est significatif?\n        if (summary(etape3)$coefficients[\"x\", \"Pr(>|t|)\"] <= alpha){\n          # Si oui,\n          interpretation <- \"L'effet indirect est significatif \n          et la médiation est partielle\"\n        } else {\n          # Si non,\n          interpretation <- \"L'effet indirect est significatif \n          et la médiation est complète\"\n        }\n      }\n    }\n  }\n  cat(interpretation)\n}"},{"path":"médier.html","id":"la-méthode-delta-multivarié","chapter":" 16 Médier","heading":"16.2.2 La méthode delta multivarié","text":"La méthode delta multivarié souvent appelée le test de Sobel (1982), auteur qui l’popularisé pour l’analyse des effets indirects est une méthode ayant recours à l’erreur type approximative de l’effet indirect.Le test de Sobel se base sur le calcul selon lequel le ratio de l’effet indirect, \\(\\beta_{2,1}\\beta_{3,2}\\) par son erreur standard asymptotique \\(\\sqrt{\\beta_{3,2}^2\\sigma^2_{\\beta_{2,1}} +\\beta^2_{2,1}\\sigma^2_{\\beta_{3,2}}}\\) se distribue selon une distribution gaussienne. L’équation (16.4) représente ce calcule\\[\\begin{equation}\nz=\\frac{\\beta_{2,1}\\beta_{3,2}}{\\sqrt{\\beta_{3,2}^2\\sigma^2_{\\beta_{2,1}} +\\beta^2_{2,1}\\sigma^2_{\\beta_{3,2}}}}\n\\tag{16.4}\n\\end{equation}\\]où \\(z\\) signifie qu’il s’agit d’un score-\\(z\\), les \\(\\beta\\) sont les coefficients de régression et les \\(\\sigma^2_\\beta\\) sont les erreurs standard des coefficients de régression.La syntaxe suivante illustre une fonction qui calcule l’équation (16.4). La syntaxe calcule deux régressions m ~ x et m ~ x. Du sommaire des résultats, elle extrait, les deux coefficients de régressions et leur erreur standard respective. Elle calcule enfin le score \\(z\\) de l’équation (16.4) avec la valeur-\\(p\\) associée. Pour bien fonctionner avec cette fonction, les variables du jeu de données doivent se nommer x, m et y.La raison selon laquelle il ne faut pas recourir au test de Sobel est que le calcul est asymptotique. En fait, la distribution de l’effet indirect tend vers la normalité lorsque la taille d’échantillon est grande. Cela poussé les statisciens à développer des corrections pour ce test afin de l’amélioré. La distribution des effets indirects peut facilement se programmer avec R. En se basant sur le chapitre Simuler, il est possible de construire une petite illustration. D’abord, il faut créer une fonction qui crée des jeux de données, ici, gen.ind.effect() qui prend une matrice de covariance Sigma et une taille d’échantillon n. La fonction extrait ensuite les deux coefficients de régression et les multiplie. Noter la fonction coef() qui permet d’extraire plus simplement les coefficients de régression d’une sortie de lm() et unname() qui dénomme le résultat (facultatif, mais plus élégant pour la sortie)24.Par la suite, la fonction replicate() permet de répéter n fois la fonction expr. Il faut bien distinguer le n (nombre de participants) de gen.ind.effect() de celui de replicate() (nombre de réplications). Enfin, un histogramme est produit pour présenter les résultats.La Figure 16.2 montre la distribution des effets indirects obtenus avec la simulation. La ligne correspond à la distribution gaussienne sous-jacente au test de Sobel. Dans le cas n = 50 testé, il appert évident que les effets ne suivent pas exactement la distribution attendue. La distribution est centrée sur la bonne valeur \\(0.2 \\times 0.3 = 0.06\\). La distribution est toutefois asymétrique.\nFigure 16.2: Distribution de l’effet indirect\nLe chapitre Simuler présente une technique statistique toute désignée lorsque la disbribution statistique n’est pas connue25, il s’agit du bootstrap.","code":"\nmdm <- function(donnees, alpha = 0.05){\n  # alpha est l'erreur de type I\n  \n  # Réaliser les deux régressions\n  etape1 <- lm(formula = m ~ x, data = donnees)\n  etape2 <- lm(formula = y ~ x + m, data = donnees)\n  \n  # Extraire les statistiques\n  # Les coefficients de régression\n  beta_21 <- etape1$coefficients[\"x\"]\n  beta_32 <- etape2$coefficients[\"m\"]\n  # Les erreurs standards\n  SEa <- summary(etape1)$coefficients[\"x\", \"Std. Error\"]\n  SEb <- summary(etape2)$coefficients[\"m\", \"Std. Error\"]\n  \n  # Calcul du score z de l'effet indirect\n  SE <- sqrt(beta_21^2 * SEb^2 + beta_32^2 * SEa^2)\n  z <- beta_21 * beta_32 / SE\n  \n  # Décision\n  if(abs(z) >= qnorm(1 - alpha/2)){\n    cat(\"L'effet indirect est significatif\")\n    decision = TRUE\n  } else {\n    cat(\"L'effet indirect n'est pas significatif\")  \n    decision = FALSE\n  }\n}\n# Pour la reproductibilité\nset.seed(1442)\n\n# Matrice de covariance\nSigma <- matrix(c( 1, .2,  0,\n                  .2,  1, .3,\n                   0, .3,  1),\n                ncol = 3, nrow = 3,\n                dimnames = list(nom <- c(\"x\", \"m\", \"y\"), \n                                nom))\n\n# Créer une fonction qui génère des données\n# selon une matrice de covariance `Sigma`\n# et un nombre de participants `n`\ngen.ind.effect <- function(Sigma, n){\n  # Créer jeu de données\n  jd <- as.data.frame(MASS::mvrnorm(n =  n, \n                                    Sigma = Sigma, \n                                    mu = rep(0, ncol(Sigma))))\n  # Réaliser deux régressions\n  etape1 <- lm(formula = m ~ x, data = jd)\n  etape2 <- lm(formula = y ~ x + m, data = jd)\n  \n  # Extraire et multiplier les coefficients de régression\n  unname(coef(etape1)[\"x\"] * coef(etape2)[\"m\"])\n}\n# Répliquer 5000 la fonction `gen.ind.effect`\ntest.ind <- replicate(n = 5000, expr = gen.ind.effect(Sigma, n = 50))\n\n# Afficher l'histogramme des résultats\nhist(test.ind, prob = TRUE)"},{"path":"médier.html","id":"la-technique-du-bootstrap","chapter":" 16 Médier","heading":"16.2.3 La technique du bootstrap","text":"La technique la plus recommandée dans la littérature méthodologique est la méthode du bootstrap. Elle effectivement plusieurs avantages comparativement à ses adversaires. Elle ne présuppose par une distribution normale de l’effet indirect, ce que sous-entend la méthode delta multivarié. Elle une erreur de type et une puissance appropriée contrairement aux deux autres. Elle ne viole aucun postulat. Dans les faits, il est assez rare de voir des différences notables avec le test de Sobel. Elle demeure la technique recommandée.Le bootstrap se base sur trois étapes :Sélectionner aléatoirement et avec remplacement les unités d’un jeu de données;Sélectionner aléatoirement et avec remplacement les unités d’un jeu de données;Calculer l’indice statistique désirée;Calculer l’indice statistique désirée;Réitérer ces étapes un nombre très élevé de fois.Réitérer ces étapes un nombre très élevé de fois.Le test d’hypothèse de l’effet indirect n’échappe pas à cette logique.","code":"\nboot <- function(donnees, alpha = .05, nreps = 5000){\n  # alpha est l'erreur de type I\n  # nreps  est le nombre de répétitions\n  \n  # Informations nécessaire au bootstrap\n  # Nombre d'unités\n  n <- nrow(donnees)\n  # Variable vide pour enregistrer\n  effet.indirect <- as.numeric()\n  \n  # La boucle\n  for (i in 1:nreps){\n    # Sélectionner aléatoirement et avec remplacement\n    # les unités d'un jeu de données\n    index <- sample(n, replace = TRUE)\n    D <- jd[index,]\n    \n    # Calculer l'indice statistique désirée\n    b21 <- coef(lm(m ~ x, data = D))[\"x\"]\n    b32 <- coef(lm(y ~ x + m, data = D))[\"m\"]\n    \n    # Enregistrer les résultats de chaque boucle\n    effet.indirect[i] <- b21 * b32\n  }\n  \n  # Créer l'intervalle de confiance avec alpha \n  CI <- quantile(effet.indirect, \n                 probs = c(alpha/2, 1 - alpha/2))\n  \n  # Si l'intervalle ne contient pas 0, \n  # l'hypothèse nulle est rejetée.\n  if(prod(CI) > 0){\n    \n    cat(\"L'effet indirect est significatif\")\n    decision = TRUE\n    \n  }else{\n    \n    cat(\"L'effet indirect est non significatif\") \n    decision = FALSE\n    \n  }\n}"},{"path":"médier.html","id":"la-création-de-données-2","chapter":" 16 Médier","heading":"16.3 La création de données","text":"La création de données d’un modèle de médiation est assez rudimentaire, particulièrement après la lecture du chapitre Créer. En fait, dans ce chapitre, un modèle de médiation simple (à trois variables) est utilisé. Par contre, le chapitre n’insiste pas sur les effets indirects, ce qui sera fait ici.Pour l’exemple, \\(\\mathbf{B}\\) spécifie les coefficients de régression.\\[\\begin{equation}\n\\mathbf{B} =\n\\left( \\begin{array}{ccc}\n0 &  0 & 0 \\\\\n.5 &  0 & 0 \\\\\n-.3 & .4 & 0 \\\\\n\\end{array}\n\\right)\n\\tag{16.5}\n\\end{equation}\\]Le jeu de données est créé en suivant les étapes de chapitre Créer. Maintenant, il reste à déterminer l’effet indirect dans le jeu de données. Le jeu de données issu du système d’équations sera utilisé pour la suite.Comme les fonctions maisons pour la méthode d’étape causale (BK()), la méthode delta multivarié (mdm()) et la méthode bootstrap (boot()), il est possible de les utiliser pour vérifier l’effet indirect.Toutes les analyses confirment la présence d’un effet indirect.Le défaut des fonctions maison (BK(), mdm() et boot()) est certainement qu’elles ne font que rapporter la décision du tests des effets indirects. Une fonction plus intéressante serait d’afficher toutes les sorties, soit les coefficients de régressions, leur erreur type, leur intervalle de confiance ou toutes autres informations jugées pertinentes.","code":"\n# Pour la reproductibilité\nset.seed(1102)\n\n# Paramètres du modèle\n# Taille d'échantillon\nn <- 100\n\n# Matrice de coefficients de régression\nB <-  matrix(c( 0,  0, 0,\n               .5,  0, 0,\n               .3, .6, 0), \n             ncol = 3, nrow = 3,\n             byrow = TRUE,\n             dimnames = list(nom <- c(\"x\", \"m\", \"y\"), \n                             nom))\n# Variance des variables\nV = c(1, 1, 1) \n\n# Première option : \n# Créer de la matrice de covariance à partir de B et V\n# Vérifier que la fonction `beta2cov()` est \n# bien dans l'environnement sinon la ligne suivante\n# ne fonctionne pas.\nS <- beta2cov(B = B, V = V)   \n\n# Créer le jeu de données avec la matrice de covariance\njd <- MASS::mvrnorm(n = n, mu = c(0, 0, 0), Sigma = S)\njd <- as.data.frame(jd)\n\n# Deuxième option : \n# Création des données en système d'équations\n# Première étape\nx <- rnorm(n, sd = V[1])\n\n# Deuxième étape\nem <- rnorm(n = n, sd = sqrt(V[2] - B[2,1]^2 * V[1]))\nm <- B[2,1] * x + em\n\n# Troisième étape\nsd_ey = sqrt(V[3] - (B[3,1]^2 * V[1] + \n                     B[3,2]^2 * V[2] + \n                     2 * B[3,1] * B[3,2] * \n                     V[1]^.5 * V[2]^.5 * B[2,1]))\ney <- rnorm(n = n, sd = sd_ey)\ny <- B[3,1] * x + B[3,2] * m + ey\n\n# Créer le de données avec les trois variables\njd <- data.frame(x = x,\n                 m = m,\n                 y = y)BK(donnees = jd)\n> L'effet indirect est significatif \n>           et la médiation est partielle\nmdm(donnees = jd)\n> L'effet indirect est significatif\nboot(donnees = jd)\n> L'effet indirect est significatif"},{"path":"médier.html","id":"analyse-complète","chapter":" 16 Médier","heading":"16.4 Analyse complète","text":"La fonction suivante extrait tous les coefficients de régression d’un modèle récursif. L’ordre des variables est ici d’une énorme importance, puisque c’est l’ordre des variables dans le jeu de données qui détermine l’ordre causal des variables: la première étant la cause de toutes, et la dernière l’effet de toutes.Pour tester ce code, il vaut la peine de tester chacune des étapes de la syntaxe précédente avec une matrice de covariance.Transformer la matrice de covariance en matrice de coefficient de régression.Vectoriser BETA.Libeller les effets directs.Lister tous les effets indirects possiblesDans cet exemple, il y deux niveaux d’effets indirects : un niveau à trois variables dont il y quatre combinaisons possibles et un second niveau à quatre variables dont il n’y qu’une combinaison.Extraire tous les effets indirects et les libeller adéquatement.Calculer les effets totaux de la première variable et mettre le tout en commun.Magnifique! Toutefois, le statisticien ne s’intéresse rarement qu’aux coefficients de régression. Il aime aussi connaître l’erreur type (erreur standard), la valeur-\\(p\\), ou peut-être même souhaite-t-il calculer un intervalle de confiance. Une solution bien simple qui ne nécessitera que peu de syntaxe, en plus de respecter les postulats sous-jacents à l’analyse de médiation est le bootstrap. Les éléments fondamentaux du bootstrap sont toujours les mêmes : prendre un jeu de données avec des unités rééchantillonnées aléatoirement avec remplacement, calculer les indices désirés, et réitérer un nombre élevé de fois.Avantageusement la fonction maison indirect() calcule tous les indices statistiques pertinents. Il ne reste que le rééchantillonnage et les réplications à programmer.La variable Resultats contient tous les résultats pertinents. La colonne Resultats$Estimates retourne tous les coefficients de régression avec leur erreur type (erreur standard ou standard error) en deuxième colonne. Les dernières colonnes donnent les intervalles de confiance inférieurs et supérieurs. Comme aucune ne contient la valeur 0 au sein de son intervalle, alors elles sont toutes significatives.\nFigure 16.3: Résultats de l’analyse de médiation\nLes résultats sont illustrés dans la Figure 16.3. Les coefficients sont ajoutés à leurs trajectoires respectivement. Par bonnes mesures, des étoiles de significativité, *, l’ultime symbole de découvertes scientifiques, sont ajoutés aux trajectoires dont les intervalles de confiance n’incluent pas 0. Il ne reste qu’à rapporter l’effet indirect dans le texte ou un tableau d’un article scientifique, comme le tableau 16.1.\nTable 16.1: Résultats de l’analyse de médiation\nCalculer les valeurs-\\(t\\) et valeurs-\\(p\\) est envisageable en utilisant les résultats déjà recueillis. La valeur-\\(t\\) est le ratio entre l’estimateur et son erreur type, la valeur-\\(p\\) est la rareté d’observer cette valeur-\\(t\\) ou une valeur plus rare par rapport à l’hypothèse nulle avec un degré de liberté de dl = n - p, soit plus exactement le nombre d’unités moins le nombre de variables indépendantes.Si les coefficients de régression standardisés sont préférés, ceux-ci s’obtiennent simplement en standardisant le jeu de données, puis en roulant l’analyse de médiation de nouveau. Pour standardiser rapidement, z.donnees = apply(donnees, MARGIN = 2, FUN = scale) applique (apply()) la fonction FUN = scale qui standardise les données (donnees) par colonne MARGIN = 2.","code":"\nindirect <-  function(donnees){\n  COV <- cov(donnees)   # Matrice de covariance\n  p <- ncol(COV)        # Nombre de variables\n  \n  # Calculer la matrice des coefficients de\n  # régression. Le lecteur assidu aura reconnu \n  # la fonction `cov2beta()`\n  BETA <- matrix(0, p, p)  \n  for(i in 1:(p-1)){\n    R <- solve(COV[1:i,1:i], COV[1+i,1:i])\n    BETA[i+1, 1:i] <- R\n  }\n  \n  # Extraire les coefficients de régression en vecteur\n  est <- as.matrix(BETA[lower.tri(BETA)])\n  \n  # Libellés des effets directs\n  name <- colnames(COV)\n  label <-matrix(name[combn(p, 2)], (p * (p-1) / 2), 2, byrow = TRUE)\n  rname <- apply(FUN = paste, as.matrix(label[,1]), MARGIN = 2, \"->\")\n  rname <- apply(FUN = paste, rname, label[,2], MARGIN = 2, \"\")\n  row.names(est) <- rname\n  \n  # Lister tous les effets indirects possibles\n  if(p != 3){\n    # S'il y a plus de 3 variables, \n    # identifier les niveaux supérieurs\n    # d'effets indirects (à 4 variables et plus)\n    listeffects <- mapply(combn, p, 3:p)\n  } else {\n    # S'il y a 3 variables, il n'y a qu'un niveau\n    listeffects <- list((matrix(1:3, 3, 1)))\n  }\n  \n  \n  # Extraire tous les effets indirects\n  for(i in 1:length(listeffects)){    # Nombre de niveaux d'effet indirect\n    J <- ncol(listeffects[[i]])       # Nombre d'effets du niveau i\n    for(j in 1:J){                \n      ide <- listeffects[[i]][,j]     # Identifier l'effet en cours\n      B <- BETA[ide, ide]             # Leur coefficients de régression \n      B <- B[-1, -ncol(B)]            # Retirer les coefficients superflus\n      e <- as.matrix(prod(diag(B)))   # Calculer l'effet indirect\n      rownames(e) <- paste(name[ide], # Le libellé\n                           collapse = \" -> \")\n      est <- rbind(est, e)            # Ajouter l'effet aux autres\n    }\n  }\n  \n  # Ajout les effets totaux\n  # Calculs\n  totald <- as.matrix(solve(COV[1,1], COV[p, 1]))\n  totali <- as.matrix(totald - BETA[p, 1])\n  \n  # Libeller\n  rownames(totali) <- paste(\"total indirect\",\n                            colnames(COV)[1],\n                            \"->\",\n                            colnames(COV)[p])\n  rownames(totald) <- paste(\"total effect\",\n                            colnames(COV)[1],\n                            \"->\",\n                            colnames(COV)[p])\n  \n  # Mettre le tout en commun\n  estimates <- rbind(est, totali, totald)\n  return(estimates)\n}# Nombre de variables\np <- 4\n\n# Voici la matrice de covariance utilisée\nCOV <- matrix(c(3, 2, 1, 4,\n                2, 6, 2, 5,\n                1, 2, 5, 1,\n                4, 5, 1, 4), ncol = p, nrow = p)\ncolnames(COV) <- letters[1:p]\nrownames(COV) <- letters[1:p]\n\n# Normalement, celle-ci serait obtenue d'un échantillon\nCOV\n>   a b c d\n> a 3 2 1 4\n> b 2 6 2 5\n> c 1 2 5 1\n> d 4 5 1 4# Les prochaines calcules la matrice de coefficients\n# régression. Le lecteur assidu aura reconnu \n# la fonction `cov2beta()`\nBETA <- matrix(0, p, p)  \nfor(i in 1:(p-1)){\n  R <- solve(COV[1:i,1:i], COV[1+i,1:i])\n  BETA[i+1, 1:i] <- R\n}\nBETA\n>       [,1]  [,2]   [,3] [,4]\n> [1,] 0.000 0.000  0.000    0\n> [2,] 0.667 0.000  0.000    0\n> [3,] 0.143 0.286  0.000    0\n> [4,] 1.033 0.567 -0.233    0# Extraire les coefficients de régression en vecteur\nest <- as.matrix(BETA[lower.tri(BETA)])\nest\n>        [,1]\n> [1,]  0.667\n> [2,]  0.143\n> [3,]  1.033\n> [4,]  0.286\n> [5,]  0.567\n> [6,] -0.233# Libellés des effets directs\nname <- colnames(COV)\nlabel <- matrix(name[combn(p, 2)], (p * (p-1) / 2), 2, byrow = TRUE)\nrname <- apply(FUN = paste, as.matrix(label[,1]), MARGIN = 2, \"->\")\nrname <- apply(FUN = paste, rname, label[,2], MARGIN = 2, \"\")\nrow.names(est) <- rname\n# Beaucoup de syntaxe pour au final bien peu, mais\n# le résultat est élégant\nest\n>           [,1]\n> a -> b   0.667\n> a -> c   0.143\n> a -> d   1.033\n> b -> c   0.286\n> b -> d   0.567\n> c -> d  -0.233# Lister tous les effets indirects possibles\nif(p != 3){\n  # S'il y a plus de 3 variables, identifier les niveaux supérieurs\n  # d'effets indirects (à 4 variables et plus)\n  listeffects <- mapply(combn, p, 3:p)\n} else {\n  # S'il y a 3 variables, il n'y a qu'un niveau\n  listeffects <- list((matrix(1:3, 3, 1)))\n}\nlisteffects\n> [[1]]\n>      [,1] [,2] [,3] [,4]\n> [1,]    1    1    1    2\n> [2,]    2    2    3    3\n> [3,]    3    4    4    4\n> \n> [[2]]\n>      [,1]\n> [1,]    1\n> [2,]    2\n> [3,]    3\n> [4,]    4# Extraire tous les effets indirects\nfor(i in 1:length(listeffects)){    # Nombre de niveaux d'effet indirect\n  J <- ncol(listeffects[[i]])       # Nombre d'effet du niveau i\n  for(j in 1:J){                \n    ide <- listeffects[[i]][,j]     # Identifier l'effet en cours\n    B <- BETA[ide, ide]             # Leur coefficients de régression \n    B <- B[-1, -ncol(B)]            # Retirer les coefficients superflus\n    e <- as.matrix(prod(diag(B)))   # Calculer l'effet indirect\n    rownames(e) <- paste(name[ide], # Le libellé\n                         collapse = \" -> \") \n    est <- rbind(est, e)            # Ajouter l'effet aux autres\n  }\n}\nest\n>                     [,1]\n> a -> b            0.6667\n> a -> c            0.1429\n> a -> d            1.0333\n> b -> c            0.2857\n> b -> d            0.5667\n> c -> d           -0.2333\n> a -> b -> c       0.1905\n> a -> b -> d       0.3778\n> a -> c -> d      -0.0333\n> b -> c -> d      -0.0667\n> a -> b -> c -> d -0.0444# Ajout les effets totaux\n# Calculs\ntotald <-  as.matrix(solve(COV[1,1], COV[p, 1]))\ntotali <-  as.matrix(totald - BETA[p, 1])\n\n# Libeller\nrownames(totali) <- paste(\"total indirect\",\n                          colnames(COV)[1],\n                          \"->\",\n                          colnames(COV)[p])\nrownames(totald) <- paste(\"total effect\",\n                          colnames(COV)[1],\n                          \"->\",\n                          colnames(COV)[p])\n\n# Mettre le tout en commun\nestimates <- rbind(est, totali, totald)\nestimates\n>                          [,1]\n> a -> b                 0.6667\n> a -> c                 0.1429\n> a -> d                 1.0333\n> b -> c                 0.2857\n> b -> d                 0.5667\n> c -> d                -0.2333\n> a -> b -> c            0.1905\n> a -> b -> d            0.3778\n> a -> c -> d           -0.0333\n> b -> c -> d           -0.0667\n> a -> b -> c -> d      -0.0444\n> total indirect a -> d  0.3000\n> total effect a -> d    1.3333# Le bootstrap de `indirect()` pour le \n# jeu de données en exemple (trois variables)\n# Informations préliminaires\nalpha <- .05         # Erreur de type I\nn <- nrow(jd)        # Nombre d'unités\nreps <- 5000         # Nombre de réplications\n\n# Vérifier que la fonction `indirect()`  est bien \n# dans l'environnement\nEst <- indirect(jd)\n\n# Variable vide (Est) pour enregistrer les résultats\n# avec comme 1ere colonne, les résultats originaux\nEst <- data.frame(Est = Est, \n                   X = matrix(0, ncol = reps)) \n\n# La boucle\n# Elle commence à 2 à cause de la première colonne\nfor(i in 2:(reps+1)){\n  index <- sample(n, replace = TRUE)\n  D <- jd[index,]\n  Est[,i] <-  indirect(D)\n}\n\n# Mettre le tout en commun\nResultats <- data.frame(\n  Estimates = Est$Est,\n  S.E. = apply(Est, 1, FUN = sd), \n  CI.inf = apply(Est, 1, FUN = quantile, probs = alpha/2),\n  CI.sup = apply(Est, 1, FUN = quantile, probs = 1-alpha/2)\n)\nResultats\n>                       Estimates   S.E. CI.inf CI.sup\n> x -> m                    0.336 0.0930 0.1602  0.521\n> x -> y                    0.312 0.0616 0.1974  0.442\n> m -> y                    0.625 0.0731 0.4786  0.765\n> x -> m -> y               0.210 0.0608 0.0976  0.333\n> total indirect x -> y     0.210 0.0608 0.0976  0.333\n> total effect x -> y       0.521 0.0843 0.3641  0.691Resultats$t.value <- Resultats$Estimates / Resultats$S.E.\nResultats$p.value <- (1 - pt(abs(Resultats$t.value), df = n - p)) * 2\n\n# Résultats arrondis à 2 décimales\nround(Resultats, 2)\n>                       Estimates S.E. CI.inf CI.sup t.value\n> x -> m                     0.34 0.09   0.16   0.52    3.61\n> x -> y                     0.31 0.06   0.20   0.44    5.06\n> m -> y                     0.63 0.07   0.48   0.76    8.55\n> x -> m -> y                0.21 0.06   0.10   0.33    3.45\n> total indirect x -> y      0.21 0.06   0.10   0.33    3.45\n> total effect x -> y        0.52 0.08   0.36   0.69    6.19\n>                       p.value\n> x -> m                      0\n> x -> y                      0\n> m -> y                      0\n> x -> m -> y                 0\n> total indirect x -> y       0\n> total effect x -> y         0"},{"path":"médier.html","id":"les-packages-2","chapter":" 16 Médier","heading":"16.5 Les packages","text":"Le présent chapitre ne fait que gratter la surface de ce qu’il est possible de faire avec l’analyse de médiation. Des articles comme Caron & Valois (2018) et Lemardelet & Caron (2022) donnent des exemples de syntaxe R en plus d’approfondir l’analyse. Il existe plusieurs packages R pour réaliser l’analyse de médiation, comme mediation (Tingley et al., 2014) et Rmediation (Tofighi & MacKinnon, 2011), tous les deux ayant leur propre documentation. Pour des analyses plus compliquées, les packages comme lavaan (Rosseel, 2012) permettent de faire des analyses de médiation avec la modélisation par équations structurelles, Toutefois, ce chapitre espère avoir convaincu le lecteur que l’analyse peut être relativement aisément fait maison.","code":""},{"path":"médier.html","id":"rapporter-lanalyse-de-médiation","chapter":" 16 Médier","heading":"16.6 Rapporter l’analyse de médiation","text":"Il existe une version préliminaire d’un package permettant de réaliser facilement l’analyse de médiation. Le package est pathanalysis (Caron, 2021). Il est disponible par GitHub et est importable sur R avec la syntaxe suivante.Comme à l’habitude, si le package est déjà téléchargé, il faut l’appeler dans l’environnement.Le package pathanalysis fourni la fonction mediation() qui permet d’obtenir le modèle saturé de l’analyse de médiation. Pour commander, le modèle désiré, il faut ordonner les variables de cette façon : des variables endogènes qui ne prédisent aucune variable, les variables endogènes qui prédisent des variables, jusqu’au variables exogènes (celles qui ne sont prédites par aucune variable). Il faut délimiter chaque variable avec le symbole ~.L’exemple suivant, basé sur le jeu de données mtcars, teste le lien indirect de wt (variable exogène) à mpg (variable endogène) par l’intermédiaire de hp. Les résultats standardisés sont demandés avec l’argument standardized = TRUE.Voici comment ces résultats peuvent être rapportés dans un article scientifique.Une analyse de médiation avec bootstrap est effectuée pour tester le lien indirect le lien indirect de wt à mpg par l’intermédiaire de hp. Les résultats montrent un lien indirect significatif de -0.238, [95%IC -0.397, -0.137]. La Figure 16.4 présente les résultats.\nFigure 16.4: Résultats de l’analyse de médiation\nEn plus de ces informations primordiales, certains domaines de recherche demanderont, en plus, de rapporter les deux régressions réalisées (voir Rapporter la régression), soit les liens directs pertinents et les modèles de régression, et également, l’aspect partiel ou complet de la médiation26.","code":"\nremotes::install_github(repo = \"quantmeth/pathanalysis\")\nlibrary(pathanalysis)mediation(model = mpg ~ hp ~ wt, \n          data = mtcars, \n          standardized = TRUE)\n>                          Estimate  S.E. CI Lower 95 %\n> wt -> hp                    0.659 0.099         0.524\n> wt -> mpg                  -0.630 0.115        -0.874\n> hp -> mpg                  -0.361 0.087        -0.558\n> wt -> hp -> mpg            -0.238 0.065        -0.393\n> total indirect wt -> mpg   -0.238 0.065        -0.393\n> total effect wt -> mpg     -0.868 0.115        -1.135\n>                          CI Upper 95 % p-value\n> wt -> hp                         0.916       0\n> wt -> mpg                       -0.414       0\n> hp -> mpg                       -0.224       0\n> wt -> hp -> mpg                 -0.138       0\n> total indirect wt -> mpg        -0.138       0\n> total effect wt -> mpg          -0.680       0"},{"path":"modérer.html","id":"modérer","chapter":" 17 Modérer","heading":" 17 Modérer","text":"En plus des relations linéaires, c’est-à-dire des effets selon lesquels plus une variable augmente (ou diminue) plus une seconde variables augmente (ou diminue), il existe des variables qui interagissent de sorte à accentuer ou amenuiser la force d’un effet, voire même à altérer sa direction. Ce type d’effet, appelée modérateur, est analysé à l’aide d’analyse de modération, analyse statistique très populaire dans laquelle la relation entre la variable dépendante (\\(y\\)) et la variable indépendante (\\(x\\)) est altérée par une tierce variable, la variable modératrice (\\(w\\)).\nBien que la modération ait son chapitre désigné, elle déjà été rencontrée dans ce livre, notamment, au chapitre Prédire. L’analyse de modération dans son essence correspond à étudier un effet d’interaction dans un modèle linaire, ou autrement dit, la multiplication entre deux variables, ici \\(x \\times w\\). Ainsi, en recourant aux fonctions lm() ou aov(), la modération se commande en employant le symbole * comme lm(y ~ x * w) ou aov(y ~ x * w). Toutefois, ayant ses défis qui lui sont propres en création de données et en interprétation, le chapitre suivant propose une description approfondie de la modération27.","code":""},{"path":"modérer.html","id":"lanalyse-de-modération","chapter":" 17 Modérer","heading":"17.1 L’analyse de modération","text":"Sur le plan quantitatif, l’analyse de modération est un modèle linéaire général réalisé en une seule étape (soit une analyse de la variance ou une régression, en fonction des variables concernées) dans laquelle une variable dépendante, \\(y\\), est prédite par trois variables, la variable indépendante, \\(x\\), l’effet simple de la variable modératrice, \\(w\\), et leur produit \\(x \\times w=xw\\). En termes du modèle linéaire général (ANOVA, régression), un modérateur se subsume à une interaction. Mathématiquement, la relation s’exprime comme l’équation (17.1)\\[\\begin{equation}\ny = \\beta_0 + \\beta_1 x + \\beta_2 w + \\beta_3 (x \\times w) + \\epsilon\n\\tag{17.1}\n\\end{equation}\\]où les \\(\\beta\\) sont les coefficients de régression reliant la variable correspondante à la variable dépendante.\nFigure 17.1: Représentations de la modération\nL’analyse de modération est illustrée à la Figure 17.1. Le cadran (gauche) montre une conceptualisation de l’effet attendu du modérateur, soit l’altération de l’effet entre \\(x\\) et \\(y\\). Le cadran b (droite) montre, quant à lui, la représentation statistique dans laquelle le lien direct entre \\(x\\), \\(w\\) et \\(xw\\) par rapport à \\(y\\).Une façon de mettre l’accent sur l’effet de modération est de réarranger l’équation (17.1) afin d’obtenir l’équation (17.2), ce qui met en évidence le rôle clé de \\(w\\).\\[\\begin{equation}\ny = \\beta_0 + \\beta_1 x + (\\beta_2 + \\beta_3 x) w + \\epsilon\n\\tag{17.2}\n\\end{equation}\\]L’équation (17.2) montre comment \\(\\beta_3\\) altère la relation entre \\(w\\) et \\(y\\).","code":""},{"path":"modérer.html","id":"création-de-données","chapter":" 17 Modérer","heading":"17.2 Création de données","text":"Une caractéristique fondamentale de la création d’interactions est qu’elles doivent être créées à partir de ces composantes. Autrement dit, le produit \\(xw\\) comme variable n’est calculable que si \\(x\\) ou \\(w\\) sont d’abord créées. Il faut connaître deux des trois variables pour calculer la troisième. En conséquence, il n’est pas possible de créer des variables d’interaction, comme c’était le cas auparavant avec la fonction MASS::mvrnorm(), malgré que la matrice de covariance et les coefficients de régression soient calculables priori.La variance du produit de deux variables peut devenir rapidement compliquée. En fait, l’étude des produits de variables est si complexe, particulièrement lorsque les variables ont des moyennes non nulles qu’il n’y eu que très dernièrement des développements mathématiques sur leur distribution formelle (Cui et al., 2016; Nadarajah & Pagány, 2016), ce pourquoi les statisticiens préfèrent recourir généralement au bootstrap pour tester les effets médiateurs.La variance du produit de deux variables standardisées (Craig, 1936; Haldane, 1942) correspond à l’équation (17.3).\\[\\begin{equation}\n\\sigma^2_{x \\times w} = 1 + \\rho_{xw}^2\n\\tag{17.3}\n\\end{equation}\\]Pour des variables centrées (non complètement standardisées), l’équation (17.3) devient l’équation (17.4) (Craig, 1936; Haldane, 1942).\\[\\begin{equation}\n\\sigma^2_{x \\times w} = \\mu_{x}\\mu_{w}((\\frac{\\sigma_{x}}{\\mu_{x}})^2+\n2\\rho_{xw}\\frac{\\sigma_{x}}{\\mu_{x}}\\frac{\\sigma_{w}}{\\mu_{w}} + (\\frac{\\sigma_{w}}{\\mu_{w}})^2+\\\\(1+\\rho^2)(\\frac{\\sigma_{x}}{\\mu_{x}})^2(\\frac{\\sigma_{w}}{\\mu_{w}})^2)\n\\tag{17.4}\n\\end{equation}\\]Celle-ci n’est pas des plus attrayante à l’oeil, surtout pour les arithmophobes. L’essentiel : il faut porter une attention particulière à la variance de \\(xw\\) lorsque la variable \\(y\\) est générée, car celle-ci est dépendante de la moyenne et de la variance de ces composantes.","code":""},{"path":"modérer.html","id":"un-modérateur-continu","chapter":" 17 Modérer","heading":"17.2.1 Un modérateur continu","text":"Dans le cas de deux variables continues (une variable indépendante \\(x\\) et une modératrice \\(w\\)), celles-ci peuvent être créées avec un système d’équations ou bien avec la matrice de covariance (des deux variables). Une fois \\(x\\) et \\(w\\) générées, il faut les multiplier pour obtenir \\(xw\\). C’est l’étape cruciale qui distingue la modération des autres analyses, car les deux variables doivent être créer avant l’interaction.La variance de \\(xw\\) suit l’équation (17.4) et les covariances avec les premières variables sont nulles dans la mesure où ces variables sont symétriques28. La matrice de covariance (et de corrélation dans ce cas-ci) est ainsi calculable priori.\\[\\begin{equation}\n\\mathbf{\\Sigma} =\n\\left( \\begin{array}{ccc}\n1 & \\rho_{xw} & 0 \\\\\n\\rho_{xw} & 1 & 0 \\\\\n0 & 0 & 1+\\rho_{xw}^2 \\\\\n\\end{array}\n\\right)\n\\tag{17.5}\n\\end{equation}\\]En spécifiant les coefficients de régression à l’avance avec \\(\\mathbf{B}\\), la variance résiduelle de la variable dépendante peut être calculée comme l’équation (15.20) afin d’assurer un scénario standardisé, rappelée ici.\\[\n\\sigma^2_{\\epsilon} = 1 - \\mathbf {B^{\\prime} \\Sigma B}\n\\]Le code ci-dessus montre un scénario standardisé (toutes les moyennes sont 0; toutes les variances égalent 1, sauf l’interaction). Le modèle est illustré à la Figure 17.2.\nFigure 17.2: Exemple de modèle de modération\nPour simuler une étude réelle, la variable d’interaction n’pas à être enregistrée dans les deux jeux de données. Elle n’était nécessaire que pour la création de la variable dépendante.","code":"\n# Pour la reproductibilité\nset.seed(1302)\n\n# Tailles d'échantillon\nn <- 500\n\n# Covariance entre x et w\nrhoxw  <-  .5    \n\n# Matrice de covariance a priori\nS <- matrix(c(1, rhoxw, 0,   \n              rhoxw, 1, 0,\n              0, 0, 1 + rhoxw^2),\n            ncol = 3, nrow = 3)\n\n# Coefficients de régression de x, w et xw\nB <- c(.1, .2, .3)             \n\n# Création des deux premières variables\nX <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = S[1:2, 1:2])\n\n# Ajouter en joignant aux deux autres leur produit\nX <- cbind(X, X[,1] * X[,2])\n\n# Calculer la variance résiduelle\nvar_ey <- (1 - B %*% S %*% B)\n\n# Création de la variable dépendante\ny <- X %*% B + rnorm(n = n, sd = sqrt(var_ey))\n\n# Mettre le tout en commun et \n# renommer les variables\njd.continue <- data.frame(x = X[,1],\n                          w = X[,2],\n                          xw = X[,3],\n                          y = y)\n# Jeu de données final \njd.continue <- jd.continue[, c(\"x\",\"w\",\"y\")]"},{"path":"modérer.html","id":"un-modérateur-nominal","chapter":" 17 Modérer","heading":"17.2.2 Un modérateur nominal","text":"Pour créer un jeu de données avec une variable indépendante continue et une variable modératrice nominale, la procédure sera similaire à celle de l’analyse de variance avec le Codage factice en combinaison avec la technique décrite ci-haut.En considérant \\(p_i = \\frac{g_i}{n}\\) comme la probabilité d’une unité d’être dans le groupe \\(\\), la variance d’un groupe, représentée par \\(w_i\\), est \\(p_i(1-p_i)\\), soit la variance d’une distribution binomiale pour une probabilité \\(p_i\\). La covariance avec un autre groupe \\(j\\) est de \\(-p_ip_j\\), la probabilité d’être dans un groupe est négativement liée à celle d’être dans un autre groupe. Enfin, la variance d’une interaction est de \\(p_i\\), car il s’agit du pourcentage de \\(x\\) qui se retrouve dans l’interaction \\(xw_i\\). En conséquence, comme il s’agit de la même information (à cause du multiple de 1 de la variable \\(w_i\\), c’est-à-dire appartenir au groupe \\(\\)), cela mène une covariance accidentelle de \\(p_i\\) entre \\(x\\) et \\(w_i\\).Toutefois, il sera plus simple de standardiser W avant de créer les interactions, ce qui éliminera les covariances accidentelles.Alors la covariance entre \\(w_i\\) et \\(w_j\\) et entre \\(xw_i\\) et \\(xw_j\\) deviennent\n\\[p_1 p_2 / \\sqrt{p_1(1 - p_1) p_2 (1 - p_2)}\\]\nsoit leur covariance originale divisée par leur écart type. Tous les autres valeurs deviennent 1 dans le cas des variances et 0 pour les covariances.Il est possible d’ajouter des corrélations entre une variable continue et les groupes, mais cela déborde du cadre de ce livre et des connaissances détaillées jusqu’ici. Mais, c’est possible!Maintenant, il reste à créer la variable dépendante. Pour ce faire, il faut spécifier cinq coefficients de régressions, car une variable continue avec deux variables modératrices (facteurs transformés par le codage factice) donnent deux variables d’interaction.Pour simuler une étude réelle, les variables d’interactions et le codage factices peuvent être écartés du jeu de données. Ces variables ne sont nécessaires que pour la création de la variable dépendante.","code":"\n# Pour la reproductibilité\nset.seed(50015610)  \n\n# Tailles des trois groupes (pourraient être différentes)\ng1 <- g2 <- g3 <- 5000\n\n# Nombre total d'unités (somme de tous les groupes)\nn <- sum(c(g1, g2, g3))\n\n# Création de variables\n# Variable continue\nx <-  rnorm(n)  # Variable continue\n# Vecteur de groupement (facteur)\nw <-  as.factor(c(rep(1, g1), rep(2, g2), rep(3, g3)))\n\n\n# Codage factice du groupement\n# Le troisième groupe (retiré) est le référent\nW <- sapply(unique(w),\n            USE.NAMES = TRUE,\n            FUN = function(x) {w == x}) * 1 \nW <- W[,-3]\n\n# Mettre tous les prédicteurs ensemble \n# pour le modèle linéaire sous forme de matrice\nX = cbind(x = x,\n          w = W,\n          xw = x * W)p1 <- g1/n\np2 <- g2/n\nS <- matrix(c(1, 0, 0, p1, p2,\n              0, p1*(1-p1), -p1*p2, 0, 0,\n              0, -p1*p2, p2*(1-p2), 0, 0,\n              p1, 0, 0, p1, 0,\n              p2, 0, 0, 0, p2), \n            ncol = 5, nrow = 5)\nS\n>       [,1]   [,2]   [,3]  [,4]  [,5]\n> [1,] 1.000  0.000  0.000 0.333 0.333\n> [2,] 0.000  0.222 -0.111 0.000 0.000\n> [3,] 0.000 -0.111  0.222 0.000 0.000\n> [4,] 0.333  0.000  0.000 0.333 0.000\n> [5,] 0.333  0.000  0.000 0.000 0.333\n# Vérifications\ncov(X)\n>          x                                      \n> x  0.97900 -0.00703 -0.00136  3.18e-01  3.21e-01\n>   -0.00703  0.22224 -0.11112 -6.21e-03  1.21e-03\n>   -0.00136 -0.11112  0.22224  3.10e-03 -2.43e-03\n>    0.31815 -0.00621  0.00310  3.18e-01 -3.39e-05\n>    0.32067  0.00121 -0.00243 -3.39e-05  3.21e-01\n# Standardiser les codes factices\nW <- scale(W)\n\n# Mettre tous les prédicteurs ensemble \n# pour le modèle linéaire sous forme de matrice\nX2 <- cbind(x = x,\n            w = W,\n            xw = x * W)# La covariance\nrho = -(p1 * p2) / (sqrt((p1 * (1 - p1)) * (p2 * (1 - p2))))\n\n# La matrice de covariance\nS2 <- matrix(c(1, 0, 0, 0, 0,\n               0, 1, rho, 0, 0,\n               0, rho, 1, 0, 0,\n               0, 0, 0,1, rho,\n               0, 0, 0, rho, 1), \n             ncol = 5, nrow = 5)\nS2\n>      [,1] [,2] [,3] [,4] [,5]\n> [1,]    1  0.0  0.0  0.0  0.0\n> [2,]    0  1.0 -0.5  0.0  0.0\n> [3,]    0 -0.5  1.0  0.0  0.0\n> [4,]    0  0.0  0.0  1.0 -0.5\n> [5,]    0  0.0  0.0 -0.5  1.0\n\n# Vérifier\ncov(X2)\n>          x                                  \n> x  0.97900 -0.0149 -0.00288 -0.0174 -0.01201\n>   -0.01492  1.0000 -0.50000 -0.0174  0.01601\n>   -0.00288 -0.5000  1.00000  0.0160 -0.00887\n>   -0.01736 -0.0174  0.01601  0.9666 -0.46885\n>   -0.01201  0.0160 -0.00887 -0.4689  0.97049\n# Les coefficients de régression\nB <-  c(.25, 0, 0, .50, -.50)\n\n# La variance résiduelle\n# La deuxième matrice de covariance est utilisée\nvar_ey <- 1 - B %*% S2 %*% B  \n\n# Création de la variable dépendante\ny <- X2 %*% B + rnorm(n = n, sd = sqrt(var_ey))\n# Jeu de données final \njd.nominale <-  data.frame(x = x,\n                           w = w,\n                           y = y)"},{"path":"modérer.html","id":"détecter-leffet-de-modération","chapter":" 17 Modérer","heading":"17.3 Détecter l’effet de modération","text":"Pour détecter les effets modérateurs, l’analyse de modèle linéaire par lm() et aov() jouera parfaitement le rôle. Par contre, il faut noter une différence cruciale entre les deux fonctions. Le calcul de la somme des carrés est différent entre les deux : lm() utilise le type III et aov() utilise le type .Le choix du type porte à controverse (Herr, 1986). Pour choisir le type d’estimateur, de nombreux facteurs entrent en ligne de compte, le plus important étant l’hypothèse de recherche. Il n’y donc pas de réponse claire; il y bien quelques recommandations.Le type teste séquentiellement le modèle \\(y = x + w + xw\\), soit l’effet de \\(x\\), puis l’effet de \\(w\\) en tenant compte de \\(x\\) (écrit \\(\\text{SC}(w|x)\\), soit la somme des carrés de \\(w\\) considérant \\(x\\)), et enfin l’effet de \\(x\\times w\\) par \\(\\text{SC}(xw|x,w)\\). Il teste très bien les termes d’interaction, en plus, d’offrir un modèle alternatif si l’interaction n’est pas significative. Le type ne traite pas les effets simples de façon égale. L’ordre importe! Le type II contourne le traitement différentiel des effets simples en considérant simultanément des effets de même niveau, comme \\(\\text{SC}(w|x)\\) et \\(\\text{SC}(x|w)\\), ce qui est très bien pour tester les effets simples s’il n’y pas de terme d’interaction. Le type III conditionne tous les effets, \\(x\\), \\(w\\) et \\(xw\\), au même niveau, soit \\(\\text{SC}(x|w,xw)\\), \\(\\text{SC}(w|x,xw)\\) et \\(\\text{SC}(xw|x,w)\\). Tous les effets sont traités de façon équivalente. Le type IV, pour le mentionner, permet de tenir compte de combinaison de groupes nulle (ou vide). Il est équivalent au type III s’il y au moins une valeur dans chaque groupe.La controverse provient du fait que les hypothèses concernées ne sont pas très utiles pour les expérimentateurs puisqu’ils s’intéressent au terme d’interaction, et non aux effets principaux lorsqu’un terme d’interaction est présent. L’avantage du type est de montrer les modèles possibles de façon hiérarchique, ce que ne fait pas le type III. Le type oblige de la réflexion : la séquence théorique des variables d’une part, la séquence de des hypothèses d’intérêt d’autre part.S’il y un effet d’interaction, alors les effets simples ne sont pas interprétés. Le type III est alors inutile pour interpréter ces effets.Les programmeurs de R championnent cette perspective en définissant l’option de type de sommes de carrés par défaut pour les fonctions lm() et aov(). Ils y vont de mots très durs à l’endroit des logiciels qui recourent automatiquement au type III.Le choix entre type ou type III revient à l’expérimentateur. Par contre, la plupart des ouvrages statistiques pour les sciences appliquées utilisent implicitement le type III. D’autres le recommandent activement. De plus, lorsque le devis n’est pas balancé, le type III est préférable. Si le lecteur veut comparer des analyses de modération produites dans d’autres ouvrages, il doit recourir au type III.Cela dit, il demeure possible d’utiliser aov() avec les SC de type III, mais il faudra recourir à la fonction Anova() du package car (Fox & Weisberg, 2019).","code":""},{"path":"modérer.html","id":"analyse-dun-modérateur-continu","chapter":" 17 Modérer","heading":"17.3.1 Analyse d’un modérateur continu","text":"Pour le modèle avec deux variables continues, la syntaxe pour rouler lm() est très similaire à ce qui se trouve dans le chapitre Prédire. Pour inclure l’effet d’interaction (modération) dans le modèle, il faut inscrire le * au lieu du +, comme lm(y ~ x * w, data = jd.continue). La formule y ~ x + w + x * w fonctionne également.Comme prévu, les résultats sont différents à cause des différentes sommes des carrés considérées. L’effet modérateur est identique. Les effets principaux sont différents.Pour observer les résultats de lm() de la même façon que aov (avec une table d’analyse de variance), le code anova(res1.lm) produira la même sortie, signalant ainsi la différence des sommes de carré pour calculer les coefficients et celles pour produire la table.Pour obtenir l’analyse de variance avec le type III, il faut utiliser le package car et sa fonction Anova() en y spécifiant la sortie de la fonction aov() et le type de somme de carrés demandé.Voilà! Les effets simples sont maintenant identiques à la sortie de lm().Pour déterminer s’il y présence de l’effet de modérateur, il faut se fier à la ligne x:w et la valeur-\\(p\\), Pr(>F), associée qui est ici de \\(3.269^{-16}\\). Par contre, il n’est pas tout à fait clair comment interpréter ce résultat. Ce sera fait dans la section portant sur la représentation graphique de la modération.","code":"# Réaliser l'analyse avec lm()\nres1.lm <- lm(y ~ x * w, data = jd.continue)\nsummary(res1.lm)\n> \n> Call:\n> lm(formula = y ~ x * w, data = jd.continue)\n> \n> Residuals:\n>    Min     1Q Median     3Q    Max \n> -2.727 -0.654  0.108  0.648  2.608 \n> \n> Coefficients:\n>             Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)   0.0418     0.0464    0.90    0.368    \n> x             0.1076     0.0491    2.19    0.029 *  \n> w             0.2108     0.0504    4.18  3.4e-05 ***\n> x:w           0.3180     0.0369    8.63  < 2e-16 ***\n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 0.931 on 496 degrees of freedom\n> Multiple R-squared:  0.201,   Adjusted R-squared:  0.196 \n> F-statistic: 41.7 on 3 and 496 DF,  p-value: <2e-16\n\n# Pour comparer avec aov\nres1.aov <- aov(y ~ x * w, data = jd.continue)\n\n# L'intercepte est inclus\nsummary(res1.aov, intercept = TRUE)\n>              Df Sum Sq Mean Sq F value  Pr(>F)    \n> (Intercept)   1     20    19.9    22.9 2.3e-06 ***\n> x             1     27    27.1    31.2 3.8e-08 ***\n> w             1     17    16.7    19.3 1.4e-05 ***\n> x:w           1     65    64.6    74.5 < 2e-16 ***\n> Residuals   496    430     0.9                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1res1.anova <- car::Anova(res1.aov, type = \"III\")\nres1.anova \n> Anova Table (Type III tests)\n> \n> Response: y\n>             Sum Sq  Df F value  Pr(>F)    \n> (Intercept)      1   1    0.81   0.368    \n> x                4   1    4.80   0.029 *  \n> w               15   1   17.50 3.4e-05 ***\n> x:w             65   1   74.49 < 2e-16 ***\n> Residuals      430 496                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"modérer.html","id":"analyse-dun-modérateur-nominal","chapter":" 17 Modérer","heading":"17.3.2 Analyse d’un modérateur nominal","text":"Comme il y un groupe de référence, R choisit par défaut le premier groupe. Dans ce cas, le groupe de référence était le troisième. Pour spécifier ce changement, la fonction relevel() déclarera le groupe de référence. Le premier argument est la variable et l’argument ref = indique le nouveau groupe de référence. La fonction ne fait que reclasser les valeurs afin que le premier groupe que ne voit R ne soit nul autre que le groupe de référence. Ce changement pourrait aussi être fait directement dans le formule de lm() comme lm(y ~ x * relevel(w, ref = 3), data = jd.nominale)La fonction car::Anova() offre un résultat différent pour \\(w\\) et \\(xw\\) car les groupes sont mis en commun. Autrement dit, le vecteur de groupement est analysé comme une seule variable. C’est cette statistique qui est préférée pour éviter d’augmenter l’erreur de Type survenant lorsque le nombre de tests d’hypothèse s’accroît (par exemple, si chaque effet est testé individuellement).Le type III certainement eu un rôle intéressant dans les résultats. Il montre que \\(w\\) n’est pas significativement lié à \\(y\\), \\(p < 0.247\\), ce qui était programmé dans le modèle alors que aov() montre un lien significatif, \\(p<0.006\\).Pour respecter la pratique et pour imiter ce qui est fait par d’autres logiciels, il est préférable de rapporter l’effet d’interaction x:w de res2.anova, soit la sortie produite par car::Anova(). La valeur-\\(p\\) de l’effet d’interaction est de \\(p<001\\), ce qui signifie qu’il y un effet modérateur.\nComme le cas précédent, ce résultat est ardu à interpréter. Des représentations graphiques sont d’une aide cruciale pour démêler les effets.","code":"# Réaliser l'analyse avec lm()\njd.nominale$w <-  relevel(jd.nominale$w, ref = 3)\nres2.lm <- lm(y ~ x * w, data = jd.nominale)\nsummary(res2.lm)\n> \n> Call:\n> lm(formula = y ~ x * w, data = jd.nominale)\n> \n> Residuals:\n>    Min     1Q Median     3Q    Max \n> -1.690 -0.295 -0.002  0.297  1.574 \n> \n> Coefficients:\n>              Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)  0.000209   0.006202    0.03     0.97    \n> x            0.259007   0.006139   42.19   <2e-16 ***\n> w1           0.009207   0.008771    1.05     0.29    \n> w2          -0.005281   0.008770   -0.60     0.55    \n> x:w1         1.055880   0.008832  119.55   <2e-16 ***\n> x:w2        -1.069859   0.008813 -121.40   <2e-16 ***\n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 0.438 on 14994 degrees of freedom\n> Multiple R-squared:  0.803,   Adjusted R-squared:  0.803 \n> F-statistic: 1.22e+04 on 5 and 14994 DF,  p-value: <2e-16\n\n# Pour comparer avec aov\nres2.aov <- aov(y ~ x * w, data = jd.nominale)\n# L'intercept est inclu\nsummary(res2.aov)\n>                Df Sum Sq Mean Sq  F value Pr(>F)    \n> x               1    930     930  4837.35 <2e-16 ***\n> w               2      2       1     5.13 0.0059 ** \n> x:w             2  10821    5411 28144.98 <2e-16 ***\n> Residuals   14994   2882       0                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Somme de carrés de type III pour `aov()`\nres2.anova <- car::Anova(res2.aov, type = \"III\")\nres2.anova \n> Anova Table (Type III tests)\n> \n> Response: y\n>             Sum Sq    Df F value Pr(>F)    \n> (Intercept)      0     1     0.0   0.97    \n> x              342     1  1779.8 <2e-16 ***\n> w                1     2     1.4   0.25    \n> x:w          10821     2 28145.0 <2e-16 ***\n> Residuals     2882 14994                   \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"modérer.html","id":"représentations-graphiques-dune-modération","chapter":" 17 Modérer","heading":"17.4 Représentations graphiques d’une modération","text":"À part quelques personnes ayant l’oeil affuté à l’interprétation de modèle linéaire contenant une interaction, il est rare pour tous les autres d’en tirer du sens immédiatement. Ainsi, des analyses graphiques sont plus que pertinentes pour en interpréter les résultats.Deux graphiques sont particulièrement pertinents : le graphique de Johnson- Neyman et le graphique des pentes simples (simple slopes). Il existe quelques packages pour produire ces graphiques. Dans cet ouvrage, le package interactions (Long, 2019) sera présenté. Ce package peut traiter des interactions doubles et triples et est compatible avec plusieurs commandes de ggplot2.Pour réaliser ces graphiques, il faudra d’abord installer et importer le package.À noter que le package interactions dépend du package sandwich (Zeileis, 2006) qu’il faut également installer.","code":"\nlibrary(interactions)"},{"path":"modérer.html","id":"lanalyse-des-pentes-simples","chapter":" 17 Modérer","heading":"17.4.1 L’analyse des pentes simples","text":"La méthode classique pour sonder un effet d’interaction consiste à calculer la pente du prédicteur à différentes valeurs du modérateur. Cette analyse se réfère à l’analyse des pentes simples. La fonction permettant de commander cette technique est sim_slopes(). Il faut y mettre en argument la sortie de lm(), désigner le prédicteur pred = et le modérateur modx =.Cette analyse est faite avec l’exemple avec la variable de groupement, jd.nominale. L’argument johnson_neyman = FALSE évite simplement un message d’erreur, car le graphique éponyme ne peut être généré lorsque le modérateur est une variable de groupement.Ce tableau pour les groupes de \\(w\\) si l’effet modérateur est significatif.\nFigure 17.3: Analyse de pentes simples pour un modérateur nominal\nLe code ci-dessus illustre ce patron de différences avec la Figure 17.3. L’axe des \\(x\\) et des \\(y\\) montrent la relation entre les variables en fonction des groupes qui sont représentées par des lignes différentes. L’option plot.point = TRUE affiche optionnellement les données dans la figure. La Figure 17.3 montre une tendance positive entre \\(x\\) et \\(y\\) pour le groupe de référence (3) qui s’accentue de façon importante pour le groupe (1), mais qui s’inverse pour le groupe 2.Le graphique des pentes peut aussi être réalisé pour un modérateur continu. La Figure 17.4 illustre ce graphique. Lorsque le modérateur représente des groupes, les niveaux sont clairement définis. Dans le cas d’un modérateur continu, des niveaux arbitraires doivent être sélectionnés.\nFigure 17.4: Analyses des pentes simples pour un modérateur continu\nPar défaut, l’analyse est réalisée avec \\(-1\\) écart type, la moyenne et \\(+1\\) écart type comme groupement de \\(w\\). Ces valeurs peuvent être choisies manuellement avec modx.values = en y spécifiant les valeurs d’intérêt. En plus, cet argument est compatible non seulement avec interact_plot() et sim_slopes(). La Figure 17.5 montre l’analyse des pentes pour des valeurs de modx.values = c(-2, -1, 0, 1, 2).\nFigure 17.5: Analyses des pentes simples pour un modérateur continu\n","code":"# Les statistiques liées aux pentes simples\nsim_slopes(model = res2.lm,\n           pred = x,\n           modx = w,\n           johnson_neyman = FALSE)\n> SIMPLE SLOPES ANALYSIS \n> \n> Slope of x when w = 2: \n> \n>    Est.   S.E.    t val.      p\n> ------- ------ --------- ------\n>   -0.81   0.01   -128.25   0.00\n> \n> Slope of x when w = 1: \n> \n>   Est.   S.E.   t val.      p\n> ------ ------ -------- ------\n>   1.31   0.01   207.10   0.00\n> \n> Slope of x when w = 3: \n> \n>   Est.   S.E.   t val.      p\n> ------ ------ -------- ------\n>   0.26   0.01    42.19   0.00\n# Le graphique des pentes simples\ninteract_plot(model = res2.lm,\n              pred = x,\n              modx = w, \n              plot.points = TRUE)\ninteract_plot(model = res1.lm,\n              pred = x,\n              modx = w, \n              plot.points = TRUE)\ninteract_plot(model = res1.lm,\n              pred = x,\n              modx = w, \n              plot.points = TRUE,\n              modx.values = c(-2, -1, 0, 1, 2))"},{"path":"modérer.html","id":"le-graphique-de-johnson-neyman","chapter":" 17 Modérer","heading":"17.4.2 Le graphique de Johnson-Neyman","text":"Lorsque le modérateur est continu, la méthode la plus rigoureuse d’explorer des effets modérateurs est de calculer l’intervalle de Johnson-Neyman qui indique la plage de valeurs du modérateur selon lesquelles la pente du prédicteur est significative ou non pour un seuil \\(\\alpha\\) spécifié. Comme les deux variables doivent être continues, c’est le jeu de données jd.continue qui sera utilisé pour cet exemple.La fonction sim_slopes() génère l’analyse des pentes simples, mais produit également l’intervalle de Johnson-Neyman et le graphique associé. La fonction est utilisée comme précédemment, à l’exception qu’il est possible de demander comme argument jnplot = TRUE pour obtenir le graphique Johnson-Neyman et qu’il n’est plus nécessaire de commander johnson_neyman = FALSE évite un message d’erreur.\nFigure 17.6: Le graphique Johnson-Neyman pour interpréter les effets modérateurs\nLa Figure 17.6 montre le graphique Johnson-Neyman. La zone bleue indique le niveau de modérateur où l’effet de \\(x\\) est significativement différent de la pente de \\(x\\) lorsque l’effet du modérateur est nul. La zone rouge montre l’étendue où il n’y pas d’effet de modération. L’étendue est également précisée dans la première phrase imprimée de la sortie. La sortie également produit la sortie de l’analyse de pentes simples ou l’information obtenue du graphique Johnson-Neyman est répétée d’une façon différente.Dans cet exemple, plus l’unité à un niveau de modérateur élevé (en termes absolus), plus il accentue l’effet de la variable indépendante (zones bleues). Le modérateur joue donc un rôle très important dans ce modèle.","code":"sim_slopes(model = res1.lm,\n           pred = x,\n           modx = w, \n           jnplot = TRUE)\n> JOHNSON-NEYMAN INTERVAL \n> \n> When w is OUTSIDE the interval [-0.53, 0.00], the slope\n> of x is p < .05.\n> \n> Note: The range of observed values of w is [-3.00, 3.20]> SIMPLE SLOPES ANALYSIS \n> \n> Slope of x when w = -1.100 (- 1 SD): \n> \n>    Est.   S.E.   t val.      p\n> ------- ------ -------- ------\n>   -0.31   0.06    -5.08   0.00\n> \n> Slope of x when w = -0.075 (Mean): \n> \n>   Est.   S.E.   t val.      p\n> ------ ------ -------- ------\n>   0.07   0.05     1.38   0.17\n> \n> Slope of x when w =  0.950 (+ 1 SD): \n> \n>   Est.   S.E.   t val.      p\n> ------ ------ -------- ------\n>   0.44   0.06     7.18   0.00"},{"path":"modérer.html","id":"rapporter-lanalyse-de-modération","chapter":" 17 Modérer","heading":"17.5 Rapporter l’analyse de modération","text":"Pour rapporter l’analyse de modération, plusieurs éléments peuvent être pertinents à rapporter et varient en fonction des domaines. Il importe de rapporter le ou les effets d’interaction. Ensuite, l’ajout de graphique, comme le Johnson-Neyman ou celui des pentes simples permettent de mieux comprendre, mais surtout de mieux visualiser les interprétations qui auront été rapporter en texte dans l’article. C’est un appui primordial pour le lecteur qui devra lui aussi dégager les tendances des résultats, mais qui, contrairement aux auteurs, n’est pas nécessairement très familier avec le jeu de données. En plus de ces informations primordiales, certains domaines de recherche demanderont de rapporter le modèle de régression réalisé (voir Rapporter la régression).","code":""},{"path":"exercice-lineaire.html","id":"exercice-lineaire","chapter":"Exercices","heading":"Exercices","text":"","code":""},{"path":"exercice-lineaire.html","id":"question-1","chapter":"Exercices","heading":"Question 1","text":"Avec le jeu de données ToothGrowth, réaliser l’anova de len par rapport à l’interaction entre supp et dose. Consulter le sommaire.","code":""},{"path":"exercice-lineaire.html","id":"question-2","chapter":"Exercices","heading":"Question 2","text":"Avec le jeu de données ToothGrowth, réaliser le régression de len par rapport à l’interaction entre supp et dose. Consulter le sommaire.","code":""},{"path":"exercice-lineaire.html","id":"question-3","chapter":"Exercices","heading":"Question 3","text":"Créer un jeu de données pour la structure de la Figure 17.7. Le jeu de données est standardisé et contient 123 sujets.\nFigure 17.7: Diagramme de trajectoire (Question 3)\n","code":""},{"path":"exercice-lineaire.html","id":"question-4","chapter":"Exercices","heading":"Question 4","text":"Créer un jeu de données pour la structure de la Figure 17.8. Le jeu de données est standardisé et contient 456 sujets.\nFigure 17.8: Diagramme de trajectoire (Question 4)\n","code":""},{"path":"exercice-lineaire.html","id":"question-5","chapter":"Exercices","heading":"Question 5","text":"Créer un jeu de données pour la structure de la Figure 17.9. Le jeu de données est standardisé et contient 789 sujets.\nFigure 17.9: Diagramme de trajectoire (Question 5)\n","code":""},{"path":"exercice-lineaire.html","id":"question-6","chapter":"Exercices","heading":"Question 6","text":"Avec le jeu de données de la Question 5, réaliser l’analyse de médiation.","code":""},{"path":"exercice-lineaire.html","id":"question-7","chapter":"Exercices","heading":"Question 7","text":"Avec le jeu de données ToothGrowth, analyser l’interaction entre supp et dose sur la variable dépendante len. Produire les graphiques","code":""},{"path":"décomposer.html","id":"décomposer","chapter":" 18 Décomposer","heading":" 18 Décomposer","text":"Prédire une variable dépendante à partir de variables indépendantes n’est pas la seule façon de décortiquer des données. Il existe des techniques statistiques qui réorganisent l’information (les corrélations) des variables. Il s’agit des analyses factorielles. Elles s’intéressent plus à la structure des corrélations qu’aux systèmes qui les lient. Bien qu’il s’agisse de nuances sur le plan statistique, les deux côtés d’une même médaille, les différences sont importantes sur le plan théorique. Plutôt que de parler de cause à effet, ce sera plutôt la structure sous-tendant les variables qui sera d’intérêt.À titre d’illustration, la Figure 18.1 présente à gauche la régression, \\(x_1\\) prédit \\(x_2\\), et à droite le facteur \\(F\\) qui est la source de \\(x_1\\) et \\(x_2\\).\nFigure 18.1: Représentations de la régression et l’analyse factorielle\nPlusieurs éléments permettent de mieux mettre en évidence les différences entre les deux modèles. Les variables manifestes \\(x_i\\) sont des mesures empiriques mesurées auprès d’unités d’observations. Elles sont représentées par des rectangles. Le facteur \\(F\\) est représenté par un cercle. Il s’agit d’une variable latente, une variable non observée et inférée à partir des variables manifestes. Par exemple, les habiletés de lecture \\(x_1\\) sont liées aux habiletés en mathématiques \\(x_2\\). D’un côté, mieux le participant lit, plus il répond rapidement et exactement aux questions de mathématiques. De l’autre côté, les habiletés de mathématiques et de lectures peuvent aussi être liées à un facteur commun : l’intelligence. Les habiletés de mathématiques et de lectures sont observables par des évaluations, mais l’intelligence s’infère à partir de ces tests29. C’est là que les analyses factorielles entrent en jeu.Les analyses factorielles sont utilisées, par exemple, lors de la création d’un test psychométrique. Le psychométricien s’intéresse à connaître quels items (variables manifestes) sont liés sur quelle dimension et à quel degré. Les items communs sont liés sur certains facteurs et peu ou pas sur les autres. Le facteur à un fort potentiel explicatif pour les items qui lui sont fortement liés, voire même représente un concept, thème ou construit théorique commun partagé entre ces items.Dans ce livre, deux types d’analyse factorielle sont abordées : l’analyse en composantes principales et l’analyse factorielle. La première est un modèle athéorique (sans hypothèses sous-jacentes) à la structure des données, alors que la seconde présuppose, comme hypothèse, un ou des facteurs latents communs. Sur le plan computationnel, l’analyse en composantes principales réorganise tous les facteurs, alors que l’analyse factorielle extrait un nombre donné de facteurs. Les analyses factorielles se distinguent également en deux catégories : exploratoire et confirmatoire. Ces termes distinguent si l’analyse contraint ou non la structure factorielle. Tout cela sera abordé plus en détails dans les prochains chapitres.","code":""},{"path":"décomposer.html","id":"lanalyse-en-composantes-principales","chapter":" 18 Décomposer","heading":"18.1 L’analyse en composantes principales","text":"L’analyse en composantes principales (ACP) fait partie de la famille des analyses factorielle exploratoire. Elle consiste à réorganiser des variables corrélées (une matrice de covariance) en nouvelles variables orthogonales (décorrélées) les unes des autres. Ces nouvelles variables sont des composantes principales, axes principaux ou encore des dimensions. Il s’agit d’une technique à la fois géométrique et statistique dont les champs d’application vont de la psychologie, la sociologie, la biologie, la chimie à la physique quantique jusqu’aux mathématiques pures. Elle ne se limite donc pas qu’à la psychométrie.L’ACP prend comme base la matrice de corrélation30 et la réorganisation (essentiellement une rotation géométrique) afin que les nouvelles dimensions soient indépendantes l’une de l’autre. L’ACP procure trois informations cruciales sur une matrice de corrélation, les valeurs propres et les vecteurs propres, desquels les loadings31 sont calculables.","code":""},{"path":"décomposer.html","id":"les-valeurs-propres","chapter":" 18 Décomposer","heading":"18.1.1 Les valeurs propres","text":"Les valeurs propres (eigenvalues) représentent l’aspect crucial de l’ACP, soit l’importance de chaque composante à représenter les variables. Plus les variables sont liées sur un même axe (en nombre et en poids), plus la valeur propre de cet axe sera élevée. Mathématiquement, les valeurs propres sont représentées par un vecteur \\(\\Lambda\\) ou une matrice diagonale \\(\\text{diag}(\\Lambda) = \\mathbf{\\Lambda}\\).La somme des valeurs propres égale la somme des variances, ce qui équivaut en matrice de corrélation à \\(p\\), le nombre de variables. Comme il s’agit du potentiel maximal de ce qui peut être expliqué et que les valeurs sont une part de ce total, il est possible de calculer leur importance relative par des pourcentages. Par exemple une valeur propre de 5 sur un total de \\(p=10\\) variables signifie que l’axe correspondant explique \\(5/10 \\times 100 = 50\\)% de la variance de la matrice de corrélation. En d’autres termes, la valeur propre est une bonne métrique de l’importance d’une dimension.En identifiant les composantes principales, l’ACP révèle du même coup l’importance de chacune d’elle. Il devient tout naturel de les classer en ordre décroissant32.","code":""},{"path":"décomposer.html","id":"les-vecteurs-propres","chapter":" 18 Décomposer","heading":"18.1.2 Les vecteurs propres","text":"Les vecteurs propres (eigenvectors) sont les axes d’orientation des valeurs propres dans le plan de la matrice de corrélation originale. Chaque axe représenté est orthogonal (indépendant) aux autres. Ils sont également normalisés, c’est-à-dire des vecteurs unitaires (la somme de leur carré égale 1). Mathématiquement, les vecteurs propres sont représentés par une matrice \\(\\mathbf{V}\\).","code":""},{"path":"décomposer.html","id":"les-loadings","chapter":" 18 Décomposer","heading":"18.1.3 Les loadings","text":"Les vecteurs propres avec les valeurs propres permettent de calculer les loadings. Ils correspondent au lien entre chaque axe et les variables. En d’autres termes, les loadings correspondent au degré selon lequel une variable corrèle avec un facteur. C’est l’importance relative de chaque variable sur chaque axe. Par exemple, les variables très associées avec un certain axe ont de très forts loadings en lien avec cet axe, mais des loadings beaucoup plus faibles avec les axes avec lesquels ils sont moins associés. L’équation (18.1) détaille l’équation.\\[\\begin{equation}\n\\mathbf{L} =\\mathbf{V}\\mathbf{\\Lambda^{\\frac{1}{2}}}\n\\tag{18.1}\n\\end{equation}\\]Lorsque dérivés d’une matrice de corrélations, les loadings sont les corrélations entre l’axe et la variable. C’est la force des liens allant de \\(F\\) aux \\(x\\) dans la Figure 18.1. En termes d’analyse factorielle, il s’agit de scores factoriels.Pour mieux illustrer, les valeurs propres, les vecteurs propres, les loadings, mais surtout la décomposition en axes principaux, la section suivante présente un exemple pour mettre le tout en commun.","code":""},{"path":"décomposer.html","id":"création-de-données-1","chapter":" 18 Décomposer","heading":"18.2 Création de données","text":"La création de données pour une ACP est très simple. Il suffit de créer une matrice de covariance ou de corrélation. Dans cet exemple, 10 participants sont mesurés sur deux variables ayant une corrélation de .80 entre elles.L’argument empirical = TRUE assure que la matrice de corrélation de la population est identique à celle de l’équation, \\(\\mathbf{\\Sigma} = \\mathbf{S}\\), ce qui facilite l’interprétation de cet exemple.La Figure 18.2 illustre la répartition des 10 participants par rapport aux variables 1 et 2. La ligne pointillée désigne la droite de régression qui les relie, soit la pente \\(\\beta = .8\\).\nFigure 18.2: Présentation des données (jd.acp)\n","code":"\n# Pour la reproductibilité\nset.seed(42)\n\n# Quelques paramètres\nn <- 10\nrho <- .80\nS <- matrix(c( 1, rho,\n              rho, 1),\n            ncol = 2, nrow = 2,\n            dimnames = list(nom <-c(\"x1\", \"x2\"), nom))\n\njd.acp <- MASS::mvrnorm(n = n, \n                        mu = rep(0, 2), \n                        Sigma = S, \n                        empirical = TRUE)"},{"path":"décomposer.html","id":"analyse-1","chapter":" 18 Décomposer","heading":"18.3 Analyse","text":"Il existe plusieurs fonctions dans R, mais aussi dans des packages, pour réaliser l’ACP. L’analyse en soi n’rien de sorcier (en autant que les statisticiens ne font pas de la magie), c’est surtout l’emballage (arguments, graphiques et sorties) qui change de l’une à l’autre des méthodes. Ces fonctions précalculent et extraient les statistiques désirées, rien que l’utilisateur ne peut faire lui-même.Il existe quatre fonctions de base R pour faire l’analyse en composantes principales. Les deux principales, eigen() et svd(), fournissent des résultats virtuellement identiques, mais se distinguent sur leur limite de ce qu’elles peuvent accomplir. Les deux autres, princomp() etprcomp()`, sont leur emballage respectif. Les détails des calculs seront présentés dans une autre section, Les calculs de l’analyse en composantes principales.","code":""},{"path":"décomposer.html","id":"eigen","chapter":" 18 Décomposer","heading":"18.3.1 eigen()","text":"La fonction eigen() est celle des puristes. Rudimentaire, elle prend en argument une matrice de covariance ou de corrélation et calcule les valeurs propres (values) et les vecteurs propres (vectors).La fonction eigen() ne fonctionne qu’avec des matrices carrées et est donc très robuste afin d’éviter de mauvais arguments.","code":"# L'analyse de la matrice de corrélation \nres.eig <- eigen(cor(jd.acp))\n\n# Les valeurs propres\nres.eig$values\n> [1] 1.8 0.2\n\n# Les vecteurs propres\nres.eig$vectors\n>       [,1]   [,2]\n> [1,] 0.707 -0.707\n> [2,] 0.707  0.707\n\n# Les loadings\nres.eig$vectors %*% diag(sqrt(res.eig$values))\n>       [,1]   [,2]\n> [1,] 0.949 -0.316\n> [2,] 0.949  0.316"},{"path":"décomposer.html","id":"svd","chapter":" 18 Décomposer","heading":"18.3.2 svd()","text":"L’analyse de décomposition en valeurs singulières (singular value decomposition, SVD) est généralement recommandée, car elle est computationnellement plus robuste que eigen(). Un puriste R choisirait probablement la fonction eigen(), car la fonction est plus robuste aux erreurs de l’utilisateur.Les résultats sont identiques à la fonction eigen(). Le lecteur attentif aura toutefois remarqué que res.svd$v n’égale pas exactement res.eig$vectors à cause des signes négatifs du premier vecteur propre. Toutefois, il convient d’affirmer que c’est seulement la polarité qui est différente, comme si un vecteur propre indiquait le nord et l’autre le sud, alors que les deux sont dos à dos, exactement à \\(180^{\\circ}\\).La fonction svd() est plus générale et peut utiliser des matrices rectangulaires. Il faut faire attention de ne pas lui fournir le jeu de données et bien la matrice de covariance ou de corrélation.Il convient également d’ajouter que, comme \\(\\mathbf{\\Sigma} = \\mathbf{S}\\) (à l’aide de l’argument empirical = TRUE de MASS::mvrnorm()), il n’était pas nécessaire de générer de données à proprement parler pour réaliser les analyses, car svd(S) ou eigen(S) offre les mêmes résultats. C’est la matrice de corrélation qui est décomposée et non les participants. Remarquez d’ailleurs comment l’ACP est indépendante de la taille d’échantillon33.","code":"# L'analyse de la matrice de corrélation\nres.svd = svd(cor(jd.acp))\n\n# Les valeurs propres\nres.svd$d\n> [1] 1.8 0.2\n\n# Les vecteurs propres\nres.svd$v\n>        [,1]   [,2]\n> [1,] -0.707 -0.707\n> [2,] -0.707  0.707\n\n# Les loadings\nres.svd$v %*% diag(sqrt(res.svd$d))\n>        [,1]   [,2]\n> [1,] -0.949 -0.316\n> [2,] -0.949  0.316"},{"path":"décomposer.html","id":"représentations-des-résultats","chapter":" 18 Décomposer","heading":"18.4 Représentations des résultats","text":"La Figure 18.3 montre à gauche les données originales sur les axes représentés par les deux variables (une reprise de la Figure 18.2). À droite, il s’agit de la rotation trouvée par l’ACP (eigen() ou svd()). Les participants conservent entre eux les mêmes distances par rapport aux autres, mais aussi par rapport aux axes originaux représentés par des lignes pointillées. C’est vraiment l’orientation du plan qui change34.\nFigure 18.3: Représentations des participants selon les variables (gauche) et les composantes principales (droite).\nLa première composante retrouvée correspond (approximativement) à la droite de régression de la Figure 18.2. Il s’agit de l’information partagée par les deux variables : c’est leur axe commun. L’erreur résiduelle correspond au deuxième axe (l’axe vertical). Cette intuition est fondamentale : une valeur propre élevée implique une dimension où de la variance est partagée entre les variables, alors qu’une valeur propre plus faible aura tendance à représenter une dimension de résidus, et par définition, d’informations non partagées.La Figure 18.4 offre une vue de l’agencement des variables sur les deux axes. Il s’agit des loadings des variables dans l’espace des composantes. Cette représentation est assez triviale pour deux variables, mais peut devenir très pertinente lorsque plusieurs variables (ou items) sont concernées. Il est possible d’observer alors des regroupements d’items sur les facteurs. Elle se limite toutefois à une ou deux composantes étant donné la complexité de réaliser et d’interpréter des figures de trois dimensions et plus.\nFigure 18.4: Représentations des variables selon les composantes principales.\n","code":""},{"path":"décomposer.html","id":"les-calculs-de-lanalyse-en-composantes-principales","chapter":" 18 Décomposer","heading":"18.5 Les calculs de l’analyse en composantes principales","text":"Il existe plusieurs techniques mathématiques pour retrouver les valeurs propres. Elles ont différents avantages selon l’objectif visé. Elles ont certainement toutes en commun que, plus le nombre de variables augmente, plus le désir de les calculer par ordinateur est grand. Ici, une technique est présentée dans l’optique de bien vérifier qu’aucun sortilège computationnel n’opère derrière le logiciel35.Une des méthodes pour réaliser l’ACP est de résoudre le polynôme caractéristique. Autrement dit, il s’agit de retrouver tous les inconnus \\(\\lambda\\) (les valeurs propres) du polynôme caractéristique, soit l’équation (18.2)\\[\\begin{equation}\n|\\mathbf{S} - \\lambda \\mathbf{}| = 0\n\\tag{18.2}\n\\end{equation}\\]qui représente le déterminant (indiqué par les \\(||\\)) nul de la matrice pour différent \\(\\lambda\\).En conservant l’exemple précédent, remplace la matrice symétrique\\[\n\\mathbf{S} = \\left(\n\\begin{array}{cc}\n1 & .8 \\\\\n.8 & 1\n\\end{array}\n\\right)\n\\]dans l’équation (18.2), ce qui donne,\\[\n\\left| \\left(\n\\begin{array}{cc}\n1 & .8 \\\\\n.8 & 1\n\\end{array}\n\\right)\n-\\lambda \\left(\\begin{array}{cc}\n1 & 0 \\\\\n0 & 1\n\\end{array}\n\\right) \\right| = 0\n\\]et ainsi\\[\n\\left| \\left(\n\\begin{array}{cc}\n1 & .8 \\\\\n.8 & 1\n\\end{array}\n\\right)\n- \\left(\\begin{array}{cc}\n\\lambda & 0 \\\\\n0 & \\lambda\n\\end{array}\n\\right) \\right| = 0\n\\]et ainsi\\[\n\\left| \\left(\n\\begin{array}{cc}\n1 - \\lambda & .8 \\\\\n.8 & 1 - \\lambda\n\\end{array}\n\\right)\n\\right| = 0\n\\]Le polynôme caractéristique est retrouvé en calculant le déterminant de la matrice, soit le produit de la diagonale moins le produit des valeurs hors diagonale36,\\[\n(1-\\lambda)(1-\\lambda) - (.8)(.8) = 0\n\\]ce qui donne\\[\\begin{equation}\n\\lambda^2 - 2 \\lambda +.36 = 0\n\\tag{18.3}\n\\end{equation}\\]où il est maintenant possible de résoudre \\(\\lambda\\) avec la très célèbre équation (18.4).\\[\\begin{equation}\n\\lambda^2+b\\lambda+c=0 \\implies \\lambda=\\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}\n\\tag{18.4}\n\\end{equation}\\]Les solutions sont \\(.2\\) et \\(1.8\\). Graphiquement, la Figure 18.5 illustre l’équation polynomiale caractéristique (18.3).\nFigure 18.5: L’équation du polynôme caractéristique\nCela fait beaucoup de mathématiques. Est-il possible d’y arriver plus simplement avec R? Le package pracma (Borchers, 2022) offre une fonction charpoly() qui permet de trouver le polynôme caractéristique d’une matrice. R de base aussi une fonction permettant de résoudre des polynômes, polyroot(). Avec ces deux fonctions, il est possible de refaire toute la présente section. Il faut toutefois noter que les coefficients polynomiaux donnés par charpoly() doivent être inversés pour polyroot(). À noter également, l’ajout de la fonctionRe() assure que les valeurs propres sont des nombres réels et non imaginaires37. Par convention, les valeurs propres sont ordonnées de façon décroissante, bien qu’elle n’est originalement pas d’ordre particulier.Et voilà! Les valeurs propres sont retrouvées. Pour les vecteurs propres, c’est un peu plus compliqué.L’objectif est de trouver pour chaque valeur propre la solution de l’équation (18.5)\\[\\begin{equation}\n\\left(\\mathbf{S} - \\lambda_i \\mathbf{} \\right) v_i= 0\n\\tag{18.5}\n\\end{equation}\\]soit pour \\(\\lambda = 1.8\\) :\\[\\left[ \\left(\n\\begin{array}{cc}\n1 & .8 \\\\\n.8 & 1\n\\end{array}\n\\right)\n- 1.8 \\left(\\begin{array}{cc}\n1 & 0 \\\\\n0 & 1\n\\end{array}\n\\right) \\right]\\left[\n\\begin{array}{c}\nv_{11} \\\\\nv_{21}\n\\end{array}\n\\right] = 0\\]puis\\[\\left[ \\left(\n\\begin{array}{cc}\n-.8 & .8 \\\\\n.8 & -.8\n\\end{array}\n\\right)\n\\right]\\left[\n\\begin{array}{c}\nv_{11} \\\\\nv_{21}\n\\end{array}\n\\right] = 0\\]\\[\\begin{aligned}\n-.8v_{11} + .8 v_{21}=0 \\\\\n.8v_{11} - .8 v_{21}=0\n\\end{aligned}\\]et de la même façon pour \\(\\lambda = .2\\) :\\[\\left[ \\left(\n\\begin{array}{cc}\n1 & .8 \\\\\n.8 & 1\n\\end{array}\n\\right)\n- .2 \\left(\\begin{array}{cc}\n1 & 0 \\\\\n0 & 1\n\\end{array}\n\\right) \\right]\\left[\n\\begin{array}{c}\nv_{11} \\\\\nv_{21}\n\\end{array}\n\\right] = 0\\]se simplifie en\\[\\left[ \\left(\n\\begin{array}{cc}\n.8 & .8 \\\\\n.8 & .8\n\\end{array}\n\\right)\n\\right]\\left[\n\\begin{array}{c}\nv_{12} \\\\\nv_{22}\n\\end{array}\n\\right] = 0\\]pour donner sous forme linéaire.\\[\\begin{aligned}\n.8v_{12} + .8 v_{22}=0\\\\\n.8v_{12} + .8 v_{22}=0\n\\end{aligned}\\]Ces systèmes d’équations sont toujours indéterminés. Pour les résoudre, la solution est de fixer arbitrairement, pour chaque colonne, un des éléments de \\(\\mathbf{V}\\), comme \\(v_{11} = v_{12} = 1\\).Pour \\(v_{11} = 1\\), cela donne\n\\[\\begin{aligned}\n-.8(1)+.8v_{21} = 0\\\\\n-.8 + .8v_{21} = 0\\\\\nv_{21} = 1\n\\end{aligned}\\]et pour \\(v_{12} = 1\\) :\\[\\begin{aligned}\n.8(1)+.8v_{22} = 0\\\\\n.8+ .8v_{22} = 0\\\\\nv_{22} = -1\n\\end{aligned}\\]Ainsi, la matrice \\(\\mathbf{V}\\) est\\[\\mathbf{V} = \\left[\n\\begin{array}{cc}\n1 & 1 \\\\\n1 & -1\n\\end{array} \\right]\\]Il ne reste qu’à normaliser les colonnes (vecteurs) pour que leur longueur soit l’unité38. Pour ce faire, il faut diviser chaque élément du vecteur et le diviser par racine carré de la somme des carrés des éléments du vecteur.\\[\n\\mathbf{V_{ij}} = \\frac{v_{ij}}{ \\sqrt{\\sum_{= 1}^p v_{}}}\n\\]\nce qui donne\\[\n\\mathbf{V} = \\left[\n\\begin{array}{cc}\n.707 & .707 \\\\\n.707 & -.707\n\\end{array} \\right]\n\\]\nLes résultats sont reproduits.Pour un grand nombre de variables, il est préférable d’utiliser une fonction d’optimisation. La logique demeure similaire. En fixant un élément du vecteur propre, la fonction tente de trouver la meilleure solution pour résoudre l’équation (18.5). Comme ces valeurs dépendent entièrement de la valeur fixée (qui peut être n’importe quelle valeur pour n’importe quel élément du vecteur), cela justifie la normalisation à la dernière étape.Pour ce faire, à chaque vecteur propre, la fonction reçoit la valeur propre associée, la matrice de corrélation et une série d’estimateur (le vecteur propre) à trouver.La fonction maison cherche.vecteur() calcule la somme (sum() des écarts absolus (abs()) entre les estimateurs et la valeur cible de 0 de l’équation (18.5). La fonction R optim() prend cette fonction et tente de minimiser les distances, c’est-à-dire d’arriver au résultat de 0 en variant les estimateurs. Noter comment un estimateur est déjà fixé à 1 dans matrix(c(1, est)) qui correspond à \\(v_j\\) où \\(v_{1j}=1\\). La fonction optim() prend un argument d’estimateur par, les paramètres à trouver et fn la fonction à optimiser et une méthode d’optimisation appropriée, method = \"BDGS dans ce cas-ci. Les deux autres arguments sont pour la fonction à optimiser cherche.vecteur(), soit la matrice de covariance et la valeur propre.Il suffit maintenant de jumeler la syntaxe pour trouver les valeurs propres et celle ci-haut pour créer sa propre fonction d’analyse en composantes principales.Pour terminer, la fonction est mise à l’épreuve.Les résultats sont virtuellement identiques.","code":"\n# Trouver les coefficients polynomiaux\ncoef.poly <- pracma::charpoly(S)\n\n# Inverser l'ordre\ncoef.poly <- coef.poly[length(coef.poly):1]\n\n# Trouver les valeurs à 0 (valeurs propres, E)\nE <- Re(polyroot(coef.poly))\n\n# Mettre en ordre décroissant\nE <- sort(E, decreasing = TRUE)\n\n# Résultats\nE\n# Fonction à optimiser\ncherche.vecteur <- function(est, Ep, M) {\n  # est = estimateur\n  # Ep = valeur propre\n  # M =  matrice de corrélation\n  sum(abs((M - diag(Ep, nrow(M))) %*% matrix(c(1, est))))\n}\n\n# Variable pour enregistrer les résultats\nV <- matrix(0, nrow(S), ncol(S))\n\n# Boucle d'optimisation pour chaque\n# valeur propre\nfor (i in 1:length(E)) {\n  V[,i] <- c(1, optim(par = rep(0, ncol(S)-1),\n                      fn = cherche.vecteur,\n                      method = \"BFGS\",\n                      Ep = E[i], # valeur.prpore\n                      M = S)$par\n  )\n}\n\n# Normaliser les vecteurs\nV <- t(t(V) / sqrt(colSums(V^2)))\nacp.maison <- function(S){\n  # Trouver les coefficients polynomiaux\ncoef.poly <- pracma::charpoly(S)\n\n# Inverser l'ordre\ncoef.poly <- coef.poly[length(coef.poly):1]\n\n# Trouver les valeurs à 0 (valeurs propres, E)\nE <- Re(polyroot(coef.poly))\n\n# Mettre en ordre décroissant\nE <- sort(E, decreasing = TRUE)\n\n# Fonction à optimiser\ncherche.vecteur <- function(est, Ep, M) {\n  sum(abs((M - diag(Ep, nrow(S))) %*% matrix(c(1, est))))\n}\n\n# Variable pour enregistrer les vecteurs propres\nV <- matrix(0, nrow(S), ncol(S))\n\n# Boucle d'optimisation pour chaque\n# valeur propre\nfor (i in 1:length(E)) {\n  V[,i] <- c(1, optim(par = rep(0, ncol(S)-1),\n                      fn = cherche.vecteur,\n                      method = \"BFGS\",\n                      Ep = E[i], \n                      M = S)$par\n  )\n}\n\n# Normaliser les vecteurs\nV <- t(t(V) / sqrt(colSums(V^2)))\n\nreturn(list(valeur.propre = E, \n            vecteur.propre = V))\n\n}acp.maison(S)\n> $valeur.propre\n> [1] 1.8 0.2\n> \n> $vecteur.propre\n>       [,1]   [,2]\n> [1,] 0.707  0.707\n> [2,] 0.707 -0.707"},{"path":"explorer.html","id":"explorer","chapter":" 19 Explorer","heading":" 19 Explorer","text":"Comme il été vu, l’analyse en composantes principales (ACP) fait partie de la famille des analyses factorielles exploratoires. Elle est exploratoire, car les loadings sont complètement libres. De plus, l’ACP n’impose pas de structure sur les données. Elle réorganise les variables corrélées (une matrice de covariance) en nouvelles variables orthogonales (décorrélées) les unes des autres (composantes). L’ACP n’informe pas du nombre de composantes signifiantes (importantes) dans le jeu de données. S’il y \\(p\\) variables, elle réorganise la variance en \\(p\\) dimensions. Cependant, le statisticien opportuniste sait que l’information pertinente se trouve dans les valeurs propres les plus élevées et peut les utiliser pour résumer les données.Le statisticien peut toutefois tenter d’isoler la structure factorielle sous-jacente aux données. Plutôt que de réorganiser la matrice de corrélation par une rotation géométrique, il peut tenter d’extraire des facteurs latents de cette matrice. Cette technique se nomme l’analyse factorielle exploratoire. Elle impose une structure aux données, soit un nombre de facteurs, mais demeure exploratoire, car les loadings restent libres.À ce stade, il est intéressant de se pencher sur ce qu’est la structure factorielle sous-jacente à la matrice de corrélation avant d’expliquer l’analyse qui permet de la retrouver. La création de données aidera à en mieux comprendre les tenants et aboutissants.","code":""},{"path":"explorer.html","id":"création-de-données-2","chapter":" 19 Explorer","heading":"19.1 Création de données","text":"Pour construire un jeu de données ayant une structure factorielle, il faut d’abord concevoir cette structure. Il s’agit des loadings des facteurs39. Il s’agit d’une matrice \\(k \\times p\\), c’est-à-dire nombre de facteurs par nombre de variables.Il est plus simple de considérer pour l’instant une structure standardisée, c’est-à-dire que les variables produites auront des variances de 1 (les moyennes sont écartées, car elles ne sont pas nécessaires). Lors de la spécification de la structure factorielle, il faut s’assurer que la somme des carrés des loadings de chaque variable ne dépasse pas \\(1\\), soit la variance désirée des variables. Ne pas respecter cette limite ne crée pas forcément une erreur. Seulement, le scénario ne sera plus standardisé.La Figure 19.1 illustre un exemple de modèle. Les rectangles sont des variables manifestes (observées) à partir desquelles les variables encerclées représentent des facteurs sous-jacents, soit des variables latentes (retrouvées à partir des variables manifestes).\nFigure 19.1: Structure factorielle\nLa syntaxe ci-dessous reconstruit le modèle de la Figure 19.1. La variable phi, pour \\(\\Phi\\), contient les loadings des deux composantes pour créer les six variables. Les lignes (les variables) sont identifiées par et les colonnes (les facteurs) sont identifiées par F.","code":"# Création de la matrice de recette de fabrication\nphi <- matrix(c(.9, .8, .7,  0,  0, .4,\n                 0,  0,  0, .4, .5, .6), \n              nrow = 6, ncol = 2)\n\n# Identification des variables et facteurs\ncolnames(phi) <- c(\"F1\", \"F2\")\nrownames(phi) <- paste0(rep(\"i\",6), 1:6)\nphi\n>     F1  F2\n> i1 0.9 0.0\n> i2 0.8 0.0\n> i3 0.7 0.0\n> i4 0.0 0.4\n> i5 0.0 0.5\n> i6 0.4 0.6"},{"path":"explorer.html","id":"première-technique","chapter":" 19 Explorer","heading":"19.1.1 Première technique","text":"À partir de phi, il est possible d’obtenir la matrice de corrélation de la population \\(\\text{P}\\) (\\(\\rho\\) majuscule) par l’équation (19.1)\\[\\begin{equation}\n\\text{P} = \\Phi \\Phi^{\\prime}\n\\tag{19.1}\n\\end{equation}\\]ou en code R.Le lecteur attentif aura remarqué qu’il ne s’agit pas encore d’une matrice de corrélation : la diagonale n’est pas constituée de 1. Le résultat de \\(\\Phi\\Phi^{\\prime}\\) représente la matrice de corrélation réduite.Il reste à ajouter le bruit, la variance résiduelle, ce qui est fait en changeant la diagonale de \\(R\\) pour l’unité, \\(\\text{diag}(R) = 1\\). D’ailleurs, si une valeur de la diagonale R dépasse 1, alors le scénario standardisé n’est pas respecté. Il s’agit d’une des méthodes pour s’en assurer.Avec la matrice R calculée, il est évidemment possible, comme il été fait précédemment de recourir à la fonction MASS::mvrnorm() avec comme argument Sigma = R pour la matrice de corrélation afin de créer n participants. Il ne reste qu’à choisir une taille d’échantillon.","code":"R <- phi %*% t(phi)\nR\n>      i1   i2   i3   i4   i5   i6\n> i1 0.81 0.72 0.63 0.00 0.00 0.36\n> i2 0.72 0.64 0.56 0.00 0.00 0.32\n> i3 0.63 0.56 0.49 0.00 0.00 0.28\n> i4 0.00 0.00 0.00 0.16 0.20 0.24\n> i5 0.00 0.00 0.00 0.20 0.25 0.30\n> i6 0.36 0.32 0.28 0.24 0.30 0.52diag(R) <- 1\nR\n>      i1   i2   i3   i4  i5   i6\n> i1 1.00 0.72 0.63 0.00 0.0 0.36\n> i2 0.72 1.00 0.56 0.00 0.0 0.32\n> i3 0.63 0.56 1.00 0.00 0.0 0.28\n> i4 0.00 0.00 0.00 1.00 0.2 0.24\n> i5 0.00 0.00 0.00 0.20 1.0 0.30\n> i6 0.36 0.32 0.28 0.24 0.3 1.00# Pour la reproductibilité\nset.seed(33)\n\n# Création du jeu de données\njd <- MASS::mvrnorm(n = 50, \n                    mu = rep(0, ncol(R)), \n                    Sigma = R)\nhead(jd)\n>           i1     i2      i3     i4      i5     i6\n> [1,]  0.0147 -0.464  0.5397  1.187 -0.2932  0.335\n> [2,] -0.4613 -0.288  0.0302  0.960  1.2070  0.789\n> [3,] -1.1191 -0.980 -0.2839 -0.684  0.0166 -0.700\n> [4,] -0.2842  0.619  0.0473 -1.013  0.8804  0.139\n> [5,]  1.0263  2.322  1.4843  2.451 -0.5138  1.936\n> [6,] -0.6026 -0.569 -0.5789 -1.501  2.2906  0.157"},{"path":"explorer.html","id":"deuxième-technique","chapter":" 19 Explorer","heading":"19.1.2 Deuxième technique","text":"Passer par MASS::mvrnorm() est toutefois contre-productif, puisque la fonction mvrnorm() utilise l’ACP pour extraire une structure factorielle pour ensuite générer les données. Une autre façon de créer des données est en passant par la loi de la somme des variances. Chaque loading \\(\\phi_k\\) d’une variable \\(x_i\\) est dans ce contexte [une constante d’échelle][Ajout des constantes d’échelle \\(\\beta\\)] jointe à un score factoriel normalement distribué \\(z_k\\). L’équation (19.2) représente cette relation.\\[\\begin{equation}\nx_i = \\phi_1 z_1 + ... + \\phi_k z_k + \\epsilon\n\\tag{19.2}\n\\end{equation}\\]Ce qui rappelle le modèle linéaire général de l’équation (14.19) (section Prédire). La variable \\(\\epsilon\\) correspond à la variance résiduelle, c’est-à-dire la part de variance de la variable non expliquée par les facteurs.En désirant le scénario standardisé, la variance de \\(x\\) est fixée à 1 et isoler \\(\\epsilon\\) pour en déterminer la variance garantit ce scénario. Cela revient à calculer l’équation (19.3).\\[\\begin{equation}\n\\sigma^2_\\epsilon =  1 - (\\phi_1^2 + ... + \\phi_k^2)  \n\\tag{19.3}\n\\end{equation}\\]Les écarts types des \\(\\epsilon\\) de chaque \\(p\\) variable de la structure factorielle phi se calculent simplement avec R.Maintenant, il faut ajouter cette variable à la structure factorielle. Comme les résidus sont tous indépendants par définition, ils ont tous leur propre facteur (chacun sa colonne), ce qui se fait bien avec la syntaxe suivante.Il suffit maintenant de joindre cette matrice à phi.Il est tentant de joindre directement le vecteur sd.eps à phi. Par contre, joindre ce vecteur comme cbind(phi, sd.eps) implique une structure à trois facteurs : les résidus sont corrélés par les loadings de ce troisième facteur. En utilisant diag(se.eps), chaque résidu à son propre facteur et est indépendant des autres. La structure finale possède \\(k\\) (nombre de facteurs, 2 dans cet exemple) communs en plus de \\(p\\) (nombre de variables, 6 dans cet exemple) facteurs résiduels et ainsi \\(k + p\\) dimensions.Comme à l’équation (19.1), phi2 permet d’obtenir la matrice de corrélation de la population.Exactement le même résultat que l’autre méthode, et ce, sans avoir à modifier la diagonale par l’unité. Les variances résiduelles sont déjà calculées.Une fois la structure factorielle obtenue, il faut générer les scores des participants (les valeurs \\(z\\) de l’équation (19.2)). Une technique usuelle est de créer une matrice \\((k + p) \\times n\\) de scores normaux, soit le nombre de facteurs plus le nombre de variables (pour les résidus) en ligne par \\(n\\) le nombre d’unités en colonnes. Cette matrice représente les scores factoriaux de chaque unité pour chacun des scores et sont multipliés avec la structure factorielle. Autrement dit, chaque poids (loadings) est multiplié à une distribution normale qui représente le score du participant pour ce facteur.Il ne reste plus qu’à faire le produit matricielle de phi2 (\\(\\left[ \\Phi, \\text{diag}(\\epsilon) \\right]\\)) et des scores individuels (score.ind).La variable jd2 contient tous les scores des \\(n\\) participants sur les \\(p\\) variables. Pour obtenir la base de données dans le sens usuel, il suffit de faire une transpose à jd avec la fonction t().Voilà une base de données prêtes à être utiliser pour une analyse factorielle. En pratique, pour donner un peu plus de réalisme, il est possible d’ajouter une moyenne (additionner), de modifier l’écart type (multiplier) de chacune des variables, d’arrondir les scores, et plus en fonction des besoins.","code":"\nsd.eps <- sqrt(1 - rowSums(phi^2))diag(sd.eps)\n>       [,1] [,2]  [,3]  [,4]  [,5]  [,6]\n> [1,] 0.436  0.0 0.000 0.000 0.000 0.000\n> [2,] 0.000  0.6 0.000 0.000 0.000 0.000\n> [3,] 0.000  0.0 0.714 0.000 0.000 0.000\n> [4,] 0.000  0.0 0.000 0.917 0.000 0.000\n> [5,] 0.000  0.0 0.000 0.000 0.866 0.000\n> [6,] 0.000  0.0 0.000 0.000 0.000 0.693\nphi2 <- cbind(phi, diag(sd.eps))R2 <- phi2 %*% t(phi2)\nR2\n>      i1   i2   i3   i4  i5   i6\n> i1 1.00 0.72 0.63 0.00 0.0 0.36\n> i2 0.72 1.00 0.56 0.00 0.0 0.32\n> i3 0.63 0.56 1.00 0.00 0.0 0.28\n> i4 0.00 0.00 0.00 1.00 0.2 0.24\n> i5 0.00 0.00 0.00 0.20 1.0 0.30\n> i6 0.36 0.32 0.28 0.24 0.3 1.00n <- 500; k <- 2; p <- 6\nscore.ind <-  matrix(rnorm(n * p * k), \n                     nrow = (k + p), \n                     ncol = n)\n> Warning in matrix(rnorm(n * p * k), nrow = (k + p), ncol =\n> n): data length differs from size of matrix: [6000 != 8 x\n> 500]\njd2 <- phi2 %*% score.indjd2 <- t(jd2)\nhead(jd2)\n>          i1     i2      i3      i4     i5     i6\n> [1,]  0.111 -0.886 -0.9769  0.5398  1.562  1.698\n> [2,] -1.332 -0.808 -1.0043  0.0559  0.372 -1.006\n> [3,] -1.451 -1.331 -1.7075  0.4404 -1.145 -2.224\n> [4,]  0.439 -0.528  1.4282 -0.8922  0.437 -0.076\n> [5,] -0.387 -0.488 -0.0509  0.5412  0.346 -0.262\n> [6,]  0.526  1.163  0.2766  0.1620  0.747  0.944"},{"path":"explorer.html","id":"lanalyse-en-composantes-principales-1","chapter":" 19 Explorer","heading":"19.1.3 L’analyse en composantes principales","text":"À titre de comparaison, voici les résultats de l’analyse en composantes principales.Elle est assez près de la structure originale, mais pas exactement. Et ce n’est pas à cause du relativement petit n ou de la graine. La cause est bel et bien que l’objectif de l’ACP est de réorganiser la variance plutôt que de rechercher une structure factorielle. L’ACP ne sait pas que la vraie structure contient des facteurs communs entre les variables. Pour tester la présence de facteurs communs, il faut procéder avec une autre analyse : l’analyse factorielle exploratoire (AFE).","code":"# Analyse en composantes principales\nres <- eigen(cor(jd2))\nres\n> eigen() decomposition\n> $values\n> [1] 2.503 1.340 0.888 0.588 0.427 0.253\n> \n> $vectors\n>         [,1]   [,2]    [,3]    [,4]    [,5]     [,6]\n> [1,] -0.5659  0.133  0.0015  0.0729 -0.1699  0.79231\n> [2,] -0.5394  0.129  0.1006  0.0207 -0.6224 -0.54259\n> [3,] -0.5118  0.143 -0.0587  0.3666  0.7118 -0.27061\n> [4,] -0.0330 -0.555  0.7725  0.3039  0.0111  0.04282\n> [5,] -0.0431 -0.629 -0.6206  0.4303 -0.1780 -0.00138\n> [6,] -0.3520 -0.491 -0.0676 -0.7631  0.2128 -0.05277\n\n# Les deux premières valeurs propres\n\n# Les loadings des deux premières composantes\nld <- res$vectors[,1:2] %*% \n  diag(sqrt(res$values)[1:2])\nld\n>         [,1]   [,2]\n> [1,] -0.8954  0.154\n> [2,] -0.8534  0.149\n> [3,] -0.8097  0.165\n> [4,] -0.0522 -0.642\n> [5,] -0.0682 -0.729\n> [6,] -0.5569 -0.568"},{"path":"explorer.html","id":"analyse-factorielle-exploratoire","chapter":" 19 Explorer","heading":"19.2 Analyse factorielle exploratoire","text":"Pour réaliser une analyse factorielle exploratoire, la fonction factanal() de R prend un jeu de données et le nombre de facteurs à tester. La fonction détecte automatiquement s’il s’agit d’un jeu de données ou une matrice de covariance; l’un ou l’autre peut être fourni.C’est aussi simple que l’ACP (même plus!). La sortie procure trois statistiques d’intérêt : les loadings, la proportion de variance expliquée (Proportion Var) et un test de \\(\\chi^2\\) avec sa valeur-\\(p\\). Les loadings entre -.1 et .1 ne sont pas affichés afin d’attirer l’attention sur la structure. Les loadings peuvent être extraits avec la fonction loadings() ou en élément de liste. L’utilisation de [] permet d’afficher complètement la matrice de loadings.La valeur-\\(p\\) concerne la qualité de de l’ajustement de la strucuture à factors facteurs. Comme la valeur-\\(p\\) est significative à un facteur, \\(p < .001\\), cela signifie que la structure testée (un facteur) n’est pas vraisemblable. Il faut tester pour deux facteurs.Les mêmes statistiques, mais pour deux facteurs, sont obtenues. Les résultats sont très près de la structure originale. La valeur-\\(p\\) n’est plus significative, ce qui suggère que le modèle semble bien de deux facteurs.","code":"res1 <- factanal(jd2, factors = 1)\nres1\n> \n> Call:\n> factanal(x = jd2, factors = 1)\n> \n> Uniquenesses:\n>    i1    i2    i3    i4    i5    i6 \n> 0.156 0.376 0.487 1.000 1.000 0.851 \n> \n> Loadings:\n>    Factor1\n> i1  0.919 \n> i2  0.790 \n> i3  0.716 \n> i4        \n> i5        \n> i6  0.386 \n> \n>                Factor1\n> SS loadings      2.131\n> Proportion Var   0.355\n> \n> Test of the hypothesis that 1 factor is sufficient.\n> The chi square statistic is 86.9 on 9 degrees of freedom.\n> The p-value is 6.84e-15loadings(res1)\n> \n> Loadings:\n>    Factor1\n> i1  0.919 \n> i2  0.790 \n> i3  0.716 \n> i4        \n> i5        \n> i6  0.386 \n> \n>                Factor1\n> SS loadings      2.131\n> Proportion Var   0.355\nres1$loadings[]\n>    Factor1\n> i1  0.9187\n> i2  0.7896\n> i3  0.7165\n> i4 -0.0140\n> i5 -0.0124\n> i6  0.3864res2 <- factanal(jd2, factors = 2)\nres2\n> \n> Call:\n> factanal(x = jd2, factors = 2)\n> \n> Uniquenesses:\n>    i1    i2    i3    i4    i5    i6 \n> 0.154 0.378 0.487 0.910 0.849 0.288 \n> \n> Loadings:\n>    Factor1 Factor2\n> i1  0.920         \n> i2  0.788         \n> i3  0.716         \n> i4          0.298 \n> i5          0.387 \n> i6  0.373   0.757 \n> \n>                Factor1 Factor2\n> SS loadings      2.121   0.812\n> Proportion Var   0.354   0.135\n> Cumulative Var   0.354   0.489\n> \n> Test of the hypothesis that 2 factors are sufficient.\n> The chi square statistic is 5.31 on 4 degrees of freedom.\n> The p-value is 0.257"},{"path":"explorer.html","id":"extraire-les-scores","chapter":" 19 Explorer","heading":"19.2.1 Extraire les scores","text":"Une fois le nombre de facteurs déterminé (voir Réduire à ce sujet), les scores factoriels des participants peuvent être utilisés. Pour les obtenir, il faut commander de nouveau l’analyse factorielle en y indiquant le type de scores désiré, soit regression ou Bartlett. Cela ajoutera les scores dans la liste de sortie de la fonction sous l’appellation scores. Voici un exemple.","code":"# Commander l'analyse avec l'argument scores = \"regression\"\nres2 <- factanal(jd2, factors = 2, scores = \"regression\")\n\n# Les scores factoriels se retouvent dans la sortie\nhead(res2$scores)\n>      Factor1 Factor2\n> [1,]  -0.244  1.7256\n> [2,]  -1.259 -0.4500\n> [3,]  -1.588 -1.5611\n> [4,]   0.384 -0.2586\n> [5,]  -0.399 -0.0404\n> [6,]   0.667  0.7026"},{"path":"explorer.html","id":"calculs-de-lanalyse-factorielle-exploratoire","chapter":" 19 Explorer","heading":"19.3 Calculs de l’analyse factorielle exploratoire","text":"Il existe deux techniques plus connues pour réaliser l’analyse factorielle exploratoire : la factorisation en axes principaux (PAF; principal axis factoring) et l’analyse factorielle par maximum de vraisemblance (MLFA; maximum likelihood factor analysis).La PAF tente de retrouver la matrice de corrélation originale sans bruit, c’est-à-dire la matrice de corrélation réduite dans laquelle la diagonale n’est pas constituée de \\(1\\) soit phi %*% t(phi) (en code) ou \\(\\Phi\\Phi^\\prime\\) (en équation). Elle se base sur l’ACP (la fonction maison ci-dessous utilise eigen()) avec quelques raffinements supplémentaires.La logique est de prendre les loadings \\(\\phi\\) d’un nombre arbitraire de \\(k\\) facteurs, puis de calculer la communalité des variables, soit l’équation (19.4) pour la variable \\(\\)\\[\\begin{equation}\nC_i = \\sum_{=1}^k \\Phi_i^2\n\\tag{19.4}\n\\end{equation}\\]pour enfin soustraire de deux communalités subséquentes la différence jusqu’à ce que celle-ci converge le plus près possible de 0 (donc, virtuellement aucune différence entre les deux communalités). Autrement dit, l’objectif de la PAF est de retrouver la matrice de corrélation réduite.Pour ce faire, il faut utiliser une technique itérative. En optimisation, il faut prendre quelques précautions pour s’assurer du bon fonctionnement du logiciel :fixer un maximum d’itérations afin de s’assurer d’éviter d’entrer dans une boucle interminable duquel le logiciel ne peut s’échapper;fixer un maximum d’itérations afin de s’assurer d’éviter d’entrer dans une boucle interminable duquel le logiciel ne peut s’échapper;fixer un seuil de convergence à atteindre afin de déterminer une fin à l’optimisation.fixer un seuil de convergence à atteindre afin de déterminer une fin à l’optimisation.Ces deux règles évitent aux programmeurs de fixer son écran pendant des heures pendant lesquelles le programme calcule des résultats de boucles interminables.Ces deux recommandations se retrouvent en arguments et peuvent être modifiéesNoter l’ajout de ncol = dans la fonction diag(). Cela pour de forcer R à créer une matrice \\(1 \\times 1\\)40. Enfin, noter l’introduction de break (aperçu dans la section Les variables) qui permet de mettre fin à une boucle lorsque une condition est atteinte.À toute fin de comparaison, voici la technique PAF maison (paf()) et l’analyse factorielle de R.Les résultats sont très près, beaucoup plus que l’ACP de la matrice R. La sortie uniqueness correspond à 1-C, la variance résiduelle. Dans les cas, elles sont virtuellement identiques. Dans la sortie de paf(), la matrice de corrélation réduite de R est sortie et montre qu’elle correspond à ce qui est attendu, soit \\(\\Phi\\Phi^{\\prime}\\).Les résultats sont toutefois légèrement différents pour les loadings. Cela est dû au fait que factanal() n’utilise pas la méthode PAF, mais bien le MLFA. Cette technique n’est pas détaillée ici. Elle est très similaire à PAF à l’exception qu’elle focalise sur l’estimation des loadings pour dériver les communalités, plutôt que d’utiliser seulement les communalités. Pour produire une fonction similaire, il faut passer par l’optimisation avec optim() (vue dans Les calculs de l’analyse en composantes principales).","code":"\npaf <- function(covmat, \n                nfactors, \n                converge = .000001, \n                max.iter = 100){\n  \n  # Nombre de variables\n  p <- ncol(covmat)\n  \n  # Boucle d'estimation avec itérations\n  # maximales (max.iter)\n  for(i in 1:max.iter){\n    \n    # ACP de la matrice\n    res <- eigen(covmat)\n    \n    # Les loadings\n    ld <- res$vector[,1:nfactors] %*% \n      sqrt(diag(res$values[1:nfactors], ncol = nfactors))\n    \n    # Les communalités\n    co <- rowSums(ld^2)\n    \n    # La différence à optimiser\n    diff <- diag(covmat) - co\n    \n    # Si toutes les différences sont plus petites que\n    # `convergence`, alors `break` cesse la boucle\n    if (all(abs(diff) < converge)) break\n    \n    # Si non,\n    # mettre à jour la nouvelle diagonale\n    # de la matrice et continuer\n    covmat <- covmat - diag(diff)\n  }\n  \n  # Sortie\n  return(list(uniquenesses = 1-co,\n              loadings = ld,\n              covmat = covmat))\n}factanal(covmat = R, n.obs = n, factors = 2)\n> \n> Call:\n> factanal(factors = 2, covmat = R, n.obs = n)\n> \n> Uniquenesses:\n>   i1   i2   i3   i4   i5   i6 \n> 0.19 0.36 0.51 0.84 0.75 0.48 \n> \n> Loadings:\n>    Factor1 Factor2\n> i1  0.899         \n> i2  0.799         \n> i3  0.699         \n> i4          0.399 \n> i5          0.499 \n> i6  0.368   0.620 \n> \n>                Factor1 Factor2\n> SS loadings      2.071   0.799\n> Proportion Var   0.345   0.133\n> Cumulative Var   0.345   0.478\n> \n> Test of the hypothesis that 2 factors are sufficient.\n> The chi square statistic is 0 on 4 degrees of freedom.\n> The p-value is 1\npaf(covmat = R, nfactors = 2)\n> $uniquenesses\n> [1] 0.19 0.36 0.51 0.84 0.75 0.48\n> \n> $loadings\n>         [,1]   [,2]\n> [1,] -0.8865 -0.155\n> [2,] -0.7880 -0.138\n> [3,] -0.6895 -0.121\n> [4,] -0.0689  0.394\n> [5,] -0.0862  0.493\n> [6,] -0.4974  0.522\n> \n> $covmat\n>      i1   i2   i3   i4   i5   i6\n> i1 0.81 0.72 0.63 0.00 0.00 0.36\n> i2 0.72 0.64 0.56 0.00 0.00 0.32\n> i3 0.63 0.56 0.49 0.00 0.00 0.28\n> i4 0.00 0.00 0.00 0.16 0.20 0.24\n> i5 0.00 0.00 0.00 0.20 0.25 0.30\n> i6 0.36 0.32 0.28 0.24 0.30 0.52phi %*% t(phi)\n>      i1   i2   i3   i4   i5   i6\n> i1 0.81 0.72 0.63 0.00 0.00 0.36\n> i2 0.72 0.64 0.56 0.00 0.00 0.32\n> i3 0.63 0.56 0.49 0.00 0.00 0.28\n> i4 0.00 0.00 0.00 0.16 0.20 0.24\n> i5 0.00 0.00 0.00 0.20 0.25 0.30\n> i6 0.36 0.32 0.28 0.24 0.30 0.52\npaf(covmat = R, nfactors = 2)$covmat\n>      i1   i2   i3   i4   i5   i6\n> i1 0.81 0.72 0.63 0.00 0.00 0.36\n> i2 0.72 0.64 0.56 0.00 0.00 0.32\n> i3 0.63 0.56 0.49 0.00 0.00 0.28\n> i4 0.00 0.00 0.00 0.16 0.20 0.24\n> i5 0.00 0.00 0.00 0.20 0.25 0.30\n> i6 0.36 0.32 0.28 0.24 0.30 0.52"},{"path":"explorer.html","id":"rapporter-lanalyse-factorielle","chapter":" 19 Explorer","heading":"19.4 Rapporter l’analyse factorielle","text":"Lorsqu’il faut rapporter l’analyse factorielle, plusieurs éléments peuvent être rapportés dont certains varient fonction des habitudes du domaine de recherche. Les informations cruciales qu’il faut fournir sont les variances expliquées par chacun des facteurs retenus (Proportion Var ou Cumulative Var dans factanal() ou les communalités avec l’ACP). Le deuxième éléments sont les loadings de la structure finale avec ou sans les loadings négligeables, c’est-à-dire ceux de faible amplitude, comme le fait factanal() qui exclut les loadings de moins de .1 par défaut. Le tableau de loadings rapporté peut exclure les plus petits que .1, .3 ou .4, selon les recommandations psychométriques du domaine. Il est possible également de tous les conserver. Enfin, si une procédure de détermination du nombre de facteurs est utilisé (voir la prochaine section Réduire), la ou les méthodes sélectionnées ainsi que leur résultats sont rapportés. Autrement, il faut justifier le nombre de facteurs retenus.","code":""},{"path":"réduire.html","id":"réduire","chapter":" 20 Réduire","heading":" 20 Réduire","text":"Comme mentionné d’entrée de jeu, l’analyse en composantes principales (ACP) n’est pas une technique de réduction des dimensions. S’il y \\(p\\) variables, l’ACP produit \\(p\\) dimensions. Toutefois, comme les valeurs propres sont des indices de partage commun entre les variables, elle peut être utilisée de cette façon. En fait, l’ACP est presque toujours utilisée pour réduire la dimension d’un jeu de données.Parallèlement, l’analyse factorielle exploratoire permet de dériver un indice de suffisance du nombre de dimensions (le \\(\\chi^2\\)), toutefois, il existe de meilleures techniques pour déterminer le nombre de dimensions à retenir dans une analyse exploratoire.Cette section porte sur les méthodes afin d’identifier le nombre de dimensions à retenir. C’est un sujet très complexe dont seulement quelques brides seront abordées.","code":""},{"path":"réduire.html","id":"illustration","chapter":" 20 Réduire","heading":"20.1 Illustration","text":"Qu’est-ce que la réduction de dimensions? La réduction de dimensions ou la détermination du nombre de dimensions se retrouvent dans tous les domaines, que ce soit en psychologie, en biologie, en informatique ou en physique. Cela peut être autant pratique que théorique. En psychologie, les chercheurs s’intéressent à déterminer le nombre de dimensions d’un test psychométrique. En écologie, les chercheurs peuvent tenter de représenter la variabilité inter-espèces d’un habitat. En ingénierie, les ingénieurs en télécommunication peuvent vouloir extraire l’information pertinente des capteurs électroniques.La compression d’images, en informatique, est un exemple de réduction dimensionnelle dont il est possible d’illustrer visuellement. La réduction de dimensions permet de résumer l’information afin d’en réduire la taille pour ne conserver que le signifiant. Ainsi, les mêmes techniques permettant d’identifier les facteurs psychologiques peuvent être utilisées pour les images, l’identification des thèmes de films et bien d’autres.La Figure 20.1 montre un exemple de compression d’une image de pigeon41. Elle montre à différents niveaux factoriels de compression la même image. À trois dimensions, le pigeon est difficilement perceptible. Progressivement, le pigeon est plus facilement reconnaissable, mais surtout, à un certain seuil (deuxième et troisième lignes de la Figure 20.1, par exemple), l’image gagne en clarté. Trente dimensions conviennent, les plus difficiles désireront peut-être retenir 125 dimensions. L’image à ce stade est très bien. Nonobstant ces nombres, ce sera toujours mieux, en termes de compression, que les 526 dimensions possibles (de l’image originale). Ainsi, à 23.764 % des dimensions, l’image est claire et le pigeon reconnaissable. Cette logique s’applique également pour les facteurs psychologiques.\nFigure 20.1: Pigeon compressé à divers niveaux de dimension \\(k\\)\n","code":""},{"path":"réduire.html","id":"importance-dune-dimension","chapter":" 20 Réduire","heading":"20.2 Importance d’une dimension","text":"Une technique pour connaître l’importance d’une dimension est d’afficher les valeurs propres. Les valeurs propres les plus élevées impliquent qu’elles sont les plus importantes (les premières) alors que de petites valeurs signalent les moins importantes (du bruit, des résidus, soit les dernières). Il est toutefois plus ardu de déterminer le seuil entre important et non important.La structure factorielle et le jeu de données du chapitre Explorer sont repris afin d’illustrer le propos.Voici l’analyse en composante principale.Une façon d’illustrer les valeurs propres graphiquement est d’utiliser un graphique nommé scree plot ou graphique des éboulis. La Figure 20.2 peut être produite avec la syntaxe ci-dessous. Il s’agit de mettre en axe des \\(x\\) la séquence (Position) des valeurs propres (leur ordre) par rapport à la valeur propre en axe des \\(y\\). Ce graphique se produit simplement avec plot() ou ggplot() (voir Visualiser).\nFigure 20.2: Les valeurs propres en fonction de la position de l’axe\nLa Figure 20.2 est une bonne représentation visuelle des valeurs propres. Elle ne répond pas à la question d’intérêt : combien y -t-il de dimensions importantes? Elle donne toutefois une idée. Probablement 2 ou 3? La première est indubitable; la valeur propre est très élevée. Les 4e, 5e et 6e sont quant à elles très petites et sont très près. Comment déterminer avec plus de rigueur le nombre de dimensions? C’est là que les règles d’arrêt entre en jeu.","code":"\n# Pour la reproductibilité\nset.seed(44)\n\n# Création de la matrice de recette de fabrication\nphi = matrix(c(.9, .8, .7,  0,  0, .4,\n                0,  0,  0, .6, .5, .4), \n             nrow = 6, ncol =  2)\n\n# Nombre de variables\np <- nrow(phi)\n\n# Identification des variables et facteurs\ncolnames(phi) = c(\"F1\", \"F2\")\nrownames(phi) = paste0(rep(\"i\", 6), 1:6)\n\nR = phi %*% t(phi)\ndiag(R) <- 1\n\n# Création du jeu de données\njd <- MASS::mvrnorm(n = 50, \n                    mu = rep(0, ncol(R)), \n                    Sigma = R)# L'ACP\nres <- eigen(cor(jd))\n\n# Les valeurs propres\nres$values\n> [1] 2.576 1.699 0.771 0.421 0.328 0.205\nPosition <-  1:p\nvp <- data.frame(Position = Position ,\n                 Valeurs.propres = c(res$values),\n                 Test = rep(\"Empiriques\", p))\n\nggplot(data = vp,\n       mapping = aes(x = Position, \n                     y = Valeurs.propres, \n                     color = Test)) +\n  geom_line() + \n  geom_point() +\n  ylab(\"Valeur propre\") +\n  theme(legend.position.inside = c(0.8, 0.8))"},{"path":"réduire.html","id":"les-règles-darrêt","chapter":" 20 Réduire","heading":"20.3 Les règles d’arrêt","text":"La question est de savoir combien de dimensions il faut retenir pour rendre adéquatement compte des données. Dès lors, l’ACP devient une technique de réduction des dimension. , les analyses exploratoires (comme l’ACP ou l’analyse factorielle exploratoire ne répondent pas explicitement à cette question. Le statisticien use de techniques complémentaires, des règles d’arrêts (stopping rules), pour déterminer le nombre de dimensions à retenir.Ces règles d’arrêt sont en général un bricolage du statisticien pour répondre à la question. Bricolage n’est pas à prendre péjorativement, mais seulement comme un rappel à la réalité : ces techniques sont souvent créées sans aucune dérivation analytique. Il n’est pas possible de leur faire aveuglément confiance (il ne faudrait jamais faire cela de toute façon). Elles ont une pertinence pratique, mais les conditions selon lesquelles elles flanchent ne sont pas ou peu connues. Il existe probablement des centaines de techniques ayant leurs avantages et inconvénients ou bien des scénarios dans lesquels elles sont plus efficaces que les autres. Il en apparaît des nouvelles chaque année depuis 1950. Comme ces techniques sont des bricolages et qu’elles sont taillées différemment, elles ne s’accordent pas toujours sur la même conclusion. Faire l’étalage de ces règles d’arrêt serait bien inutile. Ainsi, trois techniques seront présentées, le test de Kaiser, l’analyse parallèle (parallel analysis) et le test de suffisance de la prochaine valeur propre (Next eigenvalue sufficiency test), une technique récente montrant un excellent rendement. Ces trois règles représentent une même ligne évolutive.","code":""},{"path":"réduire.html","id":"le-test-de-kaiser","chapter":" 20 Réduire","heading":"20.3.1 Le test de Kaiser","text":"Historiquement, le premier test est le test de Kaiser (Kaiser, 1960). C’est la règle d’arrêt utilisée par défaut dans la plupart des logiciels traditionnels. Cela lui permet d’être encore très répandu aujourd’hui, malgré qu’il soit discrédité depuis les années 60 (Achim, 2020; Beauducel, 2001; Turner, 1998).Le test de Kaiser se fonde sur l’idée selon laquelle, si les variables ne sont pas corrélées entre elles (s’il n’y pas de facteurs), les valeurs propres seront égales à 1. La logique du test est d’affirmer que, si une valeur propre est supérieure à 1, alors de l’information commune est nécessairement partagée entre deux ou plusieurs variables. Selon le test de Kaiser, le nombre de composantes à retenir correspond au nombre de valeurs propres supérieures à 1.À titre illustratif, une matrice de corrélation sans aucune corrélation est une matrice d’identité.Les valeurs propres de cette matrice sont toutes égales à 1.Si une seule corrélation est ajoutée, aussi petite soit-elle, alors toutes les plus grandes valeurs propres tendent à capitaliser sur cette variance partagée.Autrement dit, si une valeur propre est supérieure à 1, c’est qu’il y un facteur.La Figure 20.3 illustre le test de Kaiser avec le présent exemple qui suggère deux facteurs (comme simulé).\nFigure 20.3: Les valeurs propres en fonctions de la position de l’axe\nVoici un exemple d’une fonction maison simple pour programmer le test de Kaiser.","code":"mat.cor <- diag(6)\nmat.cor\n>      [,1] [,2] [,3] [,4] [,5] [,6]\n> [1,]    1    0    0    0    0    0\n> [2,]    0    1    0    0    0    0\n> [3,]    0    0    1    0    0    0\n> [4,]    0    0    0    1    0    0\n> [5,]    0    0    0    0    1    0\n> [6,]    0    0    0    0    0    1eigen(mat.cor)$values\n> [1] 1 1 1 1 1 1mat.cor[1, 6] <- mat.cor[6,1] <- .1\nmat.cor\n>      [,1] [,2] [,3] [,4] [,5] [,6]\n> [1,]  1.0    0    0    0    0  0.1\n> [2,]  0.0    1    0    0    0  0.0\n> [3,]  0.0    0    1    0    0  0.0\n> [4,]  0.0    0    0    1    0  0.0\n> [5,]  0.0    0    0    0    1  0.0\n> [6,]  0.1    0    0    0    0  1.0\neigen(mat.cor)$values\n> [1] 1.1 1.0 1.0 1.0 1.0 0.9\nvp <- rbind(vp,\n      data.frame(Position = Position,\n                 Valeurs.propres = 1,\n                 Test = rep(\"Kaiser\", p))\n      )\n\nggplot(data = vp,\n       mapping = aes(x = Position, \n                     y = Valeurs.propres, \n                     color = Test)) +\n  geom_line() + \n  geom_point() +\n  ylab(\"Valeur propre\") +\n  theme(legend.position.inside = c(0.8, 0.8))\nkaiser <- function(jd){\n  # Obtenir la matrice de corrélation\n  R <- cor(jd)\n  \n  # Extraire uniquement les valeurs propres\n  eig <- eigen(R, only.values = TRUE)$values\n  \n  # Les valeurs propres plus grandes que 1\n  sum(eig > 1)\n}"},{"path":"réduire.html","id":"lanalyse-parallèle-de-horn","chapter":" 20 Réduire","heading":"20.3.2 L’analyse parallèle de Horn","text":"Asymptotiquement parlant, si \\(n \\\\infty\\), le test est vrai. Par contre, l’ordre des valeurs propres capitalise sur l’erreur, ce qui introduit l’idée selon laquelle les corrélations accidentelles se retrouvent dans les premières dimensions. Même s’il n’y pas de corrélation sur le plan de la population, il y aura accidentellement des corrélations dans un échantillon. Conséquemment, les premiers axes sont surestimés, alors que les derniers sont sous-estimés.La solution proposée par Horn (1965) est alors de tenir compte de cette erreur d’échantillonnage accidentelle. Pour ce faire, il propose de rééchantillonner les valeurs propres d’un jeu de données sans aucun facteur (aucune corrélation entre les variables) ayant les mêmes caractéristiques que le jeu de données cible (mêmes nombres de variables et de participants). Le jeu de données cible correspond au jeu de données de l’expérimentateur.Le rééchantillonnage est réitéré sur des milliers de jeu de données artificiels. À chaque fois, les valeurs propres sont enregistrées. Des valeurs critiques (moyennes ou percentiles) en sont retirées à la toute fin. Il s’agit du critère auquel l’hypothèse nulle est rejetée. De la première à la dernière, chaque valeur propre cible est comparée à la valeur propre moyenne correspondante. Si la cible est plus élevée, il s’agit d’une dimension à retenir. Dès que la cible est inférieure, le test est arrêté. Le nombre de valeurs propres supérieures aux valeurs propres artificielles correspond au nombre de dimensions à retenir.\nDans la syntaxe ci-dessous, les mêmes caractéristiques que le jeu de données précédent sont utilisés, soit \\(p = 6\\) et \\(n = 5000\\). À la fin, il sera possible de comparer s’il y effectivement deux facteurs selon l’analyse parallèle.Pour connaître le nombre de dimensions, il suffit de tester les valeurs propres empiriques et les comparer aux valeurs propres simulées (moyenne ou 95e percentile). Le nombre de valeur propre empirique supérieur aux valeurs propres simulés correspond au nombre de composantes à retenir.Ainsi, 2 facteurs sont à retenir. Une façon d’illustrer les résultats de l’analyse parallèle est d’utiliser le graphique des éboulis en y représentant les valeurs propres empiriques comparativement aux simulées. Le nombre de valeurs propres empiriques supérieures aux simulées est le nombre de dimensions à retenir. Ce graphique est produit à la Figure 20.4. Voici la syntaxe pour produire ce graphique avec ggplot2. La première étape est de mettre en commun les résultats obtenus dans un jeu de données. La variable Position indique l’ordre de la valeur propre, valeurs.propres contient les valeurs propres et Test indique s’il s’agit de valeurs propres empiriques ou simulées de l’analyse parallèle. Pour le reste, il s’agit de recourir à ggplot().\nFigure 20.4: Comparaison des valeurs propres empiriques et simulées\nVoici une fonction pour programmer l’analyse parallèle.","code":"# Pour la reproductibilité\nset.seed(1019)\np <- 6        # Nombre de variables\nn <- 50       # Nombre de participants \nnreps <- 1000 # Nombre d'itérations\nvaleurs.propres <- matrix(0,            # Création d'une variable\n                          nrow = p,     # vide pour enregistrer\n                          ncol = nreps) # les valeurs propres\n\n# Création d'une matrice de corrélation vide\nR <- diag(p)\n\n# Création de la boucle                                        \nfor(i in 1:nreps) {\n  D <- MASS::mvrnorm(n = n,            # Création de n * p valeurs\n                     mu = rep(0, p),   # aléatoires accidentellement\n                     Sigma = R)        # corrélées\n  \n  ACP <- eigen(cor(D),                 # Réaliser l'ACP\n               only.values = TRUE)        \n  \n  valeurs.propres[,i] <- ACP$values    # Enregistrer les valeurs propres\n}\n\n# Une fois les valeurs propres obtenues,\n# les moyennes et quantiles sont calculables\n\n# Une moyenne par ligne (valeurs propres)\nrowMeans(valeurs.propres)\n> [1] 1.495 1.242 1.057 0.899 0.739 0.568\n\n# Le 95e percentile (si préféré)\napply(X = valeurs.propres, \n      FUN = quantile,\n      MARGIN = 1,\n      probs = .95)\n> [1] 1.723 1.388 1.163 0.998 0.858 0.700# Les valeurs propres empiriques\nres$values\n> [1] 2.576 1.699 0.771 0.421 0.328 0.205\n\n# La moyenne des valeurs propres simulées\nrowMeans(valeurs.propres)\n> [1] 1.495 1.242 1.057 0.899 0.739 0.568\n\n# Nombre de valeurs propres empiriques supérieures aux simulées\nsum(res$values > rowMeans(valeurs.propres))\n> [1] 2\nvp <- rbind(vp,\n      data.frame(Position = Position,\n                 Valeurs.propres = rowMeans(valeurs.propres),\n                 Test = rep(\"Analyse parallèle\", p))\n      )\n\nggplot(data = vp,\n       mapping = aes(x = Position, \n                     y = Valeurs.propres, \n                     color = Test)) +\n  geom_line() + \n  geom_point() +\n  ylab(\"Valeur propre\") +\n  theme(legend.position.inside = c(0.8, 0.8))\nanalyse.parallele <- function(jd, \n                              p = ncol(jd), \n                              n = nrow(jd), \n                              nreps = 1000,\n                              alpha = .05){\n \n  # Extraire uniquement les valeurs propres\n  eig <- eigen(cor(jd), only.values = TRUE)$values\n\n  # Création d'une variable vide pour enregistrer\nvaleurs.propres = matrix(0,            \n                         nrow = p,     \n                         ncol = nreps) \n\n# Création d'une matrice de corrélation vide\nR <- diag(p)\n\n# Création de la boucle                                        \nfor(i in 1:nreps) {\n  D <- MASS::mvrnorm(n = n,            # Création de n * p valeurs\n                     mu = rep(0, p),   # aléatoires accidentellement\n                     Sigma = R)        # corrélées\n  \n  ACP <- eigen(cor(D),                 # Réaliser l'ACP\n               only.values = TRUE)        \n  \n  valeurs.propres[,i] <- ACP$values    # Enregistrer les valeurs propres\n}\n\n# Le 95e percentile\nvp.crit <- apply(X = valeurs.propres, \n                 FUN = quantile,\n                 MARGIN = 1,\n                 probs = 1 - alpha)\n\n# Les valeurs propres empiriques plus grandes \n# que celles simulées\nsum(eig > vp.crit) \n}"},{"path":"réduire.html","id":"test-de-suffisance-de-la-prochaine-valeur-propre-dachim","chapter":" 20 Réduire","heading":"20.3.3 Test de suffisance de la prochaine valeur propre d’Achim","text":"L’un des problèmes de l’analyse parallèle est que, bien qu’elle tient compte de l’erreur d’échantillonnage, elle ne tient pas compte de la logique séquentielle des tests d’hypothèses des valeurs propres (Beauducel, 2001; Turner, 1998). Précisément, le test d’hypothèses sous-jacent, en termes de \\(k\\) dimensions à retenir, est\n\\[\nH_0 : k = 0 \\\\\nH_1 : k \\geq1\n\\]Autrement dit, il n’y pas de facteurs, ou bien il y plus d’un facteur. Une fois la première dimension déterminée par l’analyse parallèle, il est inadéquat de tester les suivantes avec la même matrice de corrélation. Il faut tenir compte de cette nouvelle information : il y au moins \\(k\\) dimensions.Le Test de suffisance de la prochaine valeur propre (Next Eigenvalue Sufficiency Test, NEST) développé par Achim (2017, 2020) est le dernier cri en termes d’estimation du nombre de composantes à retenir. En plus de tenir compte pour l’erreur d’échantillonnage, comme l’analyse parallèle, il tient compte aussi de la logique séquentielle du test d’hypothèse. Plus précisément, le test utilise une matrice de corrélation contenant les \\(k\\) dimensions déterminées auparavant. Lorsque \\(k=0\\), le test est équivalent à l’analyse parallèle, mais uniquement pour la première valeur propre. Une fois l’hypothèse nulle rejetée, \\(k\\) est incrémenté, une nouvelle matrice de corrélation basée sur ces \\(k\\) facteurs est calculées, et un nouveau rééchantillonnage des valeurs propres est entrepris. Le test s’arrête lorsque la \\(k+1\\) valeur propre (l’hypothèse nulle) n’est pas rejetée, pour donner \\(k\\) dimensions.La syntaxe suivante illustre de façon simplifiée le test NEST. Elle utilise plusieurs éléments déjà présentés.Elle est basée sur l’analyse parallèle. Le changement principal est que la matrice de corrélation change pour différente valeur de \\(k\\), alors qu’elle reste toujours identique pour l’analyse parallèle, R = diag(p), peu importe \\(k\\).Elle est basée sur l’analyse parallèle. Le changement principal est que la matrice de corrélation change pour différente valeur de \\(k\\), alors qu’elle reste toujours identique pour l’analyse parallèle, R = diag(p), peu importe \\(k\\).Comme l’analyse parallèle, la fonction utilise une forme de rééchantillonnage qui est en fait du bootstrap.Comme l’analyse parallèle, la fonction utilise une forme de rééchantillonnage qui est en fait du bootstrap.Elle utilise factanal() (voir Explorer) pour reconstruire la matrice de corrélation à \\(k\\) facteurs.Elle utilise factanal() (voir Explorer) pour reconstruire la matrice de corrélation à \\(k\\) facteurs.La sortie ici, nfactors représente le nombre de dimensions à retenir.","code":"\nmat.cor <- cor(jd)  # Matrice de corrélation du jeu de données\np <- ncol(mat.cor)  # Nombre de variables (6)\nn <- nrow(jd)       # Nombre de participants (50)\nnreps <- 1000       # Nombre d'itérations\n\n# Test séquentiel des valeurs propres\nfor (k in 0:(p-1)){ # Pour 0 à (p-1) dimensions\n  \n  # Déterminer la matrice de corrélation pour k dimensions\n  if(k == 0) {      # Pour tester 0 dimension (analyse parallèle)\n    \n    R <- diag(p)    # Matrice de corrélation à 0 dimension\n    \n  } else {          # Pour tester k dimensions\n    \n    fa <- factanal(covmat = mat.cor, n.obs = n, factors = k)\n    R <- fa$loadings %*% t(fa$loadings)\n    diag(R) <- 1    # Matrice de corrélation à k dimensions\n  }\n  \n  # Rééchantillonnage des valeurs propres\n  # Création d'une variable vide pour enregistrer\n  valeurs.propres = matrix(0,            \n                           nrow = p,     \n                           ncol = nreps) \n  \n  # Création de la boucle                                        \n  for(i in 1:nreps) {\n    \n    # Création de n * p valeurs aléatoires corrélées\n    D <- MASS::mvrnorm(n = n,            \n                       mu = rep(0, p),   \n                       Sigma = R)\n    \n     # Réaliser l'ACP\n    ACP <- eigen(cor(D),                \n                only.values = TRUE)  \n    \n     # Enregistrer les valeurs propres\n    valeurs.propres[,i] <- ACP$values   \n  }\n  \n  # Tester la k+1 valeurs propres à alpha = .05\n  vp.crit <- quantile(valeurs.propres[k+1,], probs = .95)\n  \n  # Valeur propre empirique k testée à la valeur simulée\n  # Si la valeur propre est plus élevée, NEST continue,\n  # si la valeur propre est plus petite, NEST arrête.\n  if(!(res$values[k + 1] > vp.crit)){break}\n}\n\n# Nombre de facteurs à retenir\nnfactors = k"},{"path":"réduire.html","id":"le-package-rnest","chapter":" 20 Réduire","heading":"20.3.3.1 Le package Rnest","text":"Il existe un package permettant de réaliser facilement NEST en plus de fournir quelques fonctions utiles. Le package est Rnest (Caron, 2023) est disponible par GitHub (pour la version en développement) et CRAN. Les deux version sont importables sur R avec la syntaxe suivante.Comme à l’habitude, il faut l’appeler dans l’environnement.La fonction principale est nest(). Elle prend en argument un jeu de données ou une matrice de corrélation avec le nombre de participants. D’autres options sont possibles, comme le seuil critique (alpha), le nombre de répétitions (nreps), et la méthode d’extraction des dimensions (\"ml\", maximum likelihood; \"paf\", principal axis factoring; \"mrfa\", minimum rank factor analysis)Voici la fonction avec le présent exemple.Voici la sortie des valeurs propres comparées aux deux autres règles d’arrêt illustrée à la Figure 20.5.\nFigure 20.5: Comparaison des valeurs propres empiriques et simulées (analyses parallèle et nest)\nUn bel avantage du package est sa possibilité de créer un graphique pour différentes valeurs critiques alpha. La Figure 20.6 illustre la sortie.\nFigure 20.6: Graphique produit par nest de Rnest\n","code":"\n# CRAN\ninstall.packages(\"Rnest\")\n\n# GitHub\nremotes::install_github(repo = \"quantmeth/Rnest\")\nlibrary(Rnest)nest(jd)\n> At 95% confidence, Nest Eigenvalue Sufficiency Test (NEST) suggests 2 factors.\n# Réorganisation du jeu de données contenant\n# les valeurs propres\nvp.nest <- plot(nest(jd))$data[-(1:p),]\ncolnames(vp.nest) <- colnames(vp)\nvp.nest$Test <- \"NEST\" \nvp <- rbind(vp, vp.nest)\n\nggplot(data = vp,\n       mapping = aes(x = Position, \n                     y = Valeurs.propres, \n                     color = Test)) +\n  geom_line() + \n  geom_point() +\n  ylab(\"Valeur propre\") +\n  theme(legend.position.inside = c(0.8, 0.8))\nplot(nest(jd, alpha = c(.01, .025, .05, .10)))"},{"path":"réduire.html","id":"comparaisons-des-règles-darrêts","chapter":" 20 Réduire","heading":"20.3.4 Comparaisons des règles d’arrêts","text":"À ce stage, les trois règles d’arrêt ont sorti le même nombre de facteurs. Elles semblent ne devenir qu’inutilement alambiqué. Il revient à l’exemple original d’être très simples : des loadings assez saillants, des valeurs propres élevées et une structure simple et orthogonale. Voici un exemple plus complexe.Le package Rnest founir une matrice de corrélation basée sur 4 facteurs corrélées. La matrice se nomme ex_4factors_corr et la Figure 20.7 représente la structure factorielle sous-jacente de cette matrice de corrélation. Il s’agit de quatre facteurs avec trois items chacun avec des loadings respectif de .9, .9 et .3 et deux paires de corrélation interfacteur de .7.\nFigure 20.7: Structure factorielle de ex_4factors_corr\nÀ partir de cette matrice, il est possible de créer un jeu de données pour 2500 participants.La Figure 20.8 les valeurs propres du jeu de données.\nFigure 20.8: Les valeurs propres de ex_4factors_corr\nIl s’agit d’une structure très difficile pour les règles d’arrêt basées sur les valeurs propres, comme le test de Kaiser, l’analyse parallèle et NEST. Voici les tests comparés.Ici NEST montre clairement sa supériorité en étant sensible aux petites valeurs propres. Il trouve bien quatre facteurs, alors que le test en trouve accidentellement trois (à cause de l’erreur d’échantillonnage) et l’analyse parallèle en trouve deux.","code":"\nset.seed(1)\njd2 <- MASS::mvrnorm(n = 2500,\n                     mu = rep(0, ncol(ex_4factors_corr)),\n                     Sigma = ex_4factors_corr)kaiser(jd2)\n> [1] 3\nanalyse.parallele(jd2)\n> [1] 2\nnest(jd2)$nfactors\n>     nfactors\n> 95%        4"},{"path":"exercice-factoriel.html","id":"exercice-factoriel","chapter":"Exercices","heading":"Exercices","text":"","code":""},{"path":"exercice-factoriel.html","id":"question-1-1","chapter":"Exercices","heading":"Question 1","text":"Créer un jeu de données pour la structure de la Figure 20.9. Le jeu de données est standardisé et contient 584 sujets.\nFigure 20.9: Structure factorielle de ex_4factors_corr\n","code":""},{"path":"exercice-factoriel.html","id":"question-2-1","chapter":"Exercices","heading":"Question 2","text":"Utiliser la fonction eigen() pour extraire les valeurs propres, la variance expliquée de chacune d’elle et les loadings du jeu de données de la Question 1.","code":""},{"path":"exercice-factoriel.html","id":"question-3-1","chapter":"Exercices","heading":"Question 3","text":"Créer une fonction maison pour le test de Kaiser et utiliser le jeu de données créé à la Question 1.","code":""},{"path":"exercice-factoriel.html","id":"question-4-1","chapter":"Exercices","heading":"Question 4","text":"Créer une fonction maison pour l’analyse parallèle et utiliser le jeu de données créé à la Question 1.","code":""},{"path":"exercice-factoriel.html","id":"question-5-1","chapter":"Exercices","heading":"Question 5","text":"Utiliser Rnest avec le jeu de données créé à la Question 1. Produire une graphique.","code":""},{"path":"exercice-factoriel.html","id":"question-6-1","chapter":"Exercices","heading":"Question 6","text":"Utiliser factanal() pour 3 facteurs avec le jeu de données créé à la Question 1. Extraire les scores et les loadings.","code":""},{"path":"solutions.html","id":"solutions","chapter":"Solutions","heading":"Solutions","text":"","code":""},{"path":"solutions.html","id":"rudiments","chapter":"Solutions","heading":"Rudiments","text":"","code":""},{"path":"solutions.html","id":"question-1-2","chapter":"Solutions","heading":"Question 1","text":"Quel est le résultat de mean <- c(1, 2, 3)? Pourquoi?Le résultat est \\(1, 2, 3\\). Le fait d’assigner une valeur à une fonction (ou variable) écrase cette dernière.Voir Les variables pour plus de renseignements.","code":"mean <- c(1, 2, 3)\nmean\n> [1] 1 2 3"},{"path":"solutions.html","id":"question-2-2","chapter":"Solutions","heading":"Question 2","text":"Quelle est la différence entre # Caractère et \"Caractère\"?Le premier est une commentaire (voir Les commentaires) alors que le second est une chaîne de caractère (voir Les chaînes de caractère).","code":""},{"path":"solutions.html","id":"question-3-2","chapter":"Solutions","heading":"Question 3","text":"Créer un vecteur contenant les valeurs \\(4, 10, 32\\). Calculer la moyenne et l’écart type de ce vecteur.Voir Concaténer pour créer un vecteur et la moyenne et l’écart type pour plus d’informations.","code":"vecteur <- c(4, 10, 32)\nmean(vecteur)\n> [1] 15.3\nsd(vecteur)\n> [1] 14.7"},{"path":"solutions.html","id":"question-4-2","chapter":"Solutions","heading":"Question 4","text":"Créer un vecteur contenant les valeurs de \\(4\\) à \\(11\\). Sélectionner la deuxième valeur de ce vecteur, puis additionner 100 à cette valeur et remplacer la dans le vecteur.Voir Concaténer pour créer un vecteur et Référer à des sous-éléments pour choisir un élément.","code":"\nvecteur <- 4:11 # ou seq(4, 11)\nvecteur[2] <- vecteur[2] + 100"},{"path":"solutions.html","id":"question-5-2","chapter":"Solutions","heading":"Question 5","text":"Générer 10 valeurs aléatoires distribuées normalement avec une moyenne de 50 et un écart type de 4. Calculer la moyenne, la médiane et la variance.Voir Les graines pour la fonction set.seed(), Les distributions pour la fonction rnorm() ainsi que son aide ?rnorm pour les arguments spécifiques, et la moyenne, la médiane et la variance pour plus d’informations.","code":"# Pour la reproductibilité\nset.seed(42)\n\n# Créer\nx <- rnorm(n = 10, mean = 50, sd = 4)\n\n# La moyenne, la médiane et la variance\nmean(x)\n> [1] 52.2\nmedian(x)\n> [1] 51.5\nvar(x)\n> [1] 11.2"},{"path":"solutions.html","id":"question-6-2","chapter":"Solutions","heading":"Question 6","text":"Créer un jeu de données contenant quatre sujets avec, pour chacun, leur nom de famille, leur âge et un score d’appréciation tiré d’une distribution uniforme allant de 0 à 100.Voir Les graines pour la fonction set.seed(), Créer un jeu de données pour data.frame() et Concaténer pour les vecteurs en arguments, Les distributions pour la fonction runif() ainsi que son aide ?runif pour les arguments spécifiques. Les noms sont des chaînes de caractère.","code":"# Pour la reproductibilité\nset.seed(1234)\n\n# Créer\njd <- data.frame(nom = c(\"Eccleston\", \"Tennant\", \"Smith\", \"Capaldi\"),\n                 age = c(41, 35, 28, 55),\n                 score = runif(n = 4, min = 0, max = 100))\n\n# Imprimer le jeu dans la console\njd\n>         nom age score\n> 1 Eccleston  41  11.4\n> 2   Tennant  35  62.2\n> 3     Smith  28  60.9\n> 4   Capaldi  55  62.3"},{"path":"solutions.html","id":"question-7-1","chapter":"Solutions","heading":"Question 7","text":"Rédiger une fonction calculant l’hypoténuse d’un triangle rectangle. Rappel: le théorème de Pythagore est \\(c^2=^2+b^2\\).En prenant et b comme arguments.Voir Les fonctions pour plus d’informations.","code":"hypothenus <- function(a, b){\n  sqrt(a^2 + b^2)\n}\nhypothenus(3, 4)\n> [1] 5"},{"path":"solutions.html","id":"question-8","chapter":"Solutions","heading":"Question 8","text":"Rédiger une fonction calculant un score-\\(z\\) pour une variable. Rappel: un score-\\(z\\), correspond à \\(z=\\frac{x-\\mu}{\\sigma}\\).Deux possibilités ici. Soit x est un vecteur contenant plusieurs nombres ou il est un nombre et il faut ajouter la moyenne (\\(\\mu\\)) et l’écart type (\\(\\sigma\\)).Le premier cas est l’équivalent de la fonction scale() qui rempliera la même tâche.Voir Les fonctions pour plus d’informations. Les scores-\\(z\\) sont abordés plus en profondeur à la section Inférence avec la distribution normale sur une unité.","code":"# Premier cas\nscore.z <- function(x) {\n  (x-mean(x))/sd(x)\n} \n\n# Deuxième cas\nscore.z <- function(x, mu, sigma) {\n  (x-mu)/sigma\n} \n\nscore.z(130, mu = 100, sigma = 15)\n> [1] 2"},{"path":"solutions.html","id":"question-9","chapter":"Solutions","heading":"Question 9","text":"Rédiger une fonction calculant la médiane d’une variable (ne pas recopier celle de ce livre).Voici une autre fonction calculant une médiane d’un vecteur.Encore une fois (il n’y pas vraiment de façon de l’éviter), %%2 permet de tester si pair (FALSE, car 0 si pair) ou impair (TRUE, car 1 si impair), puis de prendre la valeur à la position correspondante. Il y plusieurs façons de tirer les éléments du vecteur.Voir Les fonctions, mais surtout La médiane, pour plus d’informations.","code":"\nmediane <- function(x){\n  x <- sort(x)\n  longueur <- length(x)\n  if (longueur%%2) {\n    # Si impair\n    # Prendre la valeur centrale\n    x[(longueur + 1)/2]\n  } else {\n    # Si pair\n    # Faire la moyenne des deux éléments du centre\n    mean(x[rep(longueur/2, 2) + 0:1])\n  }\n}"},{"path":"solutions.html","id":"question-10","chapter":"Solutions","heading":"Question 10","text":"Rédiger une fonction qui pivote une liste de \\(k\\) éléments par \\(n\\). Par exemple, une liste de six (\\(k=6\\) comme \\([1,2,3,4,5,6]\\)) pivoté de deux (\\(n=2\\)) devient (\\([3,4,5,6,1,2]\\)).Plusieurs solutions possibles dont en voici une.","code":"pivot <- function(k, n){\n  pivoter <- c((n+1):k, 1:n)\n  pivoter\n}\n# Original\nk <- 6\n1:k\n> [1] 1 2 3 4 5 6\n\n# Pivoté\npivot(k = k, n = 2)\n> [1] 3 4 5 6 1 2"},{"path":"solutions.html","id":"question-11","chapter":"Solutions","heading":"Question 11","text":"Rédiger une fonction pour générer une séquence de Fibonacci (chaque nombre est la somme des deux précédents) jusqu’à une certaine valeur, soit \\(1, 1, 2, 3, 5, 8,...\\). (Question difficile)Plusieurs solutions possibles dont en voici deux.En voici une autre en utilisant la récursion (une fonction qui s’appelle elle-même).Les voici comparées.","code":"\nfibonnaci1 <-  function(n){\n  # n est le nombre d'éléments de la série demandée.\n  # Création d'un vecteur de taille n ne contenant que des 1.\n  serie <- rep(1, n)\n  \n  # Le premier test logique rapporte la (n = 1) ou \n  # les deux (n = 2) premières valeurs.\n  if(n <= 2){\n    serie <-  serie[1:n]\n    \n    # Le deuxième réalise les calculs pour les autres options.  \n  }else{\n    for(i in 3:n){\n      serie[i] <-  serie[i-1] + serie[i-2]  \n    }\n  }\n  serie\n}\nfibonnaci2 <- function(n){\n  if(n <= 2){\n    # Si n est plus petit que 2, alors retourne 1\n    1\n  } else {\n    # Autrement, retourne les deux valeurs de Fibonncaci précédentes.\n    fibonnaci2(n-1) + fibonnaci2 (n-2)\n  }\n}n <- 8\nfibonnaci1(n)\n> [1]  1  1  2  3  5  8 13 21\nfibonnaci2(n)\n> [1] 21"},{"path":"solutions.html","id":"jeux-de-données","chapter":"Solutions","heading":"Jeux de données","text":"","code":""},{"path":"solutions.html","id":"question-1-3","chapter":"Solutions","heading":"Question 1","text":"À l’aide de data_edit() du package DataEditR, créez un jeu données contenant trois participants ayant les caractéristiques suivantes, nom = Alexandre, Samuel et Vincent et age = 20, 22 et 31.La Figure 20.10 montre le résultat dans le tableur.\nFigure 20.10: Entrée des données\n","code":"\n# Installer et appeler le package DataEditR, si ce n'est fait\njd <- DataEditR::data_edit()"},{"path":"solutions.html","id":"question-2-3","chapter":"Solutions","heading":"Question 2","text":"Prendre le jeu de données cars, sélectionner la variable dist et transformer la en mètre, plutôt qu’en pieds. Rappel: un mètre égale 3.2808 pieds.Avec le tidyverse de télécharger.La fonction head() permet d’afficher seulement les six premières lignes au lieu des 50 du jeu de données.","code":"cars %>%                               # Le jeu de données\n  select(dist) %>%                     # Sélectionner\n  mutate(dist_m = dist / 3.2808)  %>%  # Transformer\n  head()                               # Montrer les 6 premières lignes\n>   dist dist_m\n> 1    2   0.61\n> 2   10   3.05\n> 3    4   1.22\n> 4   22   6.71\n> 5   16   4.88\n> 6   10   3.05"},{"path":"solutions.html","id":"question-3-3","chapter":"Solutions","heading":"Question 3","text":"Dans le jeu de données iris, calculer la moyenne et l’écart type de la longueur de sépale (Petal.Length) en fonction de l’espèce (species). Représenter ensuite la moyenne à l’aide d’un diagramme à barreAvec le tidyverse de télécharger.Voir La moyenne, L’écart type et Manipuler. Il est aussi possible d’utiliser psych::describeBy(iris, group = \"Species\") (voir Décrire), mais cela offre plus que demandé.Pour la figure, il faut prendre le résultat de la manipulation précédente et utiliser ggplot2 avec la représentation géométrique geom_col() et la cartographie mapping = aes(x = Species, y = M), ce qui donne l’espèce à l’abscisse et la moyenne (M) à l’ordonnée.\nFigure 20.11: M par Espèce (question 3)\n","code":"iris %>% \n  group_by(Species) %>% \n  summarise(M = mean(Petal.Length), ET = sd(Petal.Length))\n> # A tibble: 3 × 3\n>   Species        M    ET\n>   <fct>      <dbl> <dbl>\n> 1 setosa      1.46 0.174\n> 2 versicolor  4.26 0.470\n> 3 virginica   5.55 0.552\niris %>% \n  group_by(Species) %>% \n  summarise(M = mean(Petal.Length), ET = sd(Petal.Length)) %>% \n  ggplot(mapping = aes(x = Species, y = M)) +\n  geom_col()"},{"path":"solutions.html","id":"question-4-3","chapter":"Solutions","heading":"Question 4","text":"Prenez le jeu de données mtcars et produisez un diagramme de dispersion montrant la puissance brute (en chevaux) (hp) par rapport à consommation en km/l (basé sur mpg) tout en soulignant l’effet du nombre de cylindres (cyl). Attention la fonction as_factor permettra d’utiliser cyl en facteur et le rapprt mpg vers kml approximativement \\(.425\\).\nFigure 20.12: Visualisation de kml par hp en fonction de cyl (question 4)\n","code":"\nmtcars %>% \n  mutate(kml = .425 * mpg,\n         cyl = as_factor(cyl)) %>% \n  ggplot(mapping = aes(x = hp, y = kml, color = cyl)) + \n  geom_point()"},{"path":"solutions.html","id":"question-5-3","chapter":"Solutions","heading":"Question 5","text":"Avec le même jeu de données et objectif que la question précédente, ajouter une droite de prédiction avec geom_smooth() selon un modèle linéare (lm) et sans erreur standard (se).\nFigure 20.13: Visualisation de kml par hp en fonction de cyl (question 5)\n","code":"mtcars %>% \n  mutate(kml = .425 * mpg,\n         cyl = as_factor(cyl)) %>% \n  ggplot(mapping = aes(x = hp, y = kml, color = cyl)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n> `geom_smooth()` using formula = 'y ~ x'"},{"path":"solutions.html","id":"question-6-3","chapter":"Solutions","heading":"Question 6","text":"Avec le jeu de données chickwts, produire une boîte à moustache du poids des poulets en fonction de leur alimentation.\nFigure 20.14: Visualiation de weight par feed (question 6)\nDans cette question, l’utilisation du filtre filter(feed %% c(\"horsebean\", \"sunflower\")) pourra poser quelques difficultés, mais ne devrait arriver assez rarement en pratique.","code":"\nchickwts %>% \n  ggplot(mapping = aes(x = feed, y = weight)) +\n  geom_boxplot()"},{"path":"solutions.html","id":"question-7-2","chapter":"Solutions","heading":"Question 7","text":"Prenez le jeu de données mtcars et produisez un histogramme montrant la variabilité de la consommation mpg par rapport à la transmission (). Attention la fonction as_factor permettra d’utiliser en facteur.\nFigure 20.15: Histogramme de mpg par (question 7-1)\nNoter que bins = 12 ne sert qu’à éviter un avertissement.Pour avoir les histogrammes dans des cadrans différents, la fonction facet_wrap() est utile.\nFigure 20.16: Histogramme de mpg par (question 7-2)\n","code":"\nmtcars %>% \n  mutate(am = as_factor(am)) %>% \n  ggplot(mapping = aes(x = mpg, fill = am)) + \n  geom_histogram(bins = 12) # Pour éviter un avertissement\nmtcars %>% \n  mutate(am = as_factor(am)) %>% \n  ggplot(mapping = aes(x = mpg)) + \n  geom_histogram(bins = 12)+\n  facet_wrap(~am)"},{"path":"solutions.html","id":"question-8-1","chapter":"Solutions","heading":"Question 8","text":"Prendre le jeu de données msleep et produire un diagramme à barres pour observer la fréquence des régimes alimentaires.\nFigure 20.17: Diagramme à barre du régime alimentaire (question 8)\n","code":"\n# msleep fait parti du package `ggplot2`\nmsleep %>% \n  ggplot(mapping = aes(x = vore)) + \n  geom_bar()"},{"path":"solutions.html","id":"question-9-1","chapter":"Solutions","heading":"Question 9","text":"Prendre le jeu de données msleep et produisez une boîte à moustache pour observer le temps total de sommeil (sleep_total) moyen par rapport aux régimes (vore). Attention aux données manquantes.\nFigure 20.18: Diagramme à barre du régime alimentaire (question 8)\n","code":"\nmsleep %>% \n  na.omit() %>% \n  ggplot(mapping = aes(x = vore, y= sleep_total)) + \n  geom_boxplot()"},{"path":"solutions.html","id":"question-10-1","chapter":"Solutions","heading":"Question 10","text":"Avec le jeu de données chickwts, produire un diagramme à barres du poids moyen des poulets par rapport à leur alimentation en ne conservant que les graines de tournesols et les fèveroles.\nFigure 20.19: Visualiation de weight par feed (question 10)\n","code":"\nchickwts %>% \n  filter(feed %in% c(\"horsebean\", \"sunflower\")) %>% \n  group_by(feed) %>% \n  summarise(poids = mean(weight)) %>% \n  ggplot(mapping = aes(x = feed, y = poids)) +\n  geom_col()"},{"path":"solutions.html","id":"statistiques","chapter":"Solutions","heading":"Statistiques","text":"","code":""},{"path":"solutions.html","id":"question-1-4","chapter":"Solutions","heading":"Question 1","text":"Avec le jeu de données mtcars, réaliser une analyse descriptive complète. Ne conservez que la moyenne, l’écart type, l’asymétrie et l’aplatissement.Voir Les variables à échelles continues pour l’utilisation de psych::describe() et Référer à des sous-éléments pour l’extraction des colonnes [c(\"mean\",\"sd\",\"skew\",\"kurtosis\")].","code":"psych::describe(mtcars)[c(\"mean\",\"sd\",\"skew\",\"kurtosis\")]\n>        mean     sd  skew kurtosis\n> mpg   20.09   6.03  0.61    -0.37\n> cyl    6.19   1.79 -0.17    -1.76\n> disp 230.72 123.94  0.38    -1.21\n> hp   146.69  68.56  0.73    -0.14\n> drat   3.60   0.53  0.27    -0.71\n> wt     3.22   0.98  0.42    -0.02\n> qsec  17.85   1.79  0.37     0.34\n> vs     0.44   0.50  0.24    -2.00\n> am     0.41   0.50  0.36    -1.92\n> gear   3.69   0.74  0.53    -1.07\n> carb   2.81   1.62  1.05     1.26"},{"path":"solutions.html","id":"question-2-4","chapter":"Solutions","heading":"Question 2","text":"Avec le jeu de données CO2, faire une table de contingence entre Treatment et Type.Voir Les variables à échelles nominales pour l’utilisation de table() et Référer à des sous-éléments pour l’extraction des colonnes [c(\"Type\",\"Treatment\")] du jeu de données.","code":"table(CO2[c(\"Type\",\"Treatment\")])\n>              Treatment\n> Type          nonchilled chilled\n>   Quebec              21      21\n>   Mississippi         21      21"},{"path":"solutions.html","id":"question-3-4","chapter":"Solutions","heading":"Question 3","text":"Produire les valeurs-\\(t\\) critiques pour \\(dl = 1,2,3 ,... ,30\\) et \\(\\alpha=.05\\) unilatérale.","code":"alpha <- .05\nqt(1 - alpha, df = 1:30) # ou qt(alpha, df = 1:30, lower.tail = FALSE) \n>  [1] 6.31 2.92 2.35 2.13 2.02 1.94 1.89 1.86 1.83 1.81 1.80\n> [12] 1.78 1.77 1.76 1.75 1.75 1.74 1.73 1.73 1.72 1.72 1.72\n> [23] 1.71 1.71 1.71 1.71 1.70 1.70 1.70 1.70"},{"path":"solutions.html","id":"question-4-4","chapter":"Solutions","heading":"Question 4","text":"Comparer la puissance de la distribution-\\(t\\) avec 20 degrés de liberté par rapport à une distribution normale centrée réduite avec une \\(\\alpha = .05\\) bilatérale. L’hypothèse alternative est distribuée normalement et fixée à une moyenne de 2 et l’écart type est de 1.","code":"# Fixer l'alpha\nalpha <- .05\n\n# Valeurs critiques pour t et z, bilatérale (alpha/2)\n# Côté supérieur avec `lower.tail = FALSE`\ncrit.t <- qt(alpha/2, df = 20, lower.tail = FALSE)\ncrit.z <- qnorm(alpha/2, lower.tail = FALSE)\n\n# Calculer la puissance pour les deux valeurs critiques\n# Fixer la moyenne à 2\npnorm(c(crit.t, crit.z), mean = 2, lower.tail = FALSE)\n> [1] 0.466 0.516"},{"path":"solutions.html","id":"question-5-4","chapter":"Solutions","heading":"Question 5","text":"Calculer la puissance d’une corrélation de \\(\\rho = .30\\) avec 80 participants et un \\(\\alpha = .05\\) bilatérale. Rappel : une corrélation peut se standardiser avec la tangente hyperbolique inverse, soit atanh(), et en multipliant par l’erreur type, \\(\\sqrt{n-3}\\). (Question difficile)L’équation (11.5) de la section sur la corrélation permet de transformer ,","code":"# Fixer les paramètres\nr <- .30\nalpha <- .05\nn <- 80\n\n# Standardiser la corrélation\nr.std <- atanh(r) * (sqrt(n-3))\n\n#trouver la valeur critique de rejet\nr.crit <- qnorm(alpha/2, lower.tail = FALSE)\n\n#Calculer la puissance\npnorm(r.crit, mean = r.std, lower.tail = FALSE)\n> [1] 0.775"},{"path":"solutions.html","id":"question-6-4","chapter":"Solutions","heading":"Question 6","text":"Avec le jeu de données ToothGrowth, réaliser un test-\\(t\\) afin de comparer les supp par rapport à la longueur des dents (len).","code":"t.test(len ~ supp, data = ToothGrowth)\n> \n>   Welch Two Sample t-test\n> \n> data:  len by supp\n> t = 2, df = 55, p-value = 0.06\n> alternative hypothesis: true difference in means between group OJ and group VC is not equal to 0\n> 95 percent confidence interval:\n>  -0.171  7.571\n> sample estimates:\n> mean in group OJ mean in group VC \n>             20.7             17.0"},{"path":"solutions.html","id":"question-7-3","chapter":"Solutions","heading":"Question 7","text":"Avec le jeu de données sleep, faire un test-\\(t\\) permettant de comparer les deux temps de mesure nommés group par rapport à la variable dépendante extra.","code":"t.test(extra ~ group, data = sleep, paired = TRUE)\n> \n>   Paired t-test\n> \n> data:  extra by group\n> t = -4, df = 9, p-value = 0.003\n> alternative hypothesis: true mean difference is not equal to 0\n> 95 percent confidence interval:\n>  -2.46 -0.70\n> sample estimates:\n> mean difference \n>           -1.58"},{"path":"solutions.html","id":"question-8-2","chapter":"Solutions","heading":"Question 8","text":"Réaliser une simulation. Calculer la probabilité de gagner au jeu du roche-papier-ciseau pour chacune des options.Plusieurs simulations sont possibles. Voici un exemple.","code":"# Énumérer les choix\nchoix <- c(\"roche\", \"papier\", \"ciseau\")\n\n# Créer la matrice de résultats possibles\nResultats <- matrix(c(.5, 0, 1,\n                      1, .5, 0,\n                      0, 1, .5), \n                    ncol = 3, nrow = 3,\n                    byrow = TRUE,\n                    dimnames = list(choix, choix))\n\n# Initialiser les gains pour chaque choix\ngain <- c(roche = 0, papier = 0, ciseau = 0)\n\n# Paramètres de la u\nset.seed(73)\nnreps <- 10000\n\n# La boucle\n\nfor(i in 1:nreps){\n  # Les joueurs des joueurs\n  joueur1 <- sample(choix, size = 1)\n  joueur2 <- sample(choix, size = 1)\n  \n  # Enregistrer les gains\n  gain[joueur1] <- gain[joueur1] + Resultats[joueur1, joueur2]\n  gain[joueur2] <- gain[joueur2] + Resultats[joueur2, joueur1]\n}\n# Probabilités (en pourcentage)\ngain / nreps\n>  roche papier ciseau \n>  0.331  0.330  0.340"},{"path":"solutions.html","id":"question-9-2","chapter":"Solutions","heading":"Question 9","text":"Réaliser une simulation. Trouver la valeur critique (c.vrit) pour un \\(\\alpha = .025\\) unilatérale d’une distribution normale centrée sur \\(0\\) et un écart type de \\(1/\\sqrt{n}\\). Le scénario : tirer aléatoirement un échantillon de \\(n=30\\) participants à partir d’une distribution normale de moyenne \\(.5\\) et un écart type de 1. Calculer la moyenne de cet échantillon. Pour chaque scénario, additionner chaque occasion où cette moyenne est plus élevée que la valeur critique. Répéter ce scénario 1000 fois. Calculer la probabilité (en pourcentage) d’occurrence selon laquelle la moyenne de l’échantillon est plus élevée que la valeur critique.Avec les renseignements précédents, calculer la puissance.","code":"# Fixer les paramètres\nalpha <- .025\nn <- 30\nmu1 <- .5\nv.crit <- qnorm(1 - alpha, sd = 1/sqrt(n)) # ou qnorm(alpha, lower.tail = FALSE)\n\n# Initialiser\nset.seed(101)\nnreps <- 1000\nsomme <- 0\n\n# La boucle\nfor(i in 1:nreps){\n  # Créer un échantillon\n  echant <- rnorm(n = n, mean = mu1)\n  \n  #Calculer la moyenne\n  moyenne <- mean(echant)\n  # Enregister si la moyenne est plus élevée que la valeur critique\n  somme <- somme + (moyenne >= v.crit)\n}\n\n# Probabilités (en pourcentage)\nsomme/nreps\n> [1] 0.792pnorm(v.crit, mean = mu1, sd = 1/sqrt(n), lower.tail = FALSE)\n> [1] 0.782"},{"path":"solutions.html","id":"question-10-2","chapter":"Solutions","heading":"Question 10","text":"Réaliser un bootstrap la corrélation entre sleep_total (temps de sommeil total) et bodywt (poids du corps) dans le jeu de données msleep (du package ggplot2). Produire la moyenne et l’écart type des échantillons bootstrapées ainsi que l’intervalle de confiance à 95%.","code":"# Initialiser \nset.seed(2018)\nnreps <- 1000\nN <- nrow(msleep)\nalpha <-  1-.95\ncorrel <- numeric()\n\n# La boucle\nfor(i in 1:nreps){\n  idx <- sample(N, replace = TRUE)\n  D <- msleep[idx, ]\n  correl[i] <- cor(D$sleep_total, D$bodywt)\n}\n\n# Les statistiques\nmean(correl)\n> [1] -0.364\nsd(correl)\n> [1] 0.0694\nquantile(correl, probs = c(alpha/2, 1-alpha/2))\n>   2.5%  97.5% \n> -0.528 -0.255"},{"path":"solutions.html","id":"modèles-linéaires","chapter":"Solutions","heading":"Modèles linéaires","text":"","code":""},{"path":"solutions.html","id":"question-1-5","chapter":"Solutions","heading":"Question 1","text":"Avec le jeu de données ToothGrowth, réaliser l’anova de len par rapport à l’interaction entre supp et dose. Consulter le sommaire.","code":"res.aov <- aov(len ~ supp * dose, data = ToothGrowth)\nsummary(res.aov)\n>             Df Sum Sq Mean Sq F value  Pr(>F)    \n> supp         1    205     205   12.32 0.00089 ***\n> dose         1   2224    2224  133.42 < 2e-16 ***\n> supp:dose    1     89      89    5.33 0.02463 *  \n> Residuals   56    934      17                    \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"solutions.html","id":"question-2-5","chapter":"Solutions","heading":"Question 2","text":"Avec le jeu de données ToothGrowth, réaliser le régression de len par rapport à l’interaction entre supp et dose. Consulter le sommaire.","code":"res.lm <- lm(len ~ supp * dose, data = ToothGrowth)\nsummary(res.lm)\n> \n> Call:\n> lm(formula = len ~ supp * dose, data = ToothGrowth)\n> \n> Residuals:\n>    Min     1Q Median     3Q    Max \n>  -8.23  -2.85   0.05   2.29   7.94 \n> \n> Coefficients:\n>             Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)    11.55       1.58    7.30  1.1e-09 ***\n> suppVC         -8.25       2.24   -3.69  0.00051 ***\n> dose            7.81       1.20    6.53  2.0e-08 ***\n> suppVC:dose     3.90       1.69    2.31  0.02463 *  \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 4.08 on 56 degrees of freedom\n> Multiple R-squared:  0.73,    Adjusted R-squared:  0.715 \n> F-statistic: 50.4 on 3 and 56 DF,  p-value: 6.52e-16"},{"path":"solutions.html","id":"question-3-5","chapter":"Solutions","heading":"Question 3","text":"Créer un jeu de données pour la structure de la Figure 17.7. Le jeu de données est standardisé et contient 123 sujets.","code":"n <- 123\nset.seed(n)\nx <- rnorm(n)\ny <- .5 * x + sqrt(1-.5^2) * rnorm(n)\nz <- .5 * y + sqrt(1-.5^2) * rnorm(n)\njd <- data.frame(x = x, y = y, z = z)\nhead(jd)\n>         x       y      z\n> 1 -0.5605 -0.5020 -0.338\n> 2 -0.2302  1.4817 -0.437\n> 3  1.5587  0.2147 -0.468\n> 4  0.0705  0.2391  0.540\n> 5  0.1293  0.1322 -0.259\n> 6  1.7151  0.0245 -0.474"},{"path":"solutions.html","id":"question-4-5","chapter":"Solutions","heading":"Question 4","text":"Créer un jeu de données pour la structure de la Figure 17.8. Le jeu de données est standardisé et contient 456 sujets.","code":"n <- 456\nset.seed(n)\nq <- rnorm(n)\nw <- .7 * q + sqrt(1-(.7)^2) * rnorm(n)\nr <- -.7 * w + sqrt(1-(-.7)^2) * rnorm(n)\ne <- -.8 * q + sqrt(1-(-.8)^2) * rnorm(n)\ny <- .8 * e + sqrt(1-(.8)^2) * rnorm(n)\njd <- data.frame(q = q, w = w, e = e, r = r,y = y)\nhead(jd)\n>        q      w      e       r       y\n> 1 -1.344 -0.591  0.849  0.6451  0.9648\n> 2  0.622  0.344 -0.404 -0.1897 -0.0720\n> 3  0.801  0.949  0.147 -0.9513 -1.1297\n> 4 -1.389 -0.913  2.880  1.2526  1.8879\n> 5 -0.714  0.121  0.255  0.0432 -0.3850\n> 6 -0.324  1.922  0.518 -1.7115  0.0752"},{"path":"solutions.html","id":"question-5-5","chapter":"Solutions","heading":"Question 5","text":"Créer un jeu de données pour la structure de la Figure 17.9. Le jeu de données est standardisé et contient 789 sujets.Pour faciliter la résolution de cette quesiton, il est beaucoup plus simplede passer par le [Cas général] de génération de données et d’utiliser la fonction maison beta2cov qui s’importe en collant la syntaxe directement dans l’environnement R ou qui est disponible dans le package pathanalysis vu dans la section Rapporter l’analyse de médiation.","code":"library(pathanalysis)\nn <- 789\nset.seed(n)\n\n# Créer de la matrice de coefficients de régression\nB <- matrix(c(0,  0,  0, 0,\n             .3,  0,  0, 0,\n            -.3,  0,  0, 0,\n             .1, .4,-.4, 0),\n            ncol = 4, nrow = 4,\n            byrow = TRUE,\n            dimnames = list(c(\"x\", \"y\", \"w\", \"z\"),\n                            c(\"x\", \"y\", \"w\", \"z\")))\n# L'argument `dimnames` ne sert qu'à nommer les dimensions\n\n# Produire la matrice de covariance à partir de la  matrice Beta\nS <- beta2cov(B)\n\n# Générer les données\njd <- MASS::mvrnorm(n = n,\n                    mu = rep(0, ncol(S)),\n                    Sigma = S)\nhead(jd)\n>           x      y       w       z\n> [1,]  0.869  0.728  0.0987  0.0627\n> [2,] -3.291 -0.738  0.7189 -1.6190\n> [3,]  0.790 -1.245 -0.7083 -0.2671\n> [4,] -0.347  1.254  0.0845 -0.1919\n> [5,]  0.280 -0.521  0.7192 -0.1251\n> [6,] -0.819 -0.867  0.1130  0.2644"},{"path":"solutions.html","id":"question-6-5","chapter":"Solutions","heading":"Question 6","text":"Avec le jeu de données de la Question 5, réaliser l’analyse de médiation.Il existe plusieurs façona de faire l’analyse de l’analyse de médiation. Voici celle avec la fonction mediation() du package pathanalysis.","code":"# Vérifier que les variables ont bien des nom de colonnes\n# Pour s'en assurer\n# colnames(jd) <- c(\"x\", \"y\", \"w\", \"z\")\n\n# La fonction `mediation()` du package `pathanalysis`\nmediation(z ~ w ~ y ~ x, data = jd, standardized = TRUE)\n>                       Estimate  S.E. CI Lower 95 %\n> x -> y                   0.312 0.036         0.243\n> x -> w                  -0.325 0.037        -0.399\n> x -> z                   0.120 0.030         0.060\n> y -> w                   0.018 0.035        -0.049\n> y -> z                   0.402 0.028         0.349\n> w -> z                  -0.368 0.028        -0.420\n> x -> y -> w              0.006 0.011        -0.016\n> x -> y -> z              0.125 0.017         0.095\n> x -> w -> z              0.120 0.016         0.089\n> y -> w -> z             -0.007 0.013        -0.032\n> x -> y -> w -> z        -0.002 0.004        -0.010\n> total indirect x -> z    0.243 0.023         0.200\n> total effect x -> z      0.363 0.034         0.294\n>                       CI Upper 95 % p-value\n> x -> y                        0.383   0.000\n> x -> w                       -0.252   0.000\n> x -> z                        0.179   0.000\n> y -> w                        0.088   0.603\n> y -> z                        0.457   0.000\n> w -> z                       -0.313   0.000\n> x -> y -> w                   0.027   0.608\n> x -> y -> z                   0.160   0.000\n> x -> w -> z                   0.153   0.000\n> y -> w -> z                   0.019   0.603\n> x -> y -> w -> z              0.006   0.607\n> total indirect x -> z         0.289   0.000\n> total effect x -> z           0.430   0.000"},{"path":"solutions.html","id":"question-7-4","chapter":"Solutions","heading":"Question 7","text":"Avec le jeu de données ToothGrowth, analyser l’interaction entre supp et dose sur la variable dépendante len. Produire les graphiquesLe graphique des pentes simples.\nFigure 20.20: Le graphique des pentes simples (question 7)\nLe graphique de Johnson-Neyman.","code":"res.lm <- lm(len ~ supp * dose, data = ToothGrowth)\nsummary(res.lm)\n> \n> Call:\n> lm(formula = len ~ supp * dose, data = ToothGrowth)\n> \n> Residuals:\n>    Min     1Q Median     3Q    Max \n>  -8.23  -2.85   0.05   2.29   7.94 \n> \n> Coefficients:\n>             Estimate Std. Error t value Pr(>|t|)    \n> (Intercept)    11.55       1.58    7.30  1.1e-09 ***\n> suppVC         -8.25       2.24   -3.69  0.00051 ***\n> dose            7.81       1.20    6.53  2.0e-08 ***\n> suppVC:dose     3.90       1.69    2.31  0.02463 *  \n> ---\n> Signif. codes:  \n> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n> \n> Residual standard error: 4.08 on 56 degrees of freedom\n> Multiple R-squared:  0.73,    Adjusted R-squared:  0.715 \n> F-statistic: 50.4 on 3 and 56 DF,  p-value: 6.52e-16\nlibrary(interactions)\ninteract_plot(model = res.lm,\n              pred = dose,\n              modx = supp)sim_slopes(model = res.lm,\n           pred = dose,\n           modx = supp,\n           jnplot = TRUE)\n> Warning: Johnson-Neyman intervals are not available for factor\n> moderators.\n> SIMPLE SLOPES ANALYSIS \n> \n> Slope of dose when supp = VC: \n> \n>    Est.   S.E.   t val.      p\n> ------- ------ -------- ------\n>   11.72   1.20     9.80   0.00\n> \n> Slope of dose when supp = OJ: \n> \n>   Est.   S.E.   t val.      p\n> ------ ------ -------- ------\n>   7.81   1.20     6.53   0.00"},{"path":"solutions.html","id":"analyses-factorielles","chapter":"Solutions","heading":"Analyses factorielles","text":"","code":""},{"path":"solutions.html","id":"question-1-6","chapter":"Solutions","heading":"Question 1","text":"Créer un jeu de données pour la structure de la Figure 20.9. Le jeu de données est standardisé et contient 584 sujets.","code":"# Programmer la structure factorielle\nFS <- matrix(c(.5, .5, .3, .2,  0,  0,  0,  0,  0,\n                0,  0,  0, .7, .7, .7,  0,  0,  0,\n                0,  0,  0,  0,  0, .2, .3, .4, .3),\n             ncol = 3, nrow = 9,\n             dimnames = list(paste0(\"i\",1:9),\n                             paste0(\"F\",1:3)))\n# L'argument `dimnames` ne sert qu'à nommer les dimensions\n\n# Transformer ne matrice de corrélation\nR <- FS %*% t(FS)\n\n# Ajouter le bruit dans la diagonale\ndiag(R) <- 1\n\n# Générer le jeu de données\nset.seed(584)\njd <- MASS::mvrnorm(n = 584,\n                    mu = rep(0, ncol(R)),\n                    Sigma = R)\n\n# Voici les premières données.\nhead(jd)\n>         i1     i2     i3     i4     i5     i6     i7     i8\n> [1,] 0.489  0.666  0.936 -0.263 -1.416 -0.565 -0.327  0.573\n> [2,] 2.251  1.601  0.563  0.636  0.443  0.998 -0.409 -1.050\n> [3,] 0.917  2.349  0.644  0.232  1.472  0.506  1.550 -0.374\n> [4,] 0.054 -0.583  0.544  0.518  1.418  0.849 -1.497  0.249\n> [5,] 0.428 -1.409 -0.319  1.803  0.800  0.584  1.459 -1.496\n> [6,] 0.122  0.459 -0.524 -0.665 -0.675 -0.393  0.395 -0.121\n>           i9\n> [1,] -0.0283\n> [2,] -0.0520\n> [3,] -0.5925\n> [4,] -0.4915\n> [5,] -0.3259\n> [6,]  0.1591"},{"path":"solutions.html","id":"question-2-6","chapter":"Solutions","heading":"Question 2","text":"Utiliser la fonction eigen() pour extraire les valeurs propres, la variance expliquée de chacune d’elle et les loadings du jeu de données de la Question 1.","code":"# L'analyse en composantes principales\nres.acp <- eigen(cor(jd))\n\n# Les valeurs propres\nres.acp$values\n> [1] 1.986 1.367 1.250 0.984 0.897 0.785 0.761 0.503 0.468\n\n# Leur variance expliquée\nres.acp$values / ncol(jd)\n> [1] 0.2206 0.1518 0.1389 0.1094 0.0996 0.0872 0.0846 0.0559\n> [9] 0.0520\n\n# Les loadings (pour toutes les dimensions)\nres.acp$vectors %*% diag(sqrt(res.acp$values))\n>          [,1]    [,2]    [,3]     [,4]     [,5]    [,6]\n>  [1,] 0.15072 -0.6776  0.1101 -0.10901 -0.16094  0.6295\n>  [2,] 0.13596 -0.5962  0.2087  0.47882  0.09149 -0.0687\n>  [3,] 0.13756 -0.6536  0.0685 -0.37469 -0.00409 -0.5843\n>  [4,] 0.83004  0.0229  0.0341  0.04461 -0.02579 -0.0199\n>  [5,] 0.79778  0.1475  0.1708  0.00734  0.01265 -0.0669\n>  [6,] 0.77087  0.2000 -0.1981 -0.05133 -0.02211  0.0780\n>  [7,] 0.03508 -0.1477 -0.5661  0.67715  0.08179 -0.1098\n>  [8,] 0.00983 -0.1087 -0.6674 -0.16089 -0.64746 -0.0442\n>  [9,] 0.06704 -0.1702 -0.5951 -0.33729  0.65972  0.1328\n>          [,7]     [,8]     [,9]\n>  [1,]  0.2625  0.00133  0.05561\n>  [2,] -0.5825 -0.02710  0.01007\n>  [3,]  0.2361 -0.10055  0.03887\n>  [4,]  0.0722  0.12182 -0.53501\n>  [5,]  0.0214  0.39869  0.38539\n>  [6,] -0.0907 -0.53358  0.15615\n>  [7,]  0.4191  0.00327  0.05981\n>  [8,] -0.2760  0.13924  0.00353\n>  [9,] -0.1772  0.11894 -0.01461"},{"path":"solutions.html","id":"question-3-6","chapter":"Solutions","heading":"Question 3","text":"Créer une fonction maison pour le test de Kaiser et utiliser le jeu de données créé à la Question 1.","code":"kaiser.maison <- function(eig, crit = 1){\n  return(sum(eig > crit))\n}\nkaiser.maison(eig = res.acp$values)\n> [1] 3"},{"path":"solutions.html","id":"question-4-6","chapter":"Solutions","heading":"Question 4","text":"Créer une fonction maison pour l’analyse parallèle et utiliser le jeu de données créé à la Question 1.","code":"parallel.maison <- function(eig, ns, nreps = 1000, percentile = .5){\n  # Répliquer (replicate) l'ACP (eigen) d'une matrice \n  # de corrélation (cor) de données aléatoires (rnorm)\n  S.eig <- replicate(n = nreps, \n                     eigen(cor(matrix(rnorm(length(eig)*ns), \n                                      ncol = length(eig))))$values)\n  crit <- apply(X = S.eig,\n                FUN = quantile,\n                MARGIN = 1, \n                probs = percentile)\n  return(min(which(eig < crit)) - 1)\n}\nparallel.maison(res.acp$values, ns = 584)\n> [1] 3"},{"path":"solutions.html","id":"question-5-6","chapter":"Solutions","heading":"Question 5","text":"Utiliser Rnest avec le jeu de données créé à la Question 1. Produire une graphique.","code":"library(Rnest)\nFALSE \nFALSE Attachement du package : 'Rnest'\nFALSE L'objet suivant est masqué depuis 'package:tidyr':\nFALSE \nFALSE     nest\nFALSE L'objet suivant est masqué depuis 'package:stats':\nFALSE \nFALSE     loadings\nres <- nest(jd)\nres\nFALSE At 95% confidence, Nest Eigenvalue Sufficiency Test (NEST) suggests 3 factors.\nplot(res)"},{"path":"solutions.html","id":"question-6-6","chapter":"Solutions","heading":"Question 6","text":"Utiliser factanal() pour 3 facteurs avec le jeu de données créé à la Question 1. Extraire les scores et les loadings.","code":"# L'analyse factorielle\nres.fa <- factanal(jd,\n         factors = 3,\n         scores = \"regression\")\nres.fa\n> \n> Call:\n> factanal(x = jd, factors = 3, scores = \"regression\")\n> \n> Uniquenesses:\n>    i1    i2    i3    i4    i5    i6    i7    i8    i9 \n> 0.770 0.857 0.812 0.471 0.454 0.463 0.938 0.837 0.892 \n> \n> Loadings:\n>    Factor1 Factor2 Factor3\n> i1          0.474         \n> i2          0.375         \n> i3          0.430         \n> i4  0.711   0.153         \n> i5  0.716          -0.173 \n> i6  0.691           0.229 \n> i7                  0.247 \n> i8                  0.403 \n> i9                  0.326 \n> \n>                Factor1 Factor2 Factor3\n> SS loadings      1.498   0.586   0.421\n> Proportion Var   0.166   0.065   0.047\n> Cumulative Var   0.166   0.232   0.278\n> \n> Test of the hypothesis that 3 factors are sufficient.\n> The chi square statistic is 14.1 on 12 degrees of freedom.\n> The p-value is 0.292\n\n# Extraire les scores\nres.fa$scores\n>         Factor1  Factor2   Factor3\n>   [1,] -0.83366  0.58358  0.342690\n>   [2,]  0.76889  1.24111 -0.160501\n>   [3,]  0.85308  1.05470 -0.275355\n>   [4,]  1.08709 -0.07155 -0.411387\n>   [5,]  1.21651 -0.14465 -0.325073\n>   [6,] -0.59656 -0.08773  0.072458\n>   [7,] -0.54468 -0.61439 -1.637814\n>   [8,] -0.05967  0.26519  0.023142\n>   [9,]  0.66283 -0.60101  0.959725\n>  [10,] -0.10487  0.13781 -1.332906\n>  [11,] -0.05522 -0.39171  0.514765\n>  [12,]  1.82187  0.67113  0.341810\n>  [13,]  0.01411  0.73428 -1.156857\n>  [14,]  0.21964  0.98978  0.380414\n>  [15,]  0.32852  0.63906  0.356436\n>  [16,]  0.35250  0.40031 -0.515344\n>  [17,] -0.09693  0.08499 -0.340046\n>  [18,]  0.21520 -0.60417  0.230251\n>  [19,] -1.29674 -0.77926 -0.055553\n>  [20,]  2.50988  0.20077  0.995164\n>  [21,] -0.17189  1.28995 -0.195673\n>  [22,]  1.08950 -0.37615  0.207520\n>  [23,]  0.66918  0.86039  0.271027\n>  [24,] -1.53354  0.09630  0.245623\n>  [25,]  0.10844  0.28343  0.704891\n>  [26,]  0.93994  0.21780  0.572182\n>  [27,]  1.49451 -1.00048 -0.057803\n>  [28,]  0.32748  2.11153 -0.003385\n>  [29,]  0.16111 -1.35284  0.278450\n>  [30,]  0.51204 -0.50717  0.428596\n>  [31,] -0.14014  0.14923 -1.258340\n>  [32,] -0.95141 -0.25945  0.571332\n>  [33,]  0.42996  0.00156 -0.182604\n>  [34,]  0.08602 -0.80171 -0.805013\n>  [35,]  0.69506 -0.31801  0.083405\n>  [36,] -0.42497 -0.37251  0.090079\n>  [37,]  1.29473  1.30164  0.825328\n>  [38,]  1.05747  1.19074 -0.984881\n>  [39,]  0.23639 -0.29245  0.661033\n>  [40,]  0.35086 -0.57473 -0.779951\n>  [41,]  0.33842 -0.97260  0.422997\n>  [42,]  0.55561 -0.27252 -0.701154\n>  [43,] -0.23122  0.09536 -0.684730\n>  [44,] -0.56670  0.15954  0.419495\n>  [45,]  1.99893  0.50315 -0.346903\n>  [46,] -0.81484  1.15375  0.047917\n>  [47,] -0.41105 -0.83073 -0.435116\n>  [48,] -1.03249 -1.29206  0.173158\n>  [49,]  0.95858 -0.69215 -0.343312\n>  [50,]  0.32455  0.46564 -0.601205\n>  [51,]  0.08558 -0.38999 -0.844208\n>  [52,]  0.99142  0.00887  1.270202\n>  [53,]  1.32885  1.07354  0.556514\n>  [54,]  0.03630  0.96568 -0.284066\n>  [55,] -1.44638  0.35967 -0.863391\n>  [56,] -0.23090 -0.14944  1.379261\n>  [57,] -0.46146  1.14282  1.495109\n>  [58,]  0.53671 -0.92566  0.451991\n>  [59,]  0.53974  1.29301  0.114702\n>  [60,]  0.00922  0.15783 -0.107781\n>  [61,]  0.34033  0.60310  0.425249\n>  [62,] -2.13393  0.26909  0.230915\n>  [63,] -0.29221 -0.69697 -0.277423\n>  [64,]  0.03429  0.03258 -0.857771\n>  [65,]  0.99198  0.19662 -0.723475\n>  [66,]  1.05837  0.28110  0.699938\n>  [67,]  1.47163  0.27783 -0.100761\n>  [68,]  0.29715  0.38838 -0.491096\n>  [69,] -0.74657  0.12301 -0.445042\n>  [70,]  0.60748  0.72057  0.588835\n>  [71,] -2.35699  0.22659  0.122496\n>  [72,] -1.45939  0.53260  0.196858\n>  [73,]  0.04597 -0.12751 -0.309984\n>  [74,]  0.47292  0.38336 -0.667654\n>  [75,]  0.35212 -0.77788 -0.274501\n>  [76,] -0.50230  0.45790 -0.413664\n>  [77,]  0.24104 -0.57589  0.413615\n>  [78,]  0.53554  0.24366  0.018391\n>  [79,]  0.09826 -0.85608  0.478283\n>  [80,] -0.43518  0.26106 -0.368034\n>  [81,]  0.29795 -0.30960  0.992972\n>  [82,]  0.89006 -0.44839 -0.081774\n>  [83,]  0.78428  0.98966  0.175311\n>  [84,] -0.31630  0.47048 -1.100788\n>  [85,]  0.62419  0.54838 -0.187761\n>  [86,] -0.33409 -0.65235 -1.271664\n>  [87,]  0.12075 -0.90995 -0.353222\n>  [88,]  0.67489  0.21985  0.405949\n>  [89,]  0.02559  0.49581  0.839946\n>  [90,] -0.93627 -0.06853  0.403047\n>  [91,] -0.46991 -0.22387  0.031650\n>  [92,] -0.39500  0.38507 -0.786081\n>  [93,] -1.53854 -0.62229  1.246333\n>  [94,]  0.58009  0.64814  0.882283\n>  [95,] -1.73038  0.79093  0.066325\n>  [96,] -0.18242 -0.00889  0.263813\n>  [97,] -0.46408  0.52134  0.200910\n>  [98,]  0.08985 -0.35297  0.565880\n>  [99,]  0.55612 -0.91685  0.000405\n> [100,]  0.30679  0.46088 -0.474611\n> [101,] -0.31214  0.83368  0.050967\n> [102,]  1.00147 -1.10772 -1.090463\n> [103,] -0.21593  0.06259  0.817170\n> [104,] -0.03331  0.08437 -0.233618\n> [105,]  0.08788 -0.07796  0.705683\n> [106,] -1.13227  0.64975  0.136063\n> [107,] -0.54390 -1.03458 -0.661280\n> [108,]  0.49542 -0.03883 -0.585572\n> [109,]  0.83970  0.35965 -1.010159\n> [110,]  0.46492  1.10603 -0.685535\n> [111,] -1.18779 -0.85958 -1.308114\n> [112,]  0.28693  0.67686 -0.610832\n> [113,] -0.91179  0.06728  0.189939\n> [114,]  0.27856 -0.61626  0.214894\n> [115,] -0.25920  0.59118 -0.198770\n> [116,]  0.05791 -1.21397  0.641136\n> [117,]  0.64682  0.35335  0.217462\n> [118,]  1.27781  0.38729  0.302326\n> [119,] -1.00092  0.46925  0.238805\n> [120,] -1.53262 -0.22758 -1.394575\n> [121,] -0.49455  0.17226  0.930756\n> [122,] -0.84313 -0.37568  0.058610\n> [123,] -0.11659  0.21998  0.810858\n> [124,] -0.87496 -0.83672  0.422494\n> [125,]  1.33830 -0.26177 -0.952818\n> [126,]  1.05112  1.39875 -0.042975\n> [127,]  0.55216 -0.61576 -0.959522\n> [128,]  0.20487 -1.04833 -0.037500\n> [129,] -0.57603 -0.80214 -0.032636\n> [130,] -1.24724  0.22529 -0.155338\n> [131,]  0.67515  1.09492 -0.815557\n> [132,]  2.01186 -0.42069  0.078669\n> [133,] -0.31607  0.50831  0.057357\n> [134,] -0.47495  0.16986 -0.210380\n> [135,] -0.04391 -0.48351 -0.445600\n> [136,]  0.49134 -0.81502  0.254078\n> [137,]  0.01861 -0.58322 -0.301274\n> [138,]  0.14252  0.97910 -0.655383\n> [139,]  0.24322  1.03970 -0.264538\n> [140,] -0.09103  0.09800  0.606684\n> [141,]  2.03356  0.32586 -0.002490\n> [142,]  0.28390  0.45164 -0.850906\n> [143,] -0.61878  0.07582  0.758275\n> [144,]  0.81753 -0.72803  0.067334\n> [145,]  1.06914 -0.97552 -1.052529\n> [146,] -0.99072 -0.84321  1.544579\n> [147,] -0.64774 -0.15631  0.044261\n> [148,]  1.41346  0.28984  0.512343\n> [149,]  0.25114  0.70591  0.759327\n> [150,]  0.14466 -0.21989  0.280057\n> [151,] -1.13382 -0.10123  0.451275\n> [152,]  0.62669 -0.18528  0.820711\n> [153,]  0.33358 -0.47331  0.375185\n> [154,]  1.10418  0.40144  0.464436\n> [155,] -0.01503 -0.32293  0.480190\n> [156,]  0.13541  0.72843 -0.614199\n> [157,] -0.30304  1.21617 -0.881363\n> [158,] -1.14589 -0.60295 -0.013967\n> [159,] -0.57276  0.22088  0.189424\n> [160,] -0.85703  0.47614 -0.578018\n> [161,]  0.30496 -0.50849  0.354217\n> [162,] -0.94122  0.23123 -0.474627\n> [163,]  0.49690 -1.07692 -0.112242\n> [164,] -0.44492  0.77375 -1.005674\n> [165,] -0.03189 -0.11696  0.637828\n> [166,] -0.40257  0.13072 -0.138532\n> [167,] -2.29408 -0.23693 -0.049400\n> [168,]  0.81532  0.85180  0.097546\n> [169,]  0.87669 -0.63879 -0.164307\n> [170,] -0.11192 -0.42086  0.589782\n> [171,]  1.82450 -0.52647 -0.879305\n> [172,]  1.15915  0.07141  0.395764\n> [173,]  0.73654  0.82785 -0.321302\n> [174,]  1.12141 -0.26747 -0.170157\n> [175,]  0.39953  1.08186 -0.022477\n> [176,] -0.48312 -0.20080 -0.720880\n> [177,] -0.32087 -0.20092 -1.157097\n> [178,] -1.29264 -0.41486  0.254806\n> [179,] -0.28013 -0.59272 -1.181823\n> [180,] -0.59174  0.19740 -0.336245\n> [181,]  2.47811 -0.02575 -0.107795\n> [182,] -0.36513 -0.05887 -0.373211\n> [183,]  0.50823 -0.91799  0.434710\n> [184,] -0.25993  0.08851  0.807036\n> [185,]  0.03759  0.29855  0.113889\n> [186,] -0.70216  1.05667 -0.886259\n> [187,]  0.57100 -0.04014  0.110798\n> [188,] -0.22546  0.94019  0.735268\n> [189,]  0.10265 -0.12756 -0.029189\n> [190,]  0.10629  0.56707 -0.357094\n> [191,] -2.01336 -0.50733 -0.163194\n> [192,]  1.45509  0.51868  0.442138\n> [193,]  0.64089  0.96288  0.383998\n> [194,]  0.14558  0.56946 -0.029618\n> [195,] -0.23784 -1.28682  0.049096\n> [196,] -1.08102  0.73301 -0.816188\n> [197,]  0.21298  0.14085  0.627924\n> [198,] -0.47199 -0.00102  0.326666\n> [199,] -0.26779 -1.21933  1.129719\n> [200,] -0.41982 -0.09976  0.009351\n> [201,] -2.15012  0.22385 -0.479056\n> [202,]  0.55725 -0.20260  0.001828\n> [203,] -0.87193 -0.20890  0.549921\n> [204,]  0.16389  0.74865 -0.579059\n> [205,] -0.14271  1.81682 -0.037279\n> [206,] -0.10104  1.26200 -0.272604\n> [207,] -1.26246 -0.98870 -0.664690\n> [208,] -0.83550 -0.15402  0.050639\n> [209,]  0.10350 -0.78933 -0.286576\n> [210,] -0.08393 -1.23916 -0.292363\n> [211,] -0.11262  0.18532  1.177732\n> [212,]  0.87216 -0.12151  0.849816\n> [213,] -0.21997 -1.11834 -0.586156\n> [214,]  0.04472 -0.05484  0.931295\n> [215,]  0.19396  0.62020  0.578771\n> [216,]  0.21466  0.24257 -0.450137\n> [217,]  0.32608  0.28378  0.147317\n> [218,]  0.27308  0.97399  0.702685\n> [219,] -0.07387  0.35214  0.688794\n> [220,]  0.54810 -0.61060 -0.004821\n> [221,] -0.36846 -0.37162  0.652137\n> [222,]  0.95125  0.11810 -0.265583\n> [223,]  1.12915  0.39455 -0.115776\n> [224,] -0.00856  0.37348 -0.881770\n> [225,] -1.77611 -0.78168 -0.498161\n> [226,]  0.47873 -0.37026 -0.035027\n> [227,]  0.13516  0.33211 -0.838501\n> [228,] -0.25566  0.39385 -0.746493\n> [229,]  0.47185 -0.70742  0.959659\n> [230,] -1.42803  0.46587 -0.081241\n> [231,]  1.00035  0.65148 -0.170668\n> [232,] -0.40474  0.38414 -0.901872\n> [233,]  0.19063  0.51127  0.330418\n> [234,] -0.27645 -0.85877  0.874765\n> [235,] -0.53454 -0.14826  0.142216\n> [236,] -0.39758  0.19231 -0.081596\n> [237,]  1.19157  0.69093  1.004801\n> [238,]  1.73616  0.29029  0.619564\n> [239,] -0.21639  0.54620  0.202189\n> [240,] -0.33191 -1.25322  0.109755\n> [241,]  0.15401  1.40200  0.421219\n> [242,] -0.79634 -0.42680 -0.191401\n> [243,]  0.77839 -0.13347 -0.184682\n> [244,]  0.70725 -0.56492 -0.582056\n> [245,] -0.93920 -0.62830  0.282492\n> [246,] -0.01252 -1.05019 -0.573996\n> [247,] -0.44232 -0.62773 -0.047710\n> [248,]  0.22532 -0.12492  0.592077\n> [249,] -1.21788  0.74633 -0.236966\n> [250,]  0.71104 -0.48661  0.185759\n> [251,] -0.82800 -0.02291 -1.198462\n> [252,] -1.05657 -0.33134  0.336086\n> [253,] -0.39066  0.83283 -0.732931\n> [254,]  0.17967  0.30150 -0.051138\n> [255,]  0.42566 -0.42791 -0.792029\n> [256,]  1.47302 -0.83453 -0.320927\n> [257,]  0.42161  1.55125 -0.823539\n> [258,] -0.17556 -0.10135  0.323337\n> [259,] -0.91810  0.50169  1.564949\n> [260,]  0.07449  0.83637 -0.102961\n> [261,] -0.27475  0.70066  0.647827\n> [262,] -1.06450 -0.16954 -0.087521\n> [263,]  1.87548 -0.20394 -1.204906\n> [264,] -1.16747  0.12721 -0.431845\n> [265,] -0.86187  0.57930 -0.024474\n> [266,]  0.47824 -1.01323 -0.046878\n> [267,]  0.56925 -0.15182  0.850935\n> [268,] -0.96625 -0.49698  0.621760\n> [269,] -0.23909 -0.47457 -0.225663\n> [270,]  0.45427 -0.07726  0.347640\n> [271,] -0.12909 -0.38269  0.950243\n> [272,] -2.31486 -0.91144 -0.498224\n> [273,]  0.95038 -0.05397 -0.047770\n> [274,] -0.32135 -0.46166 -0.236173\n> [275,] -0.10237  0.35625  0.321577\n> [276,] -0.62043 -0.27629 -0.375142\n> [277,] -1.54561 -0.35520 -1.141906\n> [278,] -0.37093 -0.55419 -0.102723\n> [279,]  1.28305  0.66901 -0.429401\n> [280,]  1.24162  0.15291 -0.409985\n> [281,] -0.11610  0.93966 -0.928237\n> [282,] -0.51709 -0.08019 -0.672568\n> [283,] -1.27537  1.15566  0.323371\n> [284,] -0.61706  0.01361 -0.349877\n> [285,]  0.52371  0.56001 -0.311415\n> [286,]  0.32707 -0.66088 -0.895286\n> [287,]  0.56562  0.39212  0.440564\n> [288,] -1.70364 -0.77350  0.575336\n> [289,] -0.50237 -1.20729  0.471056\n> [290,]  1.41713  0.95548  0.750766\n> [291,]  1.54177  1.43574 -0.617593\n> [292,]  0.05538 -0.25872 -0.370460\n> [293,]  0.89863 -0.77283  0.145793\n> [294,] -1.10537 -1.24092  0.767918\n> [295,]  0.50774  0.04623  0.106541\n> [296,]  0.29393  0.15282 -0.311169\n> [297,] -0.29032  0.27915 -0.135805\n> [298,] -0.37559 -0.16338  1.002318\n> [299,]  0.38439  0.97709  0.073120\n> [300,] -0.79387 -0.77624 -0.264213\n> [301,] -0.53111 -0.80262 -0.196966\n> [302,] -0.77401 -0.13737  0.697164\n> [303,]  2.29525 -0.04093 -1.016569\n> [304,]  0.84927 -0.46442  0.360956\n> [305,]  0.68281  0.25092  0.381776\n> [306,]  1.86410  1.20398 -0.129118\n> [307,]  0.60200 -0.81176 -0.275922\n> [308,] -0.13185 -0.71211  0.445464\n> [309,] -0.46913  0.30024  0.030282\n> [310,] -2.00381 -0.24868 -1.022072\n> [311,]  1.57091 -0.27040  0.201151\n> [312,] -0.28238  0.31986  0.328192\n> [313,]  0.42629 -0.75019 -0.056034\n> [314,] -0.68479 -0.05159  0.200631\n> [315,] -0.90255 -1.17194 -0.221483\n> [316,]  0.04894 -0.21845  0.699941\n> [317,] -0.34604 -0.78084 -0.498391\n> [318,]  0.51178 -0.05501  0.200101\n> [319,]  0.09529  1.41423 -1.382712\n> [320,]  0.33555  0.25549 -0.022258\n> [321,]  0.59167  1.35870  0.412462\n> [322,]  1.09530 -0.87092 -0.338222\n> [323,] -1.94962  0.23914 -0.605945\n> [324,] -0.90414  0.36988 -0.938254\n> [325,] -0.95494 -0.79750  0.108207\n> [326,]  0.96172 -1.13319  0.937566\n> [327,]  0.30438 -0.12261 -0.054201\n> [328,]  0.73068 -1.21731 -1.212104\n> [329,] -1.84049 -0.06647  0.712258\n> [330,]  0.42551 -0.23563 -1.223503\n> [331,]  1.47277  0.20502  1.052293\n> [332,] -0.75907  0.96233  1.085536\n> [333,] -1.21375  1.21865  0.631939\n> [334,] -0.90839  0.35460  0.783609\n> [335,] -0.75533  0.01179  0.076865\n> [336,] -0.80705  0.25524 -0.080266\n> [337,]  1.20456 -0.80241 -0.007428\n> [338,] -0.39301 -0.12431 -0.521390\n> [339,]  0.09858  0.75171  0.250673\n> [340,]  0.05565  0.89017 -0.044932\n> [341,] -1.37632  0.62437  0.351195\n> [342,] -0.71669  0.79623 -0.442349\n> [343,] -0.51330  0.30929  0.519968\n> [344,] -0.50818 -0.65325 -0.210873\n> [345,]  0.22201 -0.08255 -0.568643\n> [346,]  0.79863 -0.01926  0.296055\n> [347,] -0.75165 -0.41656 -1.045483\n> [348,] -0.44083  0.34060 -0.211199\n> [349,]  1.06070  0.62224 -0.666412\n> [350,]  0.17606 -0.15952 -0.399415\n> [351,]  1.65637 -0.28332  0.358025\n> [352,]  0.25145 -0.61578 -0.264908\n> [353,] -1.10266 -0.04530  0.481103\n> [354,]  0.41340 -0.85969 -0.513597\n> [355,] -0.21130 -0.31045 -0.285809\n> [356,]  0.06969 -0.75547  0.436153\n> [357,]  0.63839 -0.06172  0.232435\n> [358,] -0.46479  0.53129 -0.084766\n> [359,] -1.01281  0.07389 -0.950152\n> [360,]  0.29616  0.33670 -0.096206\n> [361,]  1.47887 -0.00321  1.343584\n> [362,]  0.10095 -0.75951 -1.046935\n> [363,] -0.05194  0.28141 -0.463767\n> [364,] -0.83998  0.47878  0.621333\n> [365,]  0.71411 -0.67768  0.436926\n> [366,] -0.04245 -0.68972 -0.371779\n> [367,]  0.05277 -0.31389  0.285059\n> [368,]  0.14082  0.24758  0.356710\n> [369,]  0.57168 -1.37316  0.399063\n> [370,] -0.60132  0.21719  0.334246\n> [371,] -0.27803 -1.01832  1.075438\n> [372,] -1.53384 -0.09292 -0.431847\n> [373,]  0.71200  0.43113 -0.357880\n> [374,]  0.60573  0.14440 -0.374983\n> [375,] -0.87308  0.07889 -0.096816\n> [376,] -0.01215  1.33557  0.646193\n> [377,]  0.87327 -0.29874  0.427905\n> [378,] -1.13997 -0.12785 -0.366527\n> [379,]  0.85276  0.38835 -0.125325\n> [380,] -0.29928 -0.09350 -0.324652\n> [381,] -0.77589  0.35805  0.386553\n> [382,]  0.40013  0.73075  1.242977\n> [383,]  0.01393  0.15633  0.114755\n> [384,] -0.78309  1.41191  1.148545\n> [385,]  1.44368  0.55568 -0.630277\n> [386,] -0.19104  0.61441  0.514243\n> [387,] -0.66882 -0.06045  1.426089\n> [388,]  1.86402 -0.68995  0.228943\n> [389,]  0.06095 -1.82778  0.302257\n> [390,]  0.13685 -0.72720  0.019238\n> [391,]  0.97705  0.79508  0.181438\n> [392,] -0.92785 -0.84554 -0.760591\n> [393,]  0.04939 -0.55932  0.260846\n> [394,]  1.30886 -0.67049  0.406676\n> [395,] -0.20378 -0.81770  0.874419\n> [396,] -0.91348  0.62812  1.377931\n> [397,] -0.01459 -0.36610  0.976352\n> [398,] -0.69443  0.20886 -0.211351\n> [399,]  0.18576 -0.06839 -0.822864\n> [400,] -0.49669 -0.34962 -0.296680\n> [401,]  1.27666  0.38571  1.034192\n> [402,] -1.14927  0.21575 -0.478247\n> [403,] -0.11588  0.36580 -0.267516\n> [404,]  1.13835 -0.33799  0.745534\n> [405,] -0.45256 -1.48788  0.393459\n> [406,] -0.27956 -0.10200 -0.140999\n> [407,] -1.00412 -0.05541  1.438384\n> [408,]  0.81347 -1.25384  0.698627\n> [409,]  0.45480 -0.60781  0.387385\n> [410,]  0.86309  0.06221  0.528187\n> [411,] -0.18675 -0.72675  1.302956\n> [412,] -0.64424 -1.16993  0.270698\n> [413,]  0.95631 -0.03485  0.323155\n> [414,] -0.41651  0.51591  0.798723\n> [415,]  0.00055  0.96957 -0.645252\n> [416,]  0.33199 -0.60437  0.104745\n> [417,] -0.82448  0.48150  0.211128\n> [418,]  1.43005 -0.20914  0.119165\n> [419,] -0.16739  0.33395  0.229167\n> [420,]  0.58930 -0.37437  0.065063\n> [421,]  0.20977 -0.54628 -0.536032\n> [422,] -1.11997 -0.58429  0.462717\n> [423,] -0.40402  0.92504 -0.515688\n> [424,]  1.83903 -0.10038  0.836719\n> [425,] -0.28930 -1.09939  0.484685\n> [426,]  0.21270  0.27533 -0.609612\n> [427,]  0.51170 -0.41357  0.404414\n> [428,]  0.53900 -0.54828  0.882568\n> [429,] -1.39727  0.34059 -0.580829\n> [430,] -0.01380  0.52519 -0.520526\n> [431,] -0.59480 -0.95094  0.614800\n> [432,]  1.58322  0.43127  0.469598\n> [433,]  0.12147  0.65743  0.784547\n> [434,] -0.33561  0.22418 -0.188947\n> [435,] -0.40494 -0.27930 -0.166831\n> [436,] -1.78965  0.15412  0.082562\n> [437,] -0.21693  0.18305  0.101418\n> [438,]  0.47379 -0.06431  0.464622\n> [439,]  0.86646  0.09650 -0.724021\n> [440,] -0.45103 -0.11099 -0.307164\n> [441,] -1.11906  0.55226  0.284891\n> [442,] -0.84318  0.20339  0.406598\n> [443,] -0.70096  0.01799  0.163462\n> [444,]  1.04615  0.21521  0.312986\n> [445,] -0.36443  0.07644 -0.511958\n> [446,]  0.30815 -0.34895 -0.135539\n> [447,] -0.77007 -0.82796 -0.539336\n> [448,] -0.98932  0.05354 -0.312516\n> [449,] -0.44476  0.43003 -0.548188\n> [450,] -0.88631 -0.29373 -0.952403\n> [451,] -0.51062 -0.94689 -1.046924\n> [452,] -0.43334 -0.41703 -1.777777\n> [453,] -1.18608  0.80956  0.737901\n> [454,]  0.26510 -0.20805  0.540262\n> [455,] -1.22825  0.20640  0.281669\n> [456,] -0.11349 -0.33348  0.222914\n> [457,] -0.65008 -0.46775 -0.086582\n> [458,]  0.22179  0.01446  0.407965\n> [459,]  1.11054 -0.82442 -0.456806\n> [460,]  0.89626 -1.04482 -0.556402\n> [461,]  0.38603 -0.55465  0.846175\n> [462,]  0.70497  0.17616 -0.729173\n> [463,] -1.60433 -0.15992  0.519124\n> [464,]  0.65776 -0.48668 -0.536218\n> [465,] -0.72347 -0.79815 -0.097501\n> [466,] -1.58807 -0.38819  1.505076\n> [467,] -0.35676  0.60796 -0.544321\n> [468,] -0.32351 -0.04916  0.181867\n> [469,] -0.74455 -0.43092  0.412614\n> [470,]  0.01446  0.43787  0.858690\n> [471,]  1.09513 -0.92987  0.489264\n> [472,] -1.24138 -0.15590 -0.310609\n> [473,] -1.75062  0.27055 -0.031332\n> [474,] -0.70079  1.38709 -1.128690\n> [475,] -0.46024 -0.03715 -0.127745\n> [476,] -0.47944 -0.68284  0.290525\n> [477,]  0.95117 -0.16706  0.261961\n> [478,]  1.49086 -0.06219  0.087199\n> [479,]  0.09001 -0.27006  0.199347\n> [480,] -1.27074  0.08907  0.360950\n> [481,]  0.05031  0.37567 -0.354565\n> [482,]  0.43263 -0.07188 -0.376423\n> [483,]  0.41661  1.27180  0.495694\n> [484,]  1.21374  0.79112  0.395911\n> [485,]  1.23881  1.31816  0.579235\n> [486,]  0.93056  0.10853 -1.101901\n> [487,] -0.88367  0.42990  0.363537\n> [488,]  0.15798  0.84335  0.806532\n> [489,] -1.97494  0.43747  1.004816\n> [490,]  0.25948  0.03084 -0.605225\n> [491,] -1.46092  0.09689  0.748804\n> [492,]  1.63121  0.29443  0.041260\n> [493,]  0.77078  0.74413 -0.485693\n> [494,]  1.47771  0.44470 -0.366460\n> [495,] -0.52713  0.26032  0.195517\n> [496,] -1.77483 -1.32954  0.431818\n> [497,]  2.15328 -0.90404  0.399704\n> [498,] -0.09215 -0.35912 -0.612312\n> [499,]  0.58459  0.42226 -0.419838\n> [500,]  1.83417 -0.44346  0.293011\n> [501,]  0.16381 -0.83302 -1.425063\n> [502,]  0.34142 -0.08463 -0.537581\n> [503,] -1.39970 -1.29848  0.457295\n> [504,]  1.12291  0.03779 -0.130319\n> [505,]  0.07089  0.63510 -0.720080\n> [506,]  0.27509 -0.60385 -0.265576\n> [507,] -1.64634 -0.50337 -0.518572\n> [508,] -1.72592  0.42380 -0.223833\n> [509,] -0.37786  0.09651  0.122380\n> [510,] -0.79010 -1.47491  0.154296\n> [511,]  0.02388 -0.07540 -0.736831\n> [512,] -0.43903  0.36763 -0.069755\n> [513,]  0.50565  0.87603 -0.126821\n> [514,] -0.03577 -0.57078 -0.089929\n> [515,] -0.16896 -0.46195  0.087702\n> [516,] -0.39903  0.10346 -0.680496\n> [517,] -0.35431  0.45841 -0.662897\n> [518,]  1.76033  0.43322  0.652647\n> [519,]  0.98478 -0.54026 -1.914602\n> [520,]  1.45753 -0.98461  0.205854\n> [521,]  1.77306 -0.59561 -0.357027\n> [522,]  0.09873 -1.23250 -0.755270\n> [523,] -0.50245 -0.86165  0.579785\n> [524,]  0.92275  0.48659 -0.035206\n> [525,] -0.41360 -0.90545  0.454309\n> [526,]  0.32531  0.34370 -0.736348\n> [527,]  1.14229  0.00949  0.184786\n> [528,] -0.32221 -0.57189 -0.126943\n> [529,] -0.11059 -1.03184  0.878596\n> [530,]  0.97721  0.14640 -1.099916\n> [531,]  0.55929  0.26347  0.723967\n> [532,]  1.74585  1.11444  0.002733\n> [533,] -1.51729 -0.48058 -0.309608\n> [534,] -0.76027  0.95219 -0.242572\n> [535,]  0.79158  0.16687  1.033396\n> [536,] -0.18159 -0.39598  1.176274\n> [537,] -1.20870  0.25652  0.074042\n> [538,] -1.18920  0.21829  0.105148\n> [539,]  0.23239  0.19713 -0.259996\n> [540,]  0.87332  0.20464  0.331284\n> [541,] -0.24447  0.18395  0.307492\n> [542,]  1.03938 -1.55661 -0.456260\n> [543,]  0.41200 -0.18069 -0.059858\n> [544,]  0.31538  0.88190  1.076151\n> [545,] -0.02860  0.62930  1.209828\n> [546,]  1.75216  0.03884  0.366975\n> [547,]  0.14519  0.65410  0.319849\n> [548,]  0.56229  0.60793 -0.438190\n> [549,] -1.13205  0.46319  1.067935\n> [550,] -0.40453 -0.42594  0.078046\n> [551,] -0.60662 -1.07105 -0.027671\n> [552,]  0.93800 -0.62258 -0.534508\n> [553,] -0.28281  0.20082 -0.217598\n> [554,] -0.63646  0.10114 -0.841679\n> [555,] -0.39908 -0.60743 -0.129885\n> [556,] -1.72069 -1.12001  0.018838\n> [557,] -0.35823  0.59221 -0.366177\n> [558,] -0.98268  0.01113 -0.725684\n> [559,]  0.26837 -0.00383  0.469915\n> [560,]  1.08203  0.52091 -0.146671\n> [561,] -0.70238 -0.34666 -0.456091\n> [562,]  0.43520 -0.27843 -0.502703\n> [563,] -0.13641  1.14710 -0.712778\n> [564,] -1.30729  0.92807  0.473049\n> [565,] -1.82119  1.26209 -0.992294\n> [566,] -0.65999  1.60465  0.394147\n> [567,] -1.09816  0.79374  0.082250\n> [568,] -1.53987 -0.01694 -1.320863\n> [569,] -0.88871 -1.35122 -0.431765\n> [570,] -0.28757 -0.16263  0.413989\n> [571,]  0.12577 -1.24723  0.145452\n> [572,]  0.21388  0.88245 -0.050297\n> [573,]  1.25079  0.12336 -0.467852\n> [574,] -0.74117  0.49006  0.355960\n> [575,] -0.35830 -0.19524  0.207228\n> [576,]  0.38857 -0.78682 -0.017537\n> [577,] -0.92018  0.06124  0.283730\n> [578,] -1.08115 -0.66693 -0.070281\n> [579,]  0.67258 -0.38055  0.196324\n> [580,]  0.22928  1.08180 -0.886702\n> [581,] -0.27395  1.02415 -0.517330\n> [582,] -0.63148  0.43128 -0.385444\n> [583,]  1.69891  0.97551  0.115331\n> [584,]  1.42372  0.04413  0.501831\n\n# Extraire les loadings\nloadings(res.fa)\n> \n> Loadings:\n>    Factor1 Factor2 Factor3\n> i1          0.474         \n> i2          0.375         \n> i3          0.430         \n> i4  0.711   0.153         \n> i5  0.716          -0.173 \n> i6  0.691           0.229 \n> i7                  0.247 \n> i8                  0.403 \n> i9                  0.326 \n> \n>                Factor1 Factor2 Factor3\n> SS loadings      1.498   0.586   0.421\n> Proportion Var   0.166   0.065   0.047\n> Cumulative Var   0.166   0.232   0.278\n# ou pour voir tous les loadings\nhead(res.fa$loadings[])\n>    Factor1 Factor2  Factor3\n> i1  0.0238  0.4743  0.06556\n> i2  0.0285  0.3752 -0.04153\n> i3  0.0194  0.4300  0.05369\n> i4  0.7109  0.1527  0.00653\n> i5  0.7163  0.0547 -0.17282\n> i6  0.6906 -0.0862  0.22903"},{"path":"références.html","id":"références","chapter":"Références","heading":"Références","text":"","code":""}]
