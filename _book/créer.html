<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title> 12 Créer | Méthodes Quantitatives avec R</title>
<meta name="author" content="P.-O. Caron">
<meta name="description" content="Dans le chapitre sur la régression, les variables indépendantes sont créées simultanément à partir d’une matrice de corrélation. Dans un modèle plus élaboré, plutôt que d’ignorer ces corrélations,...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content=" 12 Créer | Méthodes Quantitatives avec R">
<meta property="og:type" content="book">
<meta property="og:url" content="mqr.teluq.ca/créer.html">
<meta property="og:description" content="Dans le chapitre sur la régression, les variables indépendantes sont créées simultanément à partir d’une matrice de corrélation. Dans un modèle plus élaboré, plutôt que d’ignorer ces corrélations,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content=" 12 Créer | Méthodes Quantitatives avec R">
<meta name="twitter:description" content="Dans le chapitre sur la régression, les variables indépendantes sont créées simultanément à partir d’une matrice de corrélation. Dans un modèle plus élaboré, plutôt que d’ignorer ces corrélations,...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
</head>
<body>
<p>
    \(
    \newcommand{\EX}{\mathbb{E}}
    \)
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css"></p>

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Méthodes Quantitatives avec R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Bienvenue!</a></li>
<li><a class="" href="pr%C3%A9face.html">Préface</a></li>
<li class="book-part">Rudiments</li>
<li><a class="" href="commencer.html"><span class="header-section-number">1</span> Commencer</a></li>
<li><a class="" href="programmer.html"><span class="header-section-number">2</span> Programmer</a></li>
<li><a class="" href="calculer.html"><span class="header-section-number">3</span> Calculer</a></li>
<li><a class="" href="exercice-rudiments.html">Exercices</a></li>
<li class="book-part">Jeux de données</li>
<li><a class="" href="importer.html"><span class="header-section-number">4</span> Importer</a></li>
<li><a class="" href="entr%C3%A9e-de-donn%C3%A9es.html"><span class="header-section-number">5</span> Entrée de données</a></li>
<li><a class="" href="manipuler.html"><span class="header-section-number">6</span> Manipuler</a></li>
<li><a class="" href="visualiser.html"><span class="header-section-number">7</span> Visualiser</a></li>
<li><a class="" href="exercice-gestion.html">Exercices</a></li>
<li class="book-part">Statistique</li>
<li><a class="" href="inf%C3%A9rer.html"><span class="header-section-number">8</span> Inférer</a></li>
<li><a class="" href="analyser.html"><span class="header-section-number">9</span> Analyser</a></li>
<li><a class="" href="simuler.html"><span class="header-section-number">10</span> Simuler</a></li>
<li><a class="" href="exercice-analyse.html">Exercices</a></li>
<li class="book-part">Modèle linéaire</li>
<li><a class="" href="r%C3%A9gression.html"><span class="header-section-number">11</span> Régression</a></li>
<li><a class="active" href="cr%C3%A9er.html"><span class="header-section-number">12</span> Créer</a></li>
<li><a class="" href="lanalyse-en-composantes-principales.html"><span class="header-section-number">13</span> L’analyse en composantes principales</a></li>
<li><a class="" href="solutions.html">Solutions</a></li>
<li><a class="" href="r%C3%A9f%C3%A9rences.html">Références</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/quantmeth/MQR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="créer" class="section level1" number="12">
<h1>
<span class="header-section-number"> 12</span> Créer<a class="anchor" aria-label="anchor" href="#cr%C3%A9er"><i class="fas fa-link"></i></a>
</h1>
<p>Dans le chapitre sur la <a href="r%C3%A9gression.html#r%C3%A9gression">régression</a>, les variables indépendantes sont créées simultanément à partir d’une matrice de corrélation. Dans un modèle plus élaboré, plutôt que d’ignorer ces corrélations, comme le fait l’analyse de régression, il est possible de modéliser explicitement les liens entre les variables prédictrices.
Cela revient à décrire un modèle de façon à ce qu’une première variable <em>cause</em> la seconde; la première et la seconde <em>causent</em> la troisième et ainsi de suite.</p>
<p>La modélisation en système d’équations deviendra primordial dans les prochaines sections, notamment pour les analyses de trajectoires, de médiation, de modération, et plus. C’est le fondement de la modélisation par équations structurelles (SEM).
Dans ce chapitre<a href="r%C3%A9f%C3%A9rences.html#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, des techniques de modélisation avancées sont présentées. Elles s’appliquent à des modèles de trajectoires récursifs, c’est-à-dire que la <em>causation</em> s’enchaîne dans une direction, de la première variable vers la dernière.</p>
<div id="la-loi-de-la-somme-des-variances" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> La loi de la somme des variances<a class="anchor" aria-label="anchor" href="#la-loi-de-la-somme-des-variances"><i class="fas fa-link"></i></a>
</h2>
<p>La loi de la somme des variances explique comment additionner la variance de variables aléatoires, quelle que soit leur distribution <span class="citation">(<a href="r%C3%A9f%C3%A9rences.html#ref-casella2002" role="doc-biblioref">Casella &amp; Berger, 2002</a>)</span>. Le cas le plus simple est celui de deux variables aléatoires, comme <span class="math inline">\(x_1\)</span> et <span class="math inline">\(x_2\)</span>, et la façon dont elles s’additionnent pour former une troisième variable, <span class="math inline">\(y\)</span>. Elles donnent un modèle simple représenté par l’équation <a href="cr%C3%A9er.html#eq:eq1">(12.1)</a>.</p>
<p><span class="math display" id="eq:eq1">\[\begin{equation}
y=x_1+x_2
\tag{12.1}
\end{equation}\]</span></p>
<p>La variance de leur somme prend la forme suivante
<span class="math display" id="eq:eq2">\[\begin{equation}
\sigma_y^2=\sigma_{x_1}^2+\sigma_{x_2}^2
\tag{12.2}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(\sigma^2\)</span> est le symbole habituel de la variance. La variance de la somme (ou de la différence) de deux variables indépendantes (non-corrélées) aléatoires est leur somme. En fait, la somme de <span class="math inline">\(p\)</span> variables indépendantes est la somme de leurs variances. En syntaxe <strong>R</strong>, ces équations ressemblent à ceci.</p>
<div class="sourceCode" id="cb150"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1155</span><span class="op">)</span>                  <span class="co"># reproductibilité</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100000</span>                      <span class="co"># taille élevée pour la précision</span>
<span class="va">s.x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>                  <span class="co"># variance de 2 </span>
<span class="va">s.x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span>                  <span class="co"># variance de 3</span>

<span class="co"># Création des variables</span>
<span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sd <span class="op">=</span> <span class="va">s.x1</span><span class="op">)</span> 
<span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sd <span class="op">=</span> <span class="va">s.x2</span><span class="op">)</span>     </code></pre></div>
<p>Dans cet exemple, les variances de <code>x1</code> et de <code>x3</code> sont égales à 2 et 3 respectivement. Si ces deux variables sont additionnées pour créer <code>y</code>, l’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a> implique que la variance est de <span class="math inline">\(\sigma^2_{x_{1}} + \sigma^2_{x_2} = 2 + 3= 5\)</span></p>
<div class="sourceCode" id="cb151"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
<span class="co">#&gt; [1] 5</span>

<span class="co"># Vérifier par</span>
<span class="va">s.x1</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">s.x2</span><span class="op">^</span><span class="fl">2</span>
<span class="co">#&gt; [1] 5</span></code></pre></div>
<p>Le résultat concorde.</p>
<p>En pratique, il est rare que les variables prédictrices soient non corrélées. Ici, aucune corrélation n’a été précisée entre <code>x1</code> et <code>x2</code>. En développant le cas général où les variables sont corrélées, si la covariance entre <span class="math inline">\(x_1\)</span> et <span class="math inline">\(x_2\)</span> est <span class="math inline">\(\sigma_{x_1,x_2}\)</span>, alors la loi de la somme des variances devient ceci.</p>
<p><span class="math display" id="eq:eq3">\[\begin{equation}
\sigma_y^2=\sigma_{x_1}^2+\sigma_{x_2}^2+2\sigma_{x_1,x_2}
\tag{12.3}
\end{equation}\]</span></p>
<p>L’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a> est un cas particulier lorsque <span class="math inline">\(\sigma_{x_1 x_2}=0\)</span>. En syntaxe <strong>R</strong>, pour <span class="math inline">\(\rho = .5\)</span>, cela revient à programmer ceci.</p>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Création de variables corrélées</span>
<span class="va">rho</span> <span class="op">&lt;-</span> <span class="fl">.5</span>                          <span class="co"># corrélation de .5</span>
<span class="va">s.x1x2</span> <span class="op">&lt;-</span> <span class="va">rho</span> <span class="op">*</span> <span class="va">s.x1</span> <span class="op">*</span> <span class="va">s.x2</span>        <span class="co"># covariance de x et y</span>
<span class="va">S</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">s.x1</span><span class="op">^</span><span class="fl">2</span>, <span class="va">s.x1x2</span>,      <span class="co"># matrice de covariance</span>
             <span class="va">s.x1x2</span>, <span class="va">s.x2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, ncol <span class="op">=</span>  <span class="fl">2</span>, nrow <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>

<span class="co"># Création de variables</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, Sigma <span class="op">=</span> <span class="va">S</span>, mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, empirical <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> ; <span class="va">x2</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span>

<span class="co"># La somme de deux variables corrélées</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
<span class="co">#&gt; [1] 7.45</span>

<span class="co"># Vérifier par</span>
<span class="va">s.x1</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">s.x2</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">s.x1x2</span>
<span class="co">#&gt; [1] 7.45</span></code></pre></div>
<p>Pour rappel, la corrélation s’obtient avec <span class="math inline">\(\rho_{x_1,x_2}=\sigma_{x_1,x_2}/(\sigma_{x_1} \sigma_{x_2})\)</span> et inversement la covariance est obtenue par <span class="math inline">\(\rho_{x_1,x_2} \sigma_{x_1} \sigma_{x_2}=\sigma_{x_1,x_2}\)</span>. En calculant la covariance d’abord, soit</p>
<p><span class="math display">\[\sigma_{x_1,x_2} = \rho_{x_1,x_2} \sigma_{x_1} \sigma_{x_2} = 0.5 \times 1.414 \times 1.732 = 1.225\]</span>
et confirmé par <strong>R</strong>.</p>
<div class="sourceCode" id="cb153"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">s.x1x2</span>
<span class="co">#&gt; [1] 1.22</span></code></pre></div>
<p>L’équation <a href="cr%C3%A9er.html#eq:eq3">(12.3)</a> pour la variance de <code>y</code> mène à</p>
<p><span class="math display">\[\sigma_y^2=\sigma_{x_1}^2+\sigma_{x_2}^2+2\sigma_{x_1,x_2}= 2+3 + 2 \times 1.225  = 7.449\]</span>
très près du 7.449 simulé.</p>
<p>La loi des sommes des variances est directement reliée au théorème de Pythagore tel qu’illustré par l’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a>. Deux variables non corrélées peuvent être envisagées comme formant un triangle rectangle. L’équation <a href="cr%C3%A9er.html#eq:eq3">(12.3)</a> est la règle du cosinus, ou la généralisation du théorème de Pythagore pour les triangles non rectangulaires, de sorte que la corrélation représente géométriquement un angle <span class="citation">(<a href="r%C3%A9f%C3%A9rences.html#ref-Rodgers88" role="doc-biblioref">Rodgers &amp; Nicewander, 1988</a>)</span>.</p>
<p>La Figure <a href="cr%C3%A9er.html#fig:tri1111">12.1</a> montre un triangle rectangle dont l’hypoténuse représente l’écart type de la somme de deux variables indépendantes. Lorsque l’angle est à 90<span class="math inline">\(^\circ\)</span>, la corrélation est nulle. Il suffit de mettre les droites au carré pour retrouver l’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a>. Lorsque l’angle rétréci, plus les deux lignes <span class="math inline">\(\sigma\)</span> se rapprochent et projettent sur une plus longue distance la somme des variances. Lorsqu’elles sont collées l’une sur l’autre, la corrélation est parfaite.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tri1111"></span>
<img src="image/tri.png" alt="Représentation des variables sous forme de triangles" width="75%" height="75%"><p class="caption">
Figure 12.1: Représentation des variables sous forme de triangles
</p>
</div>
<p>Une autre façon plus mathématiquement simple est de calculer la loi de la somme des variances est de concevoir la variance de la somme de <span class="math inline">\(p\)</span> variables comme la <em>grande somme</em> de leur matrice de covariance. La grande somme est une fonction mathématique informelle qui fait référence à la somme de tous les éléments d’une matrice. Soit <span class="math inline">\(\mathbf{\Sigma}\)</span>, la matrice de variance-covariance de deux variables aléatoires <span class="math inline">\(x_1\)</span> et <span class="math inline">\(x_2\)</span> (les éléments diagonaux sont des variances), alors</p>
<p><span class="math display" id="eq:eq4">\[\begin{equation}
\sigma_y^2 = \text{grande somme}(\mathbf{\Sigma}) = \text{grande somme}\left(\begin{array}{cc}
\sigma_{x_1}^2 &amp; \sigma_{x_1,x_2} \\
\sigma_{x_1,x_2} &amp; \sigma_{x_2}^2
\end{array}\right)
\tag{12.4}
\end{equation}\]</span></p>
<p>ce qui, en faisant la somme, conduit à l’équation <a href="cr%C3%A9er.html#eq:eq3">(12.3)</a>. Encore une fois, si <span class="math inline">\(\sigma_{x_1,x_2}=0\)</span>, alors l’équation <a href="cr%C3%A9er.html#eq:eq4">(12.4)</a> est égale à l’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a>. Cette formulation a l’avantage de montrer l’origine des deux covariances dans l’équation <a href="cr%C3%A9er.html#eq:eq3">(12.3)</a>. En code <strong>R</strong>, la syntaxe est dès plus rudimentaire.</p>
<div class="sourceCode" id="cb154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span>
<span class="co">#&gt; [1] 7.45</span></code></pre></div>
<p>Même si la notation matricielle peut sembler peu attrayante au départ, elle devient beaucoup plus intéressante lorsque le nombre de variables, <span class="math inline">\(p\)</span>, augmente, car il y a <span class="math inline">\(p(p-1)/2\)</span> éléments hors diagonale à ajouter aux <span class="math inline">\(p\)</span> variances. Elle deviendra même indispensable à la fin de ce chapitre.</p>
<p>Pour un exemple à <span class="math inline">\(p=3\)</span>, l’équation <a href="cr%C3%A9er.html#eq:eq4">(12.4)</a> devient ceci.</p>
<p><span class="math display" id="eq:eq5">\[\begin{equation}
\sigma_y^2=\text{grand somme}(\mathbf{\Sigma})=\text{grand somme}
\left(\begin{array}{ccc}
\sigma_{x_1}^2&amp;\sigma_{x_1,x_2}&amp;\sigma_{x_1 x_3} \\
\sigma_{x_1,x_2}&amp;\sigma_{x_2}^2&amp;\sigma_{x_2 x_3}\\
\sigma_{x_1 x_3}&amp;\sigma_{x_2 x_3}&amp;\sigma_{x_3}^2
\end{array}
\right)
\tag{12.5}
\end{equation}\]</span></p>
<p>Sous la forme linéaire, l’équation <a href="cr%C3%A9er.html#eq:eq5">(12.5)</a> devient</p>
<p><span class="math display" id="eq:eq6">\[\begin{equation}
\sigma_y^2=\sigma_{x_1}^2+\sigma_{x_2}^2+\sigma_{x_3}^2+2\sigma_{x_1,x_2}+2\sigma_{x_1 x_3}+2\sigma_{x_2 x_3}
\tag{12.6}
\end{equation}\]</span></p>
<p>et augmente à mesure que <span class="math inline">\(p\)</span> s’accroît.</p>
<p>Une opération matricielle équivalente à la grande somme est</p>
<p><span class="math display" id="eq:GS">\[\begin{equation}
\mathbf{1}^{\prime} \mathbf{\Sigma} \mathbf{1}
\tag{12.7}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(1\)</span> est un vecteur de longueur <span class="math inline">\(p\)</span> contenant seulement 1. Voici en syntaxe <strong>R</strong>.</p>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Un</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>        <span class="co"># Création du vecteur 1</span>
<span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">Un</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Un</span>  <span class="co"># Grande somme</span>
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,] 7.45</span></code></pre></div>
<p>Cette opération produit la somme de tous les éléments de <span class="math inline">\(\mathbf{\Sigma}\)</span> (<code>Sigma</code>). Cela sera utile pour dériver un cas plus général dans la section suivante.</p>
<div id="ajout-des-constantes-déchelle-beta" class="section level3" number="12.1.1">
<h3>
<span class="header-section-number">12.1.1</span> Ajout des constantes d’échelle <span class="math inline">\(\beta\)</span><a class="anchor" aria-label="anchor" href="#ajout-des-constantes-d%C3%A9chelle-beta"><i class="fas fa-link"></i></a>
</h3>
<p>Les équations <a href="cr%C3%A9er.html#eq:eq3">(12.3)</a>, <a href="cr%C3%A9er.html#eq:eq4">(12.4)</a>, <a href="cr%C3%A9er.html#eq:eq5">(12.5)</a> et <a href="cr%C3%A9er.html#eq:eq6">(12.6)</a> sont des cas particuliers d’une loi plus générale. Elles ne fonctionneraient pas si des constantes d’échelle (<em>scaling constant</em>) étaient ajoutées (qui seront plus tard des coefficients de régression) ou pour calculer la différence (un type d’échelle également).</p>
<p>La situation où la variable <span class="math inline">\(y\)</span> correspond au produit d’une constante <span class="math inline">\(\beta\)</span> et de la variable <span class="math inline">\(x\)</span> comme :</p>
<p><span class="math display" id="eq:eq7">\[\begin{equation}
y=\beta x
\tag{12.8}
\end{equation}\]</span></p>
<p>un modèle linéaire qui ne comporte pas d’erreur (<span class="math inline">\(\epsilon\)</span> est ignoré pour l’instant).</p>
<p>Il peut être utile de considérer <span class="math inline">\(\beta\)</span> comme, éventuellement, le degré de relation entre deux variances, mais aussi comme un pur modificateur de l’écart type (également une constante d’échelle). De la même manière, une variable aléatoire avec une moyenne de <span class="math inline">\(0\)</span> et un écart type de <span class="math inline">\(1\)</span>, <span class="math inline">\(x\sim \mathcal{N}(0,1)\)</span>, multipliée par la valeur <span class="math inline">\(\beta\)</span>, un facteur d’échelle arbitraire, devient distribuée comme <span class="math inline">\(\beta x \sim \mathcal{N}(0,\beta)\)</span>. Ainsi, <span class="math inline">\(\beta\)</span> a modifié (ou mis à l’échelle) l’écart type de la distribution. Un cas connexe et fréquemment rencontré est lorsque les données sont normalisées en tant que score-<span class="math inline">\(z\)</span> ou non normalisées (divisées ou multipliées respectivement par <span class="math inline">\(\sigma\)</span>). La contribution est un écart-type, donc <span class="math inline">\(\beta\)</span> doit être élevé au carré pour donner la variance de <span class="math inline">\(y\)</span>, soit <span class="math inline">\(\beta^2\)</span>. Cela mène à l’équation <a href="cr%C3%A9er.html#eq:eq8">(12.9)</a> qui permet le calcul de la variance de <span class="math inline">\(y\)</span>.</p>
<p><span class="math display" id="eq:eq8">\[\begin{equation}
\sigma_y^2=\beta^2 \sigma_x^2
\tag{12.9}
\end{equation}\]</span></p>
<p>En termes de syntaxe <strong>R</strong>, cela se traduit (toujours avec le même exemple).</p>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">beta</span> <span class="op">=</span> <span class="fl">4</span>
<span class="co"># La variance de x1 est toujours de 2</span>
<span class="va">y</span> <span class="op">=</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">x1</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
<span class="co">#&gt; [1] 32</span>

<span class="co"># Vérifier par</span>
<span class="va">beta</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">s.x1</span><span class="op">^</span><span class="fl">2</span>
<span class="co">#&gt; [1] 32</span></code></pre></div>
<p>L’équation <a href="cr%C3%A9er.html#eq:eq8">(12.9)</a> donne <span class="math inline">\(\sigma_y^2=\beta^2 \sigma_x^2=4^2 \times 2= 32\)</span>, ce qui est identique.</p>
<p>L’étape suivante consiste à considérer un modèle avec deux variables et deux constantes d’échelles, ce qui rappelle de plus en plus la régression. Le modèle linéaire prend la forme suivante</p>
<p><span class="math display" id="eq:eq9">\[\begin{equation}
y=\beta_1 x_1+\beta_2 x_2
\tag{12.10}
\end{equation}\]</span></p>
<p>qui est le même modèle que l’équation <a href="cr%C3%A9er.html#eq:eq1">(12.1)</a> où les constantes d’échelle <span class="math inline">\(\beta_i\)</span> sont ajoutées. Pour considérer les constantes d’échelle, la loi de la somme des variances basée sur l’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a> devient pour deux variables indépendantes</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
\sigma_y^2=\beta_1^2 \sigma_{x_1}^2+\beta_2^2 \sigma_{x_2}^2
\]</span>
<!-- (\#eq:eq10) -->
<!-- \end{equation} --></p>
<p>et quand ils covarient, comme l’équation <a href="cr%C3%A9er.html#eq:eq3">(12.3)</a>, cela devient</p>
<p><span class="math display" id="eq:eq11">\[\begin{equation}
\sigma_y^2=\beta_1^2 \sigma_{x_1}^2+\beta_2^2 \sigma_{x_2}^2+2\beta_1 \beta_2 \sigma_{x_1,x_2}
\tag{12.11}
\end{equation}\]</span></p>
<p>Dans ces équations, les <span class="math inline">\(\beta_i\)</span> mettent à l’échelle la variance et la covariance. Ces équations se vérifient avec <strong>R</strong> (toujours avec le même exemple).</p>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># En suivant, l'exemple précédent</span>
<span class="va">beta1</span> <span class="op">=</span> <span class="fl">4</span>
<span class="va">beta2</span> <span class="op">=</span> <span class="fl">5</span>
<span class="va">y</span> <span class="op">=</span> <span class="va">beta1</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">beta2</span> <span class="op">*</span> <span class="va">x2</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> 
<span class="co">#&gt; [1] 156</span>

<span class="co"># Vérifier par</span>
<span class="va">beta1</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">s.x1</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">beta2</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">s.x2</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">beta1</span> <span class="op">*</span> <span class="va">beta2</span> <span class="op">*</span> <span class="va">s.x1x2</span> 
<span class="co">#&gt; [1] 156</span></code></pre></div>
<p>tel qu’attendue par l’équation <a href="cr%C3%A9er.html#eq:eq12">(12.12)</a>.</p>
<p><span class="math display">\[
\sigma_y^2=\beta_1^2 \sigma_{x_1}^2+\beta_2^2 \sigma_{x_2}^2+2\beta_1 \beta_2 \sigma_{x_1,x_2} =
4^2 \times 2 + 5^2 \times 3 + 2 \times 4 \times 5 \times 1.225 = 155.99
\]</span></p>
<p>Comme précédemment, la formule en termes d’algèbre matricielle est plus simple et plus élégante, soit</p>
<p><span class="math display" id="eq:eq12">\[\begin{equation}
\sigma_y^2 = \mathbf{B}^{\prime} \mathbf{\Sigma} \mathbf{B}
\tag{12.12}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(\mathbf{B}\)</span> (<span class="math inline">\(\beta\)</span> majuscule) est un vecteur contenant tous les coefficients de régression <span class="math inline">\(\beta_i\)</span> de longueur <span class="math inline">\(p\)</span> prédisant <span class="math inline">\(y\)</span> et s’appliquant à un nombre quelconque de prédicteurs <span class="math inline">\(x\)</span>. Le premier <span class="math inline">\(\prime\)</span> est le symbole de transposition, un opérateur qui pivote la matrice sur sa diagonale et, dans le cas d’un vecteur, retourne les colonnes en lignes et vice-versa (voir son implication dans l’équation <a href="cr%C3%A9er.html#eq:eq13">(12.13)</a> par exemple). Dans l’équation <a href="cr%C3%A9er.html#eq:eq12">(12.12)</a>, utiliser deux fois les deux vecteurs B revient à élever au carré <span class="math inline">\(\beta_i\)</span>. Si <span class="math inline">\(\mathbf{B}\)</span> sont tous égaux à 1, alors l’équation <a href="cr%C3%A9er.html#eq:eq12">(12.12)</a> est identique à la définition d’une grande somme, voir l’équation <a href="cr%C3%A9er.html#eq:GS">(12.7)</a>. En syntaxe <strong>R</strong>, l’équation <a href="cr%C3%A9er.html#eq:eq12">(12.12)</a> donne :</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">beta</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">beta1</span>, <span class="va">beta2</span><span class="op">)</span> <span class="co"># Joint beta1 et beta2 dans un vecteur</span>
<span class="va">beta</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="co"># S est la matrice de covariance calculée auparavant.</span>
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,]  156</span></code></pre></div>
<p>un code plus général et qui s’appliquera à toutes les situations (peu importe la valeur de <span class="math inline">\(p\)</span>, le nombre de variables).</p>
<p>Enfin, il est intéressant de noter que l’équation <a href="cr%C3%A9er.html#eq:eq12">(12.12)</a> est la même que les coefficients de détermination, <span class="math inline">\(R^2\)</span> <span class="citation">(<a href="r%C3%A9f%C3%A9rences.html#ref-Cohen03" role="doc-biblioref">Cohen et al., 2003</a>)</span>, lorsque toutes les variables et les coefficients de régression sont normalisés.</p>
<p>Dans le modèle linéaire simple décrit dans l’équation <a href="cr%C3%A9er.html#eq:eq9">(12.10)</a>, l’équation <a href="cr%C3%A9er.html#eq:eq11">(12.11)</a> devient :</p>
<p><span class="math display" id="eq:eq13">\[\begin{equation}
\sigma_y^2 =
\left(\begin{array}{cc}
\beta_1 &amp; \beta_2
\end{array}\right)
\left( \begin{array}{cc}
\sigma^2_{x_1} &amp; \sigma_{x_1,x_2} \\
\sigma_{x_2,x_1} &amp; \sigma^2_{x_2}
\end{array}
\right)
\left(\begin{array}{c} \beta_1 \\ \beta_2 \end{array} \right)
\tag{12.13}
\end{equation}\]</span></p>
<p>ce qui est équivalent à l’équation <a href="cr%C3%A9er.html#eq:eq11">(12.11)</a>. À ce stade, le lecteur peut avoir l’intuition que les équations <a href="cr%C3%A9er.html#eq:eq1">(12.1)</a> à <a href="cr%C3%A9er.html#eq:eq6">(12.6)</a> n’étaient qu’un cas particulier où tous les <span class="math inline">\(\beta=1\)</span>.</p>
<p>La variance de la différence de deux variables revient à affirmer que le <span class="math inline">\(\beta\)</span> des variables soustraites est de signe inverse(s’il est négatif, il devient positif ou s’il est positif, il devient négatif), c’est-à-dire <span class="math inline">\(-\beta\)</span>, ce qui pour le modèle conduit à</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
y=\beta_1 x_1-\beta_2 x_2
\]</span>
<!-- (\#eq:eq14) -->
<!-- \end{equation} --></p>
<p>ou, de manière équivalente</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
y=(\beta_1) x_1+(-\beta_2) x_2
\]</span>
<!-- (\#eq:eq15) -->
<!-- \end{equation} --></p>
<p>donnent</p>
<p><span class="math display" id="eq:eq16">\[\begin{equation}
\begin{aligned}
\sigma_y^2 &amp;=
\left(\begin{array}{cc}
\beta_1 &amp; -\beta_2
\end{array}\right)
\left( \begin{array}{cc}
\sigma^2_{x_1} &amp; \sigma_{x_1,x_2} \\
\sigma_{x_2,x_1} &amp; \sigma^2_{x_2}
\end{array}
\right)
\left(\begin{array}{c} \beta_1 \\ -\beta_2 \end{array} \right) \\[15pt]
&amp; = \beta_1^2 \sigma_{x_1}^2+(-\beta_2)^2 \sigma^2_{x_2} + 2(\beta_1)(-\beta_2)\sigma_{x_1,x_2} \\[15pt]
&amp; = \beta_1^2 \sigma_{x_1}^2+\beta_2^2 \sigma^2_{x_2} - 2\beta_1\beta_2\sigma_{x_1,x_2}
\end{aligned}
\tag{12.14}
\end{equation}\]</span></p>
<p>Ainsi, la variance de la différence de deux variables aléatoires est la somme de leurs variances en soustrayant deux fois leur échelle de covariance par <span class="math inline">\(\beta\)</span> comme prévu. Cela se vérifie avec <strong>R</strong>.</p>
<div class="sourceCode" id="cb159"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">y</span> <span class="op">=</span> <span class="va">beta1</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">-</span> <span class="va">beta2</span> <span class="op">*</span> <span class="va">x2</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
<span class="co">#&gt; [1] 58</span>

<span class="co"># Vérifier par</span>
<span class="va">beta</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">beta1</span>, <span class="op">-</span><span class="va">beta2</span><span class="op">)</span>   <span class="co"># Signe négatif, très important!</span>
<span class="va">beta</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span>       <span class="co"># Forme matricielle</span>
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,]   58</span></code></pre></div>
</div>
</div>
<div id="implications-pour-la-modélisation" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> Implications pour la modélisation<a class="anchor" aria-label="anchor" href="#implications-pour-la-mod%C3%A9lisation"><i class="fas fa-link"></i></a>
</h2>
<p>La loi de la somme des variances a de nombreuses implications dans la modélisation, en particulier si les données sont façonnées selon certaines caractéristiques souhaitables, comme dans les modèles linéaires. La régression linéaire est une approche permettant de modéliser des effets additifs (variables indépendantes, <span class="math inline">\(x_i\)</span>) pour prédire une variable dépendante (<span class="math inline">\(y\)</span>). La linéarité fait référence à la propriété d’une fonction d’être compatible avec l’addition et la mise à l’échelle. En tant que tel, il existe une relation directe entre les équations ci-dessus et le modèle linéaire :</p>
<p><span class="math display" id="eq:eq17">\[\begin{equation}
y=\beta_1 x_1 + ... +\beta_p x_p+\epsilon
\tag{12.15}
\end{equation}\]</span></p>
<p>en omettant la constante <span class="math inline">\(\beta_0\)</span>, qui ne joue aucun rôle dans la variance de la variable dépendante et en ajoutant l’erreur, une variable indépendante particulière qui est supposée ne pas être liée à <span class="math inline">\(x\)</span>, avoir une moyenne de <span class="math inline">\(0\)</span>, un écart-type <span class="math inline">\(\sigma_\epsilon\)</span>. L’équation <a href="cr%C3%A9er.html#eq:eq17">(12.15)</a> est la forme générale du modèle linéaire de l’équation <a href="cr%C3%A9er.html#eq:eq1">(12.1)</a> et <a href="cr%C3%A9er.html#eq:eq8">(12.9)</a> dans laquelle la variance de <span class="math inline">\(y\)</span> est une fonction des variances-covariances des <span class="math inline">\(x\)</span> pondérées par <span class="math inline">\(\beta\)</span>.</p>
<div id="calcul-de-la-variance-de-lerreur" class="section level3" number="12.2.1">
<h3>
<span class="header-section-number">12.2.1</span> Calcul de la variance de l’erreur<a class="anchor" aria-label="anchor" href="#calcul-de-la-variance-de-lerreur"><i class="fas fa-link"></i></a>
</h3>
<p>En prenant le modèle linéaire le plus simple, soit à deux variables :</p>
<p><span class="math display" id="eq:eq18">\[\begin{equation}
y=x+\epsilon
\tag{12.16}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(\epsilon\)</span> est le terme d’erreur résiduel. C’est le minimum pour construire un modèle bivarié. Il s’agit de la même somme bivariée que l’équation <a href="cr%C3%A9er.html#eq:eq1">(12.1)</a>, mais <span class="math inline">\(x_2\)</span> est remplacé <span class="math inline">\(\epsilon\)</span> et défini comme indépendant de <span class="math inline">\(x_1\)</span> (non corrélé). Sur la base de ce modèle, l’équation <a href="cr%C3%A9er.html#eq:eq2">(12.2)</a> conduit à l’équation suivante.</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
\sigma_y^2=\sigma_x^2+\sigma_\epsilon^2
\]</span>
<!-- (\#eq:eq19) -->
<!-- \end{equation} --></p>
<p>Dans la plupart des cas de modélisation de données, les variances <span class="math inline">\(\sigma_x^2\)</span> et <span class="math inline">\(\sigma_y^2\)</span> sont généralement spécifiées à l’avance plutôt que <span class="math inline">\(\sigma_\epsilon^2\)</span> de sorte que le calcul a priori de la variance de l’erreur au lieu de faire varier les paramètres d’intérêt est plus pertinent. En réarrangeant l’équation <a href="cr%C3%A9er.html#eq:eq18">(12.16)</a> pour isoler <span class="math inline">\(\epsilon\)</span>,</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
\epsilon=y-x
\]</span>
<!-- (\#eq:eq20) -->
<!-- \end{equation} --></p>
<p>et en termes de variance</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
\sigma_\epsilon^2=\sigma_y^2+\sigma_x^2-2\sigma_{x,y}
\]</span>
<!-- (\#eq:eq21) -->
<!-- \end{equation} --></p>
<p>où l’équation <a href="cr%C3%A9er.html#eq:eq16">(12.14)</a> pour la forme en algèbre matricielle.</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
\sigma_y^2 =
\left(\begin{array}{cc}
1 &amp; -1
\end{array}\right)
\left( \begin{array}{cc}
\sigma^2_{y} &amp; \sigma_{x,y} \\
\sigma_{x,y} &amp; \sigma^2_{x}
\end{array}
\right)
\left(\begin{array}{c} 1 \\ -1 \end{array} \right) =
\sigma^2_y + \sigma^2_x - 2\sigma_{x,y}
\]</span>
<!-- (\#eq:eq22) -->
<!-- \end{equation} --></p>
</div>
<div id="variance-des-erreurs-avec-beta" class="section level3" number="12.2.2">
<h3>
<span class="header-section-number">12.2.2</span> Variance des erreurs avec <span class="math inline">\(\beta\)</span><a class="anchor" aria-label="anchor" href="#variance-des-erreurs-avec-beta"><i class="fas fa-link"></i></a>
</h3>
<p>Une préoccupation lors de la modélisation des données est de préserver les propriétés souhaitées du modèle dans les jeux de données comme la variance de l’erreur, les paramètres de régression, les covariances, etc. Après la variance du terme d’erreur, le dernier élément à considérer est les coefficients de régression, <span class="math inline">\(\beta\)</span>, autrement dit, le degré de la relation entre les variables indépendantes et dépendantes. À partir de l’équation <a href="cr%C3%A9er.html#eq:eq18">(12.16)</a>, on ajoute la pente et l’erreur :</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
y=\beta x+\epsilon
\]</span>
<!-- (\#eq:eq23) -->
<!-- \end{equation} --></p>
<p>La loi de la somme des variances pour ce modèle devient</p>
<p><span class="math display" id="eq:eq24">\[\begin{equation}
\sigma_y^2=\beta^2 \sigma_x^2+\sigma_\epsilon^2
\tag{12.17}
\end{equation}\]</span></p>
<p>Pour rappel, l’erreur n’est pas corrélée à la variable indépendante <span class="math inline">\(x_i\)</span>. Comme précédemment, la forme d’algèbre matricielle est plus simple et plus élégante :</p>
<p><span class="math display" id="eq:eq25">\[\begin{equation}
\sigma_y^2 = \mathbf{B}^{\prime} \mathbf{\Sigma} \mathbf{B} + \sigma_\epsilon^2
\tag{12.18}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(\beta\)</span> est un vecteur contenant tous les coefficients de régression <span class="math inline">\(\beta\)</span> prédisant <span class="math inline">\(y\)</span> et s’applique à un nombre quelconque de prédicteurs <span class="math inline">\(x\)</span>. En profitant de l’indépendance du terme d’erreur, l’équation <a href="cr%C3%A9er.html#eq:eq25">(12.18)</a> est réarrangée comme :</p>
<p><span class="math display" id="eq:eq26">\[\begin{equation}
\sigma_\epsilon^2=\sigma_y^2 -\mathbf{B}^{\prime} \mathbf{\Sigma} \mathbf{B}
\tag{12.19}
\end{equation}\]</span></p>
<p>ce qui donne la variance du terme d’erreur. Sous une forme linéaire (pour une seule variable indépendante), il est possible de réarranger l’équation <a href="cr%C3%A9er.html#eq:eq24">(12.17)</a> pour isoler <span class="math inline">\(\sigma_\epsilon\)</span>.</p>
<!-- \begin{equation} -->
<p><span class="math display">\[
\sigma_\epsilon^2=\sigma_y^2-\beta^2 \sigma_x^2
\]</span>
<!-- (\#eq:eq27) -->
<!-- \end{equation} --></p>
</div>
<div id="le-scénario-standardisé" class="section level3" number="12.2.3">
<h3>
<span class="header-section-number">12.2.3</span> Le scénario standardisé<a class="anchor" aria-label="anchor" href="#le-sc%C3%A9nario-standardis%C3%A9"><i class="fas fa-link"></i></a>
</h3>
<p>Pour conclure cette section, le scénario standardisé, c’est-à-dire lorsque les variances des variables sont égales à <span class="math inline">\(1\)</span>. Plus précisément, la variance de chaque variable est fixe ; seules les erreurs résiduelles doivent être ajustées pour maintenir ces propriétés. Dans ce scénario, la matrice de variance-covariance, <span class="math inline">\(\Sigma\)</span>, est une matrice de corrélation, qui définit les variances des erreurs résiduelles. Pour calculer la variance des erreurs des variables, l’équation <a href="cr%C3%A9er.html#eq:eq26">(12.19)</a> est utilisée en remplaçant la variance des variables par <span class="math inline">\(1\)</span> comme suit</p>
<p><span class="math display" id="eq:eq28">\[\begin{equation}
\sigma_\epsilon^2=1 - \mathbf{B}^{\prime} \mathbf{\Sigma} \mathbf{B}
\tag{12.20}
\end{equation}\]</span></p>
<p>ou, pour un seul prédicteur.</p>
<p><span class="math display" id="eq:eq29">\[\begin{equation}
\sigma_\epsilon^2=1-\beta^2 \sigma_x^2
\tag{12.21}
\end{equation}\]</span></p>
<p>Comme mentionner précédemment à propos de l’équation <a href="cr%C3%A9er.html#eq:eq12">(12.12)</a>, ces deux dernières équations rappellent le coefficient de détermination. La valeur <span class="math inline">\(1\)</span> correspond au potentiel explicatif d’une variable, <span class="math inline">\(\sigma_\epsilon^2\)</span> est la variance résiduelle et, par conséquent, <span class="math inline">\(1-\sigma_\epsilon^2\)</span> est la variance expliquée par le modèle.</p>
</div>
</div>
<div id="le-scénario-non-standardisé" class="section level2" number="12.3">
<h2>
<span class="header-section-number">12.3</span> Le scénario non standardisé<a class="anchor" aria-label="anchor" href="#le-sc%C3%A9nario-non-standardis%C3%A9"><i class="fas fa-link"></i></a>
</h2>
<p>Dans la pratique, les scénarios sont rarement standardisés. Les variables n’ont pas toutes des variances de <span class="math inline">\(1\)</span> et des moyennes de <span class="math inline">\(0\)</span>. Pour ajouter une touche de naturelle aux jeux de données, il est possible, une fois que le système d’équations est complètement obtenu, d’ajouter des variances différentes en multipliant une variable par l’écart-type souhaité et d’additionner un moyenne. Cela modifiera la matrice de covariance et les coefficients de régression, mais la force relative des liens et la matrice de corrélation resteront identiques. En d’autres termes, la <em>déstandardisation</em> est l’inverse d’un score <span class="math inline">\(z\)</span>. Les données créées jusqu’ici sont des scores <span class="math inline">\(z\)</span> et pour les déstandardisés, il faut procéder ainsi <span class="math inline">\(x = s(z + \bar{x})\)</span>, où <span class="math inline">\(s\)</span> est l’écart-type et <span class="math inline">\(\bar{x}\)</span> est la moyenne de la variable déstandardisée.</p>
</div>
<div id="création-de-variables-en-série" class="section level2" number="12.4">
<h2>
<span class="header-section-number">12.4</span> Création de variables en série<a class="anchor" aria-label="anchor" href="#cr%C3%A9ation-de-variables-en-s%C3%A9rie"><i class="fas fa-link"></i></a>
</h2>
<p>Jusqu’à présent, seule la création d’une variable est présentée. Le défi pour créer un système d’équations est d’obtenir la matrice de covariance des variables précédentes pour chaque variable subséquente.</p>
<p>Puisque le sujet peut devenir compliqué rapidement, cette section est basée sur un exemple à trois variables avec un scénario standardisé (variables centrées et réduites). Le cas général sera développé par la suite.</p>
<div id="cas-spécifique" class="section level3" number="12.4.1">
<h3>
<span class="header-section-number">12.4.1</span> Cas spécifique<a class="anchor" aria-label="anchor" href="#cas-sp%C3%A9cifique"><i class="fas fa-link"></i></a>
</h3>
<p>Le défi comment lorsqu’il y a trois ou plus variables à générer. Le cas à trois variables est tout de même abordable. La <em>causalité</em> est unidirectionnelle, c’est-à-dire d’une variable vers une autre, sans retour en arrière. Cela se nomme un modèle récursif, impliquant du même coup l’existence de modèles non-récursifs, mais qui ne sont pas abordés. Pour un modèle à trois variables, une seule configuration récursive complète est possible. Elle est présentée à la Figure <a href="cr%C3%A9er.html#fig:mod3">12.2</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mod3"></span>
<img src="image/mod3.png" alt="Modèle à trois variables" width="50%" height="50%"><p class="caption">
Figure 12.2: Modèle à trois variables
</p>
</div>
<p>La Figure <a href="cr%C3%A9er.html#fig:mod3">12.2</a> montre un modèle à trois variables. La première variable <span class="math inline">\(x_1\)</span> prédit <span class="math inline">\(x_2\)</span> et <span class="math inline">\(x_3\)</span>, en plus <span class="math inline">\(x_2\)</span> prédit <span class="math inline">\(x_3\)</span>. Ce modèle peut aussi se représenter en matrice <span class="math inline">\(\mathbf{B}\)</span> dans laquelle se retrouvent les coefficients de régression qui relient les variables. Dans ce cas-ci, <span class="math inline">\(\beta_{3,1}\)</span> signifie que la variable 1 prédit la 3 à un degré <span class="math inline">\(\beta_{3,1}\)</span>; <span class="math inline">\(\beta_{3,2}\)</span> signifie que la variable 2 prédit la 3 à un degré <span class="math inline">\(\beta_{3,2}\)</span>. Le premier indice correspond à l’effet (variable dépendante) et le deuxième à la cause (variable indépendante).</p>
<p><span class="math display" id="eq:B">\[\begin{equation}
\mathbf{B} =
\left( \begin{array}{ccc}
0 &amp; 0 &amp; 0 \\
\beta_{2,1} &amp; 0 &amp; 0 \\
\beta_{3,1} &amp; \beta_{3,2} &amp; 0 \\
\end{array}
\right)
\tag{12.22}
\end{equation}\]</span></p>
<p>Afin de bâtir un exemple complet avec <strong>R</strong>, la syntaxe ci-dessous montre les paramètres arbitraires pour la création de données à parti du modèle de la figure <a href="cr%C3%A9er.html#fig:mod3">12.2</a>.</p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Pour reproductibilité</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1448</span><span class="op">)</span>            
<span class="va">n</span> <span class="op">=</span> <span class="fl">100000</span>
<span class="co"># Paramètres arbitraire</span>
<span class="va">beta21</span> <span class="op">=</span> <span class="fl">.2</span> 
<span class="va">beta31</span> <span class="op">=</span> <span class="fl">.4</span>
<span class="va">beta32</span> <span class="op">=</span> <span class="op">-</span><span class="fl">.5</span></code></pre></div>
<p>La première variable <span class="math inline">\(x_1\)</span> est exogène, c’est-à-dire qu’elle n’est prédite par aucune autre variable, et n’obtient aucune information d’aucune autre variable. Cela paraît notamment dans la matrice <span class="math inline">\(\mathbf{B}\)</span> avec la première ligne ne contenant que des 0. Ainsi, pour créer <code>x1</code>, il suffit de connaître sa variance (1 dans un scénario standardisé). Il faudra en supplément un vecteur de variances (ici, toutes fixées à 1 par le scénario standardisé).</p>
<div class="sourceCode" id="cb161"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Création de la première variable</span>
<span class="va">x1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># Variance de 1</span></code></pre></div>
<p>À l’aide de la loi de la somme des variances, la création de <span class="math inline">\(x_2\)</span> est assez simple puisqu’il n’y a qu’un seul prédicteur.</p>
<p><span class="math display">\[
x_2 = \beta_{2,1}x_1+\epsilon_{x_2}
\]</span></p>
<p>En suivant l’équation <a href="cr%C3%A9er.html#eq:eq29">(12.21)</a>, il est possible de calculer la variance résiduelle, toujours en assumant que <span class="math inline">\(\sigma^2_{x_1} = \sigma^2_{x_2}=1\)</span>.</p>
<p><span class="math display">\[
\sigma^2_{\epsilon_{x_2}} = 1-\beta_{2,1}^2\sigma^2_{x_1}=1-\beta_{21}^2
\]</span></p>
<p>En code <strong>R</strong>, il est possible de procéder ainsi.</p>
<div class="sourceCode" id="cb162"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Variance résiduelle</span>
<span class="va">e_x2</span> <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">beta21</span><span class="op">^</span><span class="fl">2</span>

<span class="co"># Création de la variable</span>
<span class="va">x2</span> <span class="op">=</span> <span class="va">beta21</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>,  sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">e_x2</span><span class="op">)</span><span class="op">)</span> <span class="co"># Variance de 1</span></code></pre></div>
<p>Maintenant, il reste à créer la variable <span class="math inline">\(x_3\)</span>. Celle-ci est générée à partir de <span class="math inline">\(x_1\)</span> et <span class="math inline">\(x_2\)</span> selon l’équation suivante.</p>
<p><span class="math display">\[
x_3 = \beta_{3,1}x_1 + \beta_{3,2}x_2 + \epsilon_{x_3}
\]</span></p>
<p>La variance résiduelle suit l’équation <a href="cr%C3%A9er.html#eq:eq28">(12.20)</a>. En équation linéaire, elle s’écrit comme suit.</p>
<p><span class="math display">\[
\sigma^2{\epsilon_{x_3}} =\sigma^2_{x_3} -( \beta_{3,1}^2\sigma^2_{x_1} + \beta_{3,2}^2\sigma^2_{x_2}  +  2\beta_{3,1}\beta_{3,2}\sigma_{x_1,x_2}) = 1-(\beta_{3,1}^2+\beta_{3,2}^2  +  2\beta_{3,1}\beta_{3,2}\sigma_{x_1,x_2})
\]</span></p>
<p>Elle occasionne toutefois un défi, car la covariance entre <span class="math inline">\(x_1\)</span> et <span class="math inline">\(x_2\)</span> n’est pas explicitement connue. Dans le cas d’une variable prédite exclusivement par une autre variable qui elle est exogène (sans prédicteur), leur covariance est égale au coefficient de régression, soit <span class="math inline">\(\beta_{21}\)</span>. Ce cas ne survient que dans cette situation précise. Il sera impératif de dégager une solution générale pour des modèles ayant plus de trois variables, la complexité du calcul de la covariance augmentant avec la croissance de <span class="math inline">\(p\)</span>. En <strong>R</strong>, <span class="math inline">\(x_3\)</span> se génère ainsi.</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Variance résiduelle</span>
<span class="va">e_x3</span> <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="va">beta31</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">beta32</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">beta31</span> <span class="op">*</span> <span class="va">beta32</span> <span class="op">*</span> <span class="va">beta21</span><span class="op">)</span>

<span class="co"># Création de la variable</span>
<span class="va">x3</span> <span class="op">=</span> <span class="va">beta31</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">beta32</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>,
                                       sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">e_x3</span><span class="op">)</span><span class="op">)</span> <span class="co"># Variance = 1</span></code></pre></div>
<p>Il est possible de vérifier les caractéristiques des trois modèles soit</p>
<div class="sourceCode" id="cb164"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Création du jeu de données</span>
<span class="va">X</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x1 <span class="op">=</span> <span class="va">x1</span>,
               x2 <span class="op">=</span> <span class="va">x2</span>,
               x3 <span class="op">=</span> <span class="va">x3</span><span class="op">)</span>

<span class="co"># Comme il s'agit d'un scénario standardisé, la matrice de</span>
<span class="co"># corrélation est similaire à la matrice de covariance.</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="co">#&gt;       x1     x2     x3</span>
<span class="co">#&gt; x1 1.002  0.202  0.303</span>
<span class="co">#&gt; x2 0.202  1.004 -0.418</span>
<span class="co">#&gt; x3 0.303 -0.418  0.997</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="co">#&gt;       x1     x2     x3</span>
<span class="co">#&gt; x1 1.000  0.201  0.303</span>
<span class="co">#&gt; x2 0.201  1.000 -0.417</span>
<span class="co">#&gt; x3 0.303 -0.417  1.000</span>

<span class="co"># Retrouver beta21</span>
<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">x2</span> <span class="op">~</span> <span class="va">x1</span>, data <span class="op">=</span> <span class="va">X</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = x2 ~ x1, data = X)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)           x1  </span>
<span class="co">#&gt;     0.00148      0.20113</span>

<span class="co"># Retrouver beta31 et beta32</span>
<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">x3</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>, data <span class="op">=</span> <span class="va">X</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = x3 ~ x1 + x2, data = X)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)           x1           x2  </span>
<span class="co">#&gt;     0.00578      0.40254     -0.49663</span></code></pre></div>
<p>Comme il s’agit d’un scénario standardisé, la matrice de corrélation est similaire à la matrice de covariance. Notamment, les variances sont près de <span class="math inline">\(1\)</span>. Aussi, les résultats confirment que la covariance entre <span class="math inline">\(x_1\)</span> et <span class="math inline">\(x_2\)</span> est bel et bien le coefficient de régression, mais surtout, comme il était mentionné, qu’il s’agit du seul cas où cela est vrai. Les régressions effectuées par <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> retrouvent également les <span class="math inline">\(\beta\)</span> choisis pour l’exemple.</p>
</div>
<div id="cas-général" class="section level3" number="12.4.2">
<h3>
<span class="header-section-number">12.4.2</span> Cas général<a class="anchor" aria-label="anchor" href="#cas-g%C3%A9n%C3%A9ral"><i class="fas fa-link"></i></a>
</h3>
<p>Un cas général permet d’obtenir la matrice de covariance <span class="math inline">\(\mathbf{\Sigma}\)</span> à partir de la matrice <span class="math inline">\(\mathbf{B}\)</span> et d’un vecteur de variance <span class="math inline">\(\text{diag}(\mathbf{\Sigma})\)</span>. Par la suite, il est possible de créer des variables en série comme la section précédente, ou bien de revenir à ce qui se faisait dans les chapitres précédents, c’est-à-dire d’utiliser <code>MASS:mvrnorm()</code> pour générer des données.</p>
<p>Pour obtenir la matrice de covariance, il est nécessaire d’avoir une matrice de coefficients de régression</p>
<p><span class="math display" id="eq:B2">\[\begin{equation}
\mathbf{B} =
\left( \begin{array}{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
\beta_{2,1} &amp; 0 &amp; 0 &amp; 0\\
...&amp; ...  &amp; 0 &amp; 0 \\
\beta_{p,1} &amp; ... &amp; \beta_{p,p-1} &amp; 0
\end{array}
\right)
\tag{12.23}
\end{equation}\]</span></p>
<p>et d’un vecteur de variances</p>
<p><span class="math display">\[\begin{equation}
\text{diag}(\mathbf{\Sigma}) = \left(\sigma^2_{x_1},...,\sigma^2_{x_p} \right)
\end{equation}\]</span></p>
<p>Ces matrices sont construites de façon générale. La matrice <span class="math inline">\(\mathbf{B}\)</span> est de dimension <span class="math inline">\(p \times p\)</span> avec des valeurs nulles comme triangle supérieur incluant la diagonale et toutes celles qui se trouvent au-dessus. Les coefficients de régresison se trouvent dans le triangle extérieur. Le vecteur de variance contient <span class="math inline">\(p\)</span> valeurs qui représentent les variances de variables. Elles sont toutes à l’unité quand le scénario est standardisé.</p>
<p>L’idée sous-jacente est qu’il est possible de calculer les covariances de la variable <span class="math inline">\(i\)</span> à partir de la matrice de covariance des variables précédente <span class="math inline">\(1:(i-1)\)</span> soit <span class="math inline">\(\mathbf{\Sigma}_{1:(i-1),1:(i-1)}\)</span> et des coefficients de régression associés <span class="math inline">\(\mathbf{B}_{i, 1:(i-1)}\)</span> en procédant en série pour toutes variables de <span class="math inline">\(x_2\)</span> (<span class="math inline">\(i=2\)</span>) (prédite par au moins une variable, toutes sauf la première) jusqu’à la dernière variable <span class="math inline">\(x_p\)</span> (<span class="math inline">\(i=p\)</span>). Le calcul est décrit à l’équation <a href="cr%C3%A9er.html#eq:buildS">(12.24)</a>, malheureusement, elle recourt à l’algèbre matricielle, mais demeure toujours beaucoup plus simple et générale que si elle était présentée en algèbre linéaire.</p>
<p><span class="math display" id="eq:buildS">\[\begin{equation}
\begin{aligned}
\mathbf{\Sigma}_{i,1:(i-1)} = \mathbf{B}_{i+1,1:i}\mathbf{\Sigma}_{1:i,1:i}\\
\mathbf{\Sigma}_{1:(i-1),i} = \mathbf{B}_{i+1,1:i}\mathbf{\Sigma}_{1:i,1:i}
\end{aligned}
\tag{12.24}
\end{equation}\]</span></p>
<p>Il faut à chaque étape s’assurer de faire le remplacement des valeurs obtenues sur chaque côté de la diagonale, ce pourquoi deux équations sont reproduites à l’équaiton <a href="cr%C3%A9er.html#eq:buildS">(12.24)</a> avec des indices différents pour <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<div class="sourceCode" id="cb165"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Matrice B</span>
<span class="va">B</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>   <span class="fl">0</span>,     <span class="fl">0</span>,    <span class="fl">0</span>,
             <span class="va">beta21</span>,   <span class="fl">0</span>,    <span class="fl">0</span>,
             <span class="va">beta31</span>, <span class="va">beta32</span>, <span class="fl">0</span><span class="op">)</span>, 
           ncol  <span class="op">=</span> <span class="fl">3</span>, nrow <span class="op">=</span> <span class="fl">3</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># Vecteur de variances</span>
<span class="va">V</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>

<span class="co"># Montrer</span>
<span class="va">B</span>
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]  0.0  0.0    0</span>
<span class="co">#&gt; [2,]  0.2  0.0    0</span>
<span class="co">#&gt; [3,]  0.4 -0.5    0</span></code></pre></div>
<p>Voici pour l’exemple précédent les étapes de calcul décrites une à une de l’équation <a href="cr%C3%A9er.html#eq:buildS">(12.24)</a>. D’abord, il faut construire une matrice, <span class="math inline">\(\mathbf{\Sigma}\)</span> (<code>S</code> en code <strong>R</strong> ci-dessous) avec comme diagonale les variances. Puis, calculer l’équation <a href="cr%C3%A9er.html#eq:buildS">(12.24)</a>. Ensuite, il faut remplacer le résultat obtenu des deux côtés de la diagonale de façon à ce que <code>S</code> demeure symétrique. Ces étapes sont répétées pour <span class="math inline">\(i=2,...,p\)</span> (dans cet exemple, <span class="math inline">\(p=3\)</span>).</p>
<div class="sourceCode" id="cb166"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Créer une matrice de covariance préliminaire</span>
<span class="va">S</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span>

<span class="co"># Aucune covariance n'est inscrite dans S</span>
<span class="va">S</span>
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]    1    0    0</span>
<span class="co">#&gt; [2,]    0    1    0</span>
<span class="co">#&gt; [3,]    0    0    1</span>

<span class="co"># Première variable</span>
<span class="va">i</span> <span class="op">=</span> <span class="fl">2</span>

<span class="co"># Calcul de la covariance entre x1 et x2</span>
<span class="va">B</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span>
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,]  0.2</span>

<span class="co"># Il faut remplacer ce résultat de chaque côté de la diagonale</span>
<span class="co"># Calcul de la covariance entre x1 et x2 assignée à COV</span>
<span class="va">COV</span> <span class="op">=</span> <span class="va">B</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span>
<span class="va">S</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span> <span class="op">=</span> <span class="va">COV</span>
<span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">i</span><span class="op">]</span> <span class="op">=</span> <span class="va">COV</span>

<span class="co"># La matrice est mise à jour pour i = 2</span>
<span class="va">S</span>
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]  1.0  0.2    0</span>
<span class="co">#&gt; [2,]  0.2  1.0    0</span>
<span class="co">#&gt; [3,]  0.0  0.0    1</span>

<span class="co"># Pour i = 3, l'équation est reprise</span>
<span class="va">i</span> <span class="op">=</span> <span class="fl">3</span>
<span class="co"># Calcul de la covariance entre x1 et x2 assignée à COV</span>
<span class="va">COV</span> <span class="op">=</span> <span class="va">B</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span>
<span class="va">S</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span> <span class="op">=</span> <span class="va">COV</span>
<span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="va">i</span><span class="op">]</span> <span class="op">=</span> <span class="va">COV</span>

<span class="co"># La matrice est mise à jour pour i = 3</span>
<span class="va">S</span>
<span class="co">#&gt;      [,1]  [,2]  [,3]</span>
<span class="co">#&gt; [1,]  1.0  0.20  0.30</span>
<span class="co">#&gt; [2,]  0.2  1.00 -0.42</span>
<span class="co">#&gt; [3,]  0.3 -0.42  1.00</span>

<span class="co"># Elle est approximativement identique aux données de l'exemple précédent</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="co">#&gt;       x1     x2     x3</span>
<span class="co">#&gt; x1 1.002  0.202  0.303</span>
<span class="co">#&gt; x2 0.202  1.004 -0.418</span>
<span class="co">#&gt; x3 0.303 -0.418  0.997</span></code></pre></div>
<p>Avant de procéder, une courte digression sur une façon de réaliser en moins de lignes, mais avec une syntaxe plus complexe, la réassignation des valeurs dans <code>S</code>. Le code est présenté dans la syntaxe ci-dessous. Il appert qu’il n’est pas aussi intéressant à programmer pour sauver deux lignes.</p>
<div class="sourceCode" id="cb167"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Une note pour indiquer que les valeurs à remplacer pourrait</span>
<span class="co"># être fait en une seule ligne de syntaxe en bénéficiant du </span>
<span class="co"># recycle vectoriel de R, mais la solution n'est ni simple, ni élégante.</span>
<span class="va">remplacer</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">i</span>, <span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">i</span>, <span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">S</span><span class="op">[</span><span class="va">remplacer</span><span class="op">]</span></code></pre></div>
<p>Trêve de digression, une fois la matrice de covariance <code>S</code> calculée, la fonction <code><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">MASS::mvrnorm()</a></code> peut être utilisée pour créer des données. Les résultats sont presque identiques pour les régressions.</p>
<div class="sourceCode" id="cb168"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1448</span><span class="op">)</span> <span class="co"># La même que l'exemple précédent</span>

<span class="co"># Création de données</span>
<span class="va">X</span> <span class="op">=</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, Sigma <span class="op">=</span> <span class="va">S</span><span class="op">)</span>

<span class="co"># Configurer en tableau de données (data.frame)</span>
<span class="va">X</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x1"</span>,<span class="st">"x2"</span>,<span class="st">"x3"</span><span class="op">)</span>

<span class="co"># Retrouver beta21</span>
<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">x2</span> <span class="op">~</span> <span class="va">x1</span>, data <span class="op">=</span> <span class="va">X</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = x2 ~ x1, data = X)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)           x1  </span>
<span class="co">#&gt;    -0.00219      0.20191</span>

<span class="co"># Retrouver beta31 et beta32</span>
<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">x3</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>, data <span class="op">=</span> <span class="va">X</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = x3 ~ x1 + x2, data = X)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)           x1           x2  </span>
<span class="co">#&gt;    -0.00579      0.40295     -0.49899</span></code></pre></div>
<p>Comme la formule est générale et qu’elle implique plusieurs itérations, il est envisageable de programmer ces calculs avec une boucle dans une fonction maison. La fonction maison <code>beta2cov()</code> permet à partir d’une matrice de coefficient de régression et de</p>
<div class="sourceCode" id="cb169"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># De Beta vers covariance (beta 2 covariance)</span>
<span class="va">beta2cov</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">B</span>, <span class="va">V</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span><span class="op">{</span>
  
  <span class="va">p</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>                 <span class="co"># Nombre de variables</span>
  
  <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span><span class="op">)</span><span class="op">{</span>               <span class="co"># Si V est nulle, alors V est une</span>
    <span class="va">S</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>                 <span class="co"># matrice diagonale d'identité,</span>
  <span class="op">}</span><span class="kw">else</span><span class="op">{</span>                        <span class="co"># autrement il s'agit d'une matrice</span>
    <span class="va">S</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span>                 <span class="co"># avec les variances en diagonale</span>
  <span class="op">}</span>  
  
  <span class="co"># Boucle de calcul pour la covariance de la variable i (i = 2:p)</span>
  <span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">p</span><span class="op">)</span><span class="op">{</span>
    <span class="va">COV</span> <span class="op">=</span> <span class="va">B</span><span class="op">[</span><span class="op">(</span><span class="va">i</span><span class="op">)</span>, <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span>
    <span class="va">S</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span> <span class="op">=</span> <span class="va">COV</span>
    <span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="va">i</span><span class="op">]</span> <span class="op">=</span> <span class="va">COV</span>
  <span class="op">}</span>
  
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span>
  
<span class="op">}</span></code></pre></div>
<p>Évidemment, comme il est possible de passer de matrice de coefficient de régression à une matrice de covariance, il est possible de faire l’inverse. L’équation <a href="cr%C3%A9er.html#eq:buildS">(12.24)</a> est réarrangée pour isoler <span class="math inline">\(\mathbf{B}\)</span> de l’équation ce qui donne l’équation <a href="cr%C3%A9er.html#eq:buildB">(12.25)</a>.</p>
<p><span class="math display" id="eq:buildB">\[\begin{equation}
\mathbf{B}_{i+1, 1:i} = \mathbf{\Sigma}^{-1}_{1:i,1:i}\mathbf{\Sigma}_{1+i,1:i}
\tag{12.25}
\end{equation}\]</span></p>
<p>L’équation <a href="cr%C3%A9er.html#eq:buildB">(12.25)</a> se transforme (relativement) aisément en fonction maison. Par rapport à <code>beta2cov()</code>, la boucle n’inclut pas la <span class="math inline">\(p\)</span><sup>e</sup> variable, mais bien la première.</p>
<div class="sourceCode" id="cb170"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># De la covariance à Beta (cov2beta)</span>
<span class="va">cov2beta</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">S</span><span class="op">)</span><span class="op">{</span>
  
  <span class="va">p</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>          <span class="co"># Nombre de variable</span>
  <span class="va">BETA</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">p</span>, <span class="va">p</span><span class="op">)</span> <span class="co"># Matrice vide</span>
  
  <span class="co"># Boucle de calcul pour la covariance de la variable i (i = 1:(p-1))</span>
  <span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">p</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">{</span>
    <span class="va">BETA</span><span class="op">[</span><span class="va">i</span><span class="op">+</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">:</span><span class="va">i</span><span class="op">]</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="va">i</span><span class="op">]</span>, <span class="va">S</span><span class="op">[</span><span class="fl">1</span><span class="op">+</span><span class="va">i</span>, <span class="fl">1</span><span class="op">:</span><span class="va">i</span><span class="op">]</span><span class="op">)</span>
  <span class="op">}</span>
  
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">BETA</span><span class="op">)</span>
  
<span class="op">}</span></code></pre></div>
<p>La fonction maison <code>cov2beta()</code> est maintenant testée sur <code>S</code> pour évaluer si elle retourne bien la matrice <code>B</code> originale.</p>
<div class="sourceCode" id="cb171"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">cov2beta</span><span class="op">(</span><span class="va">S</span><span class="op">)</span>
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]  0.0  0.0    0</span>
<span class="co">#&gt; [2,]  0.2  0.0    0</span>
<span class="co">#&gt; [3,]  0.4 -0.5    0</span></code></pre></div>
<p>Ce qui est le cas.</p>
<p>Une seule contrainte s’impose lors de la du calcul de la covariance à partir des coefficients de régression. Il s’agit de s’assurer que la matrice de covariance demeure positive semi-définie à chaque étape. Cela se manifeste notamment lorsque les coefficients de régression pour une variable dépendante sont si élevés que le coefficient de détermination surpasse la variance de la variable en question. Autrement dit, la variance de la variable n’est pas assez élevée pour le potentiel explicatif. Mathématiquement le problème est que <span class="math inline">\(1-R^2_X &lt; 0\)</span> ou plus généralement que <span class="math inline">\(\sigma^2_y-\mathbf{B^\prime\Sigma B}&lt;0\)</span>. Dans ces cas, la variance résiduelle négative, ce qui est impossible. L’une des corrections a apporté sont les suivantes, réduire les coefficients de régression <span class="math inline">\(\mathbf{B}\)</span> pour cette variable ou encore d’augmenter sa variance <span class="math inline">\(\sigma^2_y\)</span> de sorte que <span class="math inline">\(\sigma^2_y-\mathbf{B^\prime\Sigma B}&gt;=0\)</span> soit toujours vrai à chaque étape.</p>
<p>Une dernière note, comme ces équations et syntaxes procèdent de <span class="math inline">\(i=2,...,p\)</span>, l’ordre des variables est primordiale et interchanger leur ordre a des conséquences substantielles sur les résultats. Lorsque la matrice <span class="math inline">\(\mathbf{B}\)</span> est créées, il faut être sûr de l’ordre <em>déterministe</em> des variables, c’est-à-dire, quelle variable <em>cause</em> quelles autres variables, comme c’était spécifié à la Figure <a href="cr%C3%A9er.html#fig:mod3">12.2</a> par exemple. Changer ou retirer ne serait-ce qu’une variable changerait les coefficients de régression : les régressions ne seraient plus identiques, la matrice de coefficients de régression ne serait pas retrouvée. Il ne faut pas être surpris donc, si les résultats changent dans cette situation. Nonobstant, changer ou retirer une variable peut être pertinent dans certains contextes, surtout pour l’étude de la <strong>misspécification</strong> (en anglais) des modèles, c’est-à-dire lorsqu’un modèle erroné est utilisé plutôt que le vrai modèle, ce qui entraîne notamment des biais. Les études à ce sujet emprunteront une méthode statistique similaire.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="r%C3%A9gression.html"><span class="header-section-number">11</span> Régression</a></div>
<div class="next"><a href="lanalyse-en-composantes-principales.html"><span class="header-section-number">13</span> L’analyse en composantes principales</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cr%C3%A9er"><span class="header-section-number">12</span> Créer</a></li>
<li>
<a class="nav-link" href="#la-loi-de-la-somme-des-variances"><span class="header-section-number">12.1</span> La loi de la somme des variances</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#ajout-des-constantes-d%C3%A9chelle-beta"><span class="header-section-number">12.1.1</span> Ajout des constantes d’échelle \(\beta\)</a></li></ul>
</li>
<li>
<a class="nav-link" href="#implications-pour-la-mod%C3%A9lisation"><span class="header-section-number">12.2</span> Implications pour la modélisation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#calcul-de-la-variance-de-lerreur"><span class="header-section-number">12.2.1</span> Calcul de la variance de l’erreur</a></li>
<li><a class="nav-link" href="#variance-des-erreurs-avec-beta"><span class="header-section-number">12.2.2</span> Variance des erreurs avec \(\beta\)</a></li>
<li><a class="nav-link" href="#le-sc%C3%A9nario-standardis%C3%A9"><span class="header-section-number">12.2.3</span> Le scénario standardisé</a></li>
</ul>
</li>
<li><a class="nav-link" href="#le-sc%C3%A9nario-non-standardis%C3%A9"><span class="header-section-number">12.3</span> Le scénario non standardisé</a></li>
<li>
<a class="nav-link" href="#cr%C3%A9ation-de-variables-en-s%C3%A9rie"><span class="header-section-number">12.4</span> Création de variables en série</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#cas-sp%C3%A9cifique"><span class="header-section-number">12.4.1</span> Cas spécifique</a></li>
<li><a class="nav-link" href="#cas-g%C3%A9n%C3%A9ral"><span class="header-section-number">12.4.2</span> Cas général</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/quantmeth/MQR/blob/master/12-Creer.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/quantmeth/MQR/edit/master/12-Creer.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Méthodes Quantitatives avec R</strong>" was written by P.-O. Caron. It was last built on 2022-05-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
